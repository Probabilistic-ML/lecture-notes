% Encoding: UTF-8

@Book{Klenke2013,
  title     = {Wahrscheinlichkeitstheorie},
  publisher = {Springer Spektrum},
  year      = {2013},
  author    = {Klenke, A.},
  series    = {Masterclass},
  edition   = {3rd},
  doi       = {10.1007/978-3-642-36018-3},
}

@InProceedings{Deisenroth2011,
  author    = {Deisenroth, M.P. and Rasmussen, C.E.},
  title     = {PILCO: A Model-Based and Data-Efficient Approach to Policy Search},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
  year      = {2011},
  pages     = {465-472},
  publisher = {Omnipress},
  editors   = {L Getoor and T Scheffer},
}

@Book{Rooch2014,
  title     = {Statistik für Ingenieure - Wahrscheinlichkeitsrechnung und Datenauswertung endlich verst{\"a}ndlich},
  publisher = {Springer Spektrum},
  year      = {2014},
  author    = {Rooch, A.},
  series    = {Springer-Lehrbuch},
  edition   = {1st},
  doi       = {10.1007/978-3-642-54857-4},
}

@Book{Cramer2017,
  title     = {Grundlagen der Wahrscheinlichkeitsrechnung und Statistik - Eine Einf{\"u}hrung für Studierende der Informatik, der Ingenieur- und Wirtschaftswissenschaften},
  publisher = {Springer Spektrum},
  year      = {2017},
  author    = {Cramer, E. and Kamps, U.},
  series    = {Springer-Lehrbuch},
  edition   = {4th},
  doi       = {10.1007/978-3-662-54161-6},
}

@Book{Theodoridis2020,
  title     = {Machine Learning: A Bayesian and Optimization Perspective},
  publisher = {Elsevier Science},
  year      = {2020},
  author    = {Theodoridis, S.},
  edition   = {2nd},
  doi       = {10.1016/C2019-0-03772-7},
}

@Book{Murphy2012,
  title     = {Machine learning : A Probabilistic Perspective},
  publisher = {MIT Press},
  year      = {2012},
  author    = {Murphy, K. P.},
}

@Book{Rasmussen2006,
  title     = {Gaussian Processes for Machine Learning},
  publisher = {MIT Press},
  year      = {2006},
  author    = {Rasmussen, C.E. and Williams, C.K.I.},
  series    = {Adaptive Computation and Machine Learning},
  edition   = {2nd},
  url       = {http://www.gaussianprocess.org/gpml/},
}

@book{Kochenderfer2019,
  author = {Kochenderfer, Mykel J. and Wheeler, Tim A.},
  title = {Algorithms for Optimization},
  year = {2019},
  isbn = {0262039427},
  publisher = {The MIT Press},
}

@book{Spall2003,
  author = {Spall, James C.},
  title = {Introduction to Stochastic Search and Optimization},
  year = {2003},
  isbn = {0471330523},
  publisher = {John Wiley &amp; Sons, Inc.},
  address = {USA},
  edition = {1},
}

@article{Roh2019,
author = {Roh, Yuji and Heo, Geon and Whang, Steven},
year = {2019},
month = {10},
pages = {1-1},
title = {A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective},
volume = {PP},
journal = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2019.2946162}
}

@article{Kotsiantis2006,
author = {Kotsiantis, Sotiris and Kanellopoulos, Dimitris and Pintelas, P.},
year = {2006},
month = {01},
pages = {111-117},
title = {Data Preprocessing for Supervised Learning},
volume = {1},
journal = {International Journal of Computer Science}
}

@InProceedings{Plagemann2008,
  author    = {Christian Plagemann and Kristian Kersting and Wolfram Burgard},
  booktitle = {ECML/PKDD},
  title     = {Nonstationary {G}aussian Process Regression Using Point Estimates of Local Smoothness},
  year      = {2008},
}

@PhdThesis{Cremanns2021,
  author = {Kevin Cremanns and Dirk Roos and Stefan Reh and Sebastian M{\"u}nstermann},
  school = {RWTH Aachen},
  title  = {Probabilistic machine learning for pattern recognition and design exploration},
  year   = {2021},
}

@InProceedings{Titsias2009,
  author    = {Titsias, Michalis},
  booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  title     = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
  year      = {2009},
  address   = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  editor    = {van Dyk, David and Welling, Max},
  month     = {16--18 Apr},
  pages     = {567--574},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {5},
  abstract  = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  pdf       = {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},
  url       = {https://proceedings.mlr.press/v5/titsias09a.html},
}

@InProceedings{Salimbeni2017,
  author    = {Salimbeni, Hugh and Deisenroth, Marc},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Doubly Stochastic Variational Inference for Deep Gaussian Processes},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper/2017/file/8208974663db80265e9bfe7b222dcb18-Paper.pdf},
}

@InProceedings{Damianou2013,
  author    = {Damianou, Andreas and Lawrence, Neil D.},
  booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  title     = {Deep {G}aussian Processes},
  year      = {2013},
  address   = {Scottsdale, Arizona, USA},
  editor    = {Carvalho, Carlos M. and Ravikumar, Pradeep},
  month     = {29 Apr--01 May},
  pages     = {207--215},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {31},
  abstract  = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.},
  pdf       = {http://proceedings.mlr.press/v31/damianou13a.pdf},
  url       = {https://proceedings.mlr.press/v31/damianou13a.html},
}

@InProceedings{Wilson2015,
  author    = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  title     = {Deep Kernel Learning},
  year      = {2016},
  address   = {Cadiz, Spain},
  editor    = {Gretton, Arthur and Robert, Christian C.},
  month     = {09--11 May},
  pages     = {370--378},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {51},
  abstract  = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods.  Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation.  These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability.  We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process.  Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point.  On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
  pdf       = {http://proceedings.mlr.press/v51/wilson16.pdf},
  url       = {https://proceedings.mlr.press/v51/wilson16.html},
}

@InProceedings{Wilson2016,
  author    = {Andrew Gordon Wilson and Zhiting Hu and Ruslan Salakhutdinov and Eric P. Xing},
  booktitle = {NIPS},
  title     = {Stochastic Variational Deep Kernel Learning},
  year      = {2016},
}

@InProceedings{Casale2018,
  author    = {Francesco Paolo Casale and Adrian V. Dalca and Luca Saglietti and Jennifer Listgarten and Nicol{\'o} Fusi},
  booktitle = {NeurIPS},
  title     = {Gaussian Process Prior Variational Autoencoders},
  year      = {2018},
}

@InProceedings{Jazbec2021,
  author    = {Jazbec, Metod and Ashman, Matt and Fortuin, Vincent and Pearce, Michael and Mandt, Stephan and R{\"a}tsch, Gunnar},
  booktitle = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  title     = {Scalable Gaussian Process Variational Autoencoders},
  year      = {2021},
  editor    = {Banerjee, Arindam and Fukumizu, Kenji},
  month     = {13--15 Apr},
  pages     = {3511--3519},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {130},
  abstract  = {Conventional variational autoencoders fail in modeling correlations between data points due to their use of factorized priors. Amortized Gaussian process inference through GP-VAEs has led to significant improvements in this regard, but is still inhibited by the intrinsic complexity of exact GP inference. We improve the scalability of these methods through principled sparse inference approaches. We propose a new scalable GP-VAE model that outperforms existing approaches in terms of runtime and memory footprint, is easy to implement, and allows for joint end-to-end optimization of all components.},
  pdf       = {http://proceedings.mlr.press/v130/jazbec21a/jazbec21a.pdf},
  url       = {https://proceedings.mlr.press/v130/jazbec21a.html},
}

@Misc{Kingma2014,
  author        = {Diederik P Kingma and Max Welling},
  title         = {Auto-Encoding Variational Bayes},
  year          = {2014},
  archiveprefix = {arXiv},
  eprint        = {1312.6114},
  primaryclass  = {stat.ML},
}

@Misc{Leibfried2021,
  author        = {Felix Leibfried and Vincent Dutordoir and ST John and Nicolas Durrande},
  title         = {A Tutorial on Sparse Gaussian Processes and Variational Inference},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2012.13962},
  primaryclass  = {cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}
