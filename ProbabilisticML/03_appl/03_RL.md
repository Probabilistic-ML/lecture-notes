# Efficient Reinforcement Learning
So far, supervised learning (SML) tasks were considered, where an algorithm was expected to learn the relationship $f: \mathbb{R}^{n_i} : \mathbb{R}^{n_o}$ using a labeled dataset $\mathbb{D} := \{ (x_j, y_j), j\in [1,  m]\}$ with $m$ samples, where each sample consists of a tuple of input features $ x_j \in \mathbb{R}^{n_i}$  and the corresponding labels $ y_j \in \mathbb{R}^{n_o}$. Although SML is useful for many engineering tasks (e.g. design optimization, model validation and calibration, reliability analysis), it in itself does not provide a solution to problems, where some system parameters are controlled to achieve the desired system behaviour. Such problems are often investigated using the optimal control theory in engineering applications.

Whether the aim is to obtain the optimal trajectory for a satellite to maximize the fuel efficiency or to obtain the optimal pressure and flow rate of a medicinal ventilator, the description of the task does not coincide with SML. In contrast, reinforcement learning (RL) seeks to directly solve such problems. As such, there are many intersections regarding the optimal control theory and RL. 

## Problem definition
 In optimal control problems, the aim is to find the optimal sequence of *control* values with respect to objective and constraint functions, one or more of which are formulated over a dynamic system. In RL, the aim is to find the sequence of *action* values that maximize the *expected reward*, where reward is a dynamic function as part of the *environment*. 
 
 Although the definitions look different since the used terminologies stem from different domains, there are many overlaps between the kind of tasks solved in these domains. Let us define individual terms to discuss this further.

### Dynamic model and environment
In this work, a dynamic model refers to a time dependent model of following type:
 
 $$ s_{t+dt} = f(s_{t}, u_{t})  $$

where $s_t \in \mathbb{R}^{n_s}$ are the *state* variables at time $t \in \mathbb{R}$ and $u_t \in \mathbb{R}^{n_u}$ are the *control* variables and $f: \mathbb{R}^{n_a + n_s} \rightarrow \mathbb{R}^{n_s}$ is the *transition* function. Often, $f$ is the first derivative of the solution of a set of differential equations with respect to time.

One approach to evaluate such systems is to discretize the time dimension. Besides being helpful regarding the optimization problem (see e.g. [First-discretize-then-optimize](https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/tech-papers/09/09t_005.pdf)), it also simplifies the notation. The time discretized model can be denoted as

$$ s_{t+1} = f(s_{t}, u_{t})  $$

where $t \in \mathbb{N}$ now represents index of the discretization step, i.e. the actual time can be computed 
as $t \cdot \Delta t$ for an equidistant discretization with step size $\Delta t \in \mathbb{R}$.

Thus, given an initial configuration $s_0$ and the transition function $f$, the state of the system at any point $t$ in 
time can be computed as

$$ s_t = f(\dots f(f(s_0, u_0), u_1)\dots, u_{t-1})  $$

and a trajectory $\mathbb{T}$ of the system, i.e. its evolution in time, can be constructed from tuples of state action pairs

$$ \mathbb{T} := \left\{(s_t, u_t); t \in [0, n_t] \right\}$$

where $n_t \in \mathbb{N}$ is the number of discretization steps. 

```{note}
The trajectory $\mathbb{T}$ as formulated above will always have one state more than the actions leading to that state. For ease of notation, we assume that the last action is set to $a_n = 0$ in the following.
```



### Optimization
In the context of optimal control, there are various kinds of optimization problems considering the kind of dynamic model described above. 
Here, we will investigate closed-loop control problems \cite{AnyControlTheoryBook} without constraints. 
In such problems, the aim is to reach and maintain a target state $s^*$. Moreover, the expected result is not an optimal trajectory $\mathbb{T}^*$ but a controller $\pi: \mathbb{R}^{n_s} \rightarrow \mathbb{R}^{n_u}$. The actual problem has an infinite horizon, although a finite one $t_f$ has to be chosen for evaluation due to practical reasons. 
Thus, a possible objective for this kind of problems can be given as

$$ \mathrm{arg} \min_{\pi} \sum_{t=0}^{n_t}\left(s_t - s^*\right)^2 $$

where expectation is computed along the time dimension. Notice that any other error metric could also be used instead of the 
squared error. As such, the optimal controller $\pi^*$ is the one, that minimizes the chosen error metric.
 

### Reinforcement learning
In RL literature, the dynamic model is part of the *environment* which represents the interaction space for an *agent* $\pi$. For our purposes, *agent* has the same function as the *controller* before. 
Besides the dynamic model, environment may also define sources of noise as well as a reward function $r: \mathbb{R}^{n_s + n_u} \rightarrow \mathbb{R}$ equivalent to an optimization objective. Thus, RL seeks to solve the following optimization problem

$$ \mathrm{arg}\max_{\pi} \sum_{t=0}^{n_t} r\left(s_t, u_t\right) $$

As such, the optimal agent $\pi^*$ is the one, that maximizes the total return. Notice that this equation is equivalent to the optimization objective given before. Typically, the dataset $\mathbb{D}$ in RL consists of multiple trajectories $\mathbb{T}_i$ representing different trials.


Possibly the most important difference between SML and RL algorithms is the existence of the optimal actions. If these are available as examples, SML framework could be theoretically used to to learn to output them. Otherwise RL is used to obtain them by interacting with the environment and observing its response. Furthermore, existing good actions can also be used in RL (e.g. expert demonstrations \citepeg{OpenAI2018,OtherGuyWithFewShotRL}) In any case, recent research \citepeg{UpsideDownRL,RewardToRL) shows that such distinctions are not necessarily clear in all cases.

A more detailed description of reinforcement learning is beyond the scope of this book. See [Reinforcement Learning by Sutton \& Barto](http://incompleteideas.net/book/RLbook2020.pdf) for further reference.


## Probabilistic Inference for Optimal Control 

Assume that the transition function $f$ in the dynamic model is not known or too expensive to optimize.
As discussed earlier, SML algorithms can be used to acquire surrogate models, that are usually faster to evaluate at the cost of some accuracy loss.


Let us use a $\mathcal{GP}$ to approximate the unknown transition function $f$. Specifically, let us train a model $\tilde{f}: \mathbb{R}^{n_s + n_a} \rightarrow \mathbb{R}^{n_s}$ to use 
the current state $s_t$ and action $u_t$ as input and predict the state difference $\Delta^s_{t} \in \mathbb{R}^{n_s}$ such that

$$ s_{t + 1} = s_t + \Delta^s_{t} $$


We can take a trajectory $\mathbb{T}$ and create tuples of input $x^T = [s_t^T, a_t^T]$ and output $\Delta^s_t = s_{t+1} - s_t$ features from each transition.
Using this data set, a $\mathcal{GP}$ model can be trained. The resulting one step (approximate) dynamics model can be given as

\begin{align}
p(s_{t + 1} | x_t, u_t) &= \mathcal{N}(s_{t+1} | \mu_{t+1}, \Sigma_{t+1}) \\
\mu_{t+1} &= s_t + \mu_{\Delta^s_{t}} \\
\Sigma_{t+1} &= \Sigma^2_{\Delta^s_{t}} \\
\end{align}

where $\mu_{\Delta^s_{t}} \in \mathbb{R}^{n_s}$ and $\Sigma^2_{\Delta^s_{t}}  \in \mathbb{R}^{n_s}$ are the posterior mean and the variance of the $\mathcal{GP}$ (see [Gaussian Process](https://probabilistic-ml.github.io/lecture-notes/02_probML/kernelmethods.html#gaussian-process)).

Now that we have a model of the dynamics, we could use control theory (or any other appropriate method) to find an optimal controller $\pi$.
Moreover, we could use Bayesian optimization to account for the model uncertainty, which could accelerate the optimization.
However, notice that the output of each step is the input to the next step. 
Even if we assume the initial state $s_0$ to be known, $s_1$ is an uncertain variable, which will be part of the input when predicting $s_2$.
A better strategy would be to propagate the uncertainty for a more accurate estimate of $\Sigma_{t+1}$.

Propagating the uncertainty is not a trivial task {cite}`Deisenroth2011`. Let us elaborate using an example.











```{bibliography}
:filter: docname in docnames
:style: plain
```


