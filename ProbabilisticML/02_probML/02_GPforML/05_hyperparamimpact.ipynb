{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa75b32",
   "metadata": {},
   "source": [
    "(sec:impactofhyperp)=\n",
    "# Impact of Hyperparameters\n",
    "\n",
    "Most kernels depend on so-called hyperparameters. For example, the length scale $l$ which appears in many of the previously stated examples, the noise variance $\\sigma_{\\text{noise}}^2$ as well as the scaling parameter $\\sigma^2$ mentioned in {ref}```sec:combofkernels``` are hyperparameters. In addition to the genereal choice of the kernel, the exact values of these parameters determine the properties of the underlying Gaussian process. In general, we denote the collection of all hyperparameters of some kernel $k$ by $\\theta$. Please note that $\\theta$ is possibly vector valued, if $k$ possesses more than one hyperparameter.\n",
    "\n",
    "For example, the scaled squared exponential kernel with positive hyperparameters $l$ and $\\sigma^2$ is given by\n",
    "\n",
    "$$k(x, x^{\\prime}) = \\sigma^2~\\exp \\big(-\\frac{r^2}{2~l^2} \\big)$$\n",
    "\n",
    "where $r = |x - x^{\\prime}|$ for $x, x^{\\prime} \\in \\mathbb{R}^d$. Moreover, noise is variance $\\sigma_{\\text{noise}}^2$ is added. Thus, this kernel yields $\\theta = (l, \\sigma^2, \\sigma_{\\text{noise}}^2)$. A larger length scale implies a higher correlation between $f(x)$ and $f(x^{\\prime})$. Similarly, higher value of $\\sigma^2$ implies a smaller correlation.\n",
    "\n",
    "Given a test point $x^*$, the {ref}```mean prediction and variance<lem:gpregr>``` in Gaussian process regression depend on $K(X, X)$ as well as $K(x^*, X)$ which in turn depend on $\\theta$. Consequently, the hyperparmeters impact the model predictions.\n",
    "\n",
    "For the case of the scaled squared exponential kernel with optional noise, the impact is shown in the following visualization which can be used in Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63941f0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Gaussian process posterior\n",
    "def cond_distr(X1, y1, X2, kernel):\n",
    "    \"\"\"\n",
    "    Calculate the posterior mean and covariance matrix for y2\n",
    "    based on the corresponding input X2, the observations (y1, X1), \n",
    "    and the prior kernel function.\n",
    "    \"\"\"\n",
    "    # Kernel of the observations\n",
    "    Σ11 = kernel(X1)\n",
    "    # Kernel of observations vs to-predict\n",
    "    Σ12 = kernel(X1, X2)\n",
    "    # Solve\n",
    "    solved = scipy.linalg.solve(Σ11, Σ12, assume_a='pos').T\n",
    "    # Compute posterior mean\n",
    "    μ2 = solved @ y1\n",
    "    # Compute the posterior covariance\n",
    "    Σ22 = kernel(X2, X2)\n",
    "    Σ2 = Σ22 - (solved @ Σ12)\n",
    "    return μ2[:, 0], Σ2  # mean, covariance\n",
    "\n",
    "def _plot(mean, cov):\n",
    "    fig= plt.figure(figsize=(14, 10))\n",
    "    ax = plt.axes(xlim=(0, 10), ylim=(-2.5, 2.5))\n",
    "    ax.scatter(X, Y)\n",
    "    plt.plot(t, np.sin(t), label='true function')\n",
    "    plt.plot(t, mean, c='purple', label='mean prediction')\n",
    "    plt.fill_between(t, mean - 1.96 * np.sqrt(np.diag(cov)), mean + 1.96 * np.sqrt(np.diag(cov)), \n",
    "                     color='purple', alpha=.25, label='credible interval')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6848c1f6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d0fa431ecc4273912674bbb37bcf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.05, continuous_update=False, description='l', max=3.0, min=1e-05, st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "!pip install ipympl\n",
    "clear_output()\n",
    "\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interact_manual, FloatSlider\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, FFMpegFileWriter\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "\n",
    "# sample training data\n",
    "X = np.array([1.01, 3.51, 4.51, 7.01, 7.91, 9.01]).reshape(-1, 1)\n",
    "Y = np.sin(X)\n",
    "nb_steps = 500\n",
    "delta_t = 10 / nb_steps\n",
    "t = np.arange(0, 10, delta_t)\n",
    "\n",
    "@interact(l=FloatSlider(value=2.05, min=1e-5, max=3, step=0.05, continuous_update=False), \n",
    "          sigma2=FloatSlider(value=1., min=1e-5, max=10, step=0.1, continuous_update=False),\n",
    "          noise_level=FloatSlider(value=0., min=0., max=1., step=0.01, continuous_update=False))\n",
    "def anim_hyper(l, sigma2, noise_level):\n",
    "    kernel = ConstantKernel(constant_value=sigma2) * RBF(length_scale=l) + WhiteKernel(noise_level=noise_level)\n",
    "    mean, cov = cond_distr(X, Y, t.reshape(-1, 1), kernel) \n",
    "    _plot(mean, cov)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
