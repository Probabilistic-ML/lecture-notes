{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844e423a",
   "metadata": {},
   "source": [
    "## Scalable Gaussian Processes\n",
    "\n",
    "**Recap Gaussian Process Regression**\n",
    "\n",
    "In [GPR lemma](lem:gpregrnoise) we have fully discribed a Gaussian process for regression. We will shorten this notation of $K(X^*, X^*)$ to $K_{**}$ , $K(X^*, X)$ to $K_{*}^T$ and $K(X, X^*)$ to $K_{*}$, which leads to\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y_* \\lvert X_*X,y) &= \\mathcal{N}(y_* \\lvert \\mu_*, \\Sigma_*)\\\\\n",
    "\\mu_* &= K_{*}^T \\big(K + \\sigma_{\\mathrm{noise}}^2 I_n\\big)^{-1} y\\\\\n",
    "\\Sigma_* &= K_{**} - K_{*}^T \\big(K + \\sigma_{\\mathrm{noise}}^2 I_n\\big)^{-1} K_{*}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can predict function values $f^*$ at new inputs $X^*$ with this. The most costly computation step of this is to compute the invserse of a covariance matrix (with noise)\n",
    "\n",
    "$$\n",
    "\\big(K + \\sigma_{\\mathrm{noise}}^2 I_n\\big)^{-1}\n",
    "$$\n",
    "\n",
    "which is needed in the mean and the covariance of the gaussian process.\n",
    "This matrix has the size of $n \\times n$, where $n$ is the number of training samples. The problem now is that, for large $n$ a matrix inversion is intractable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8e5d8",
   "metadata": {},
   "source": [
    "The posterior can also be defined as\n",
    "\n",
    "\\begin{align}\n",
    "p(f_* | X^*,X, y) &= \\int p(f_*,f| X^*,X,y)df \\quad  \\textrm{marginalize }f\\\\\n",
    "p(f_* | X^*,X, y) &= \\int p(f_*| X^*,X,f,y)p(f|X^*,X, y)df \\quad \\textrm{reserve chain rule}\\\\\n",
    "p(f_* | X^*,X, y) &= \\int p(f_* | X^*,X,f) p(f| X^*,X, y) df \\quad \\textrm{drop irrelevant y}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b61e1",
   "metadata": {},
   "source": [
    "**Sparse Gaussian Process Regression**\n",
    "\n",
    "To avoid costly matrix inversion of $n \\times n$-matrix, Authors in {cite}```Titsias2009``` propose to use the so called _inducing variables_. They assume the existence a set of $m$ inducing variables $f_m$ with corresponding inputs $X_m$ that will discribe our model well enough as if we used all datapoints $n$. Note that inducing variables $f_m$ evaluated at the pseudo-inputs $X_m$, which are independent from the training inputs. We will define the approximation posterior as\n",
    "$$\n",
    "q(f_*) = \\int p(f_* | f_m) \\phi(f_m) df_m ,\n",
    "$$\n",
    "\n",
    "where $\\phi(f_m)$ is the approximation of the intractable $p(f_m|y)$ and is defined by\n",
    "\n",
    "$$\n",
    "\\phi(f_m) = \\mathcal{N}(f_m | \\mu_m, A_m).\n",
    "$$\n",
    "Here is $\\mu_m$ the mean and $A_m$ the covariance matrix. The goal is to find optimal values for the mean $\\mu_m$ and covarince $A_m$ as well as the optimal location of the inducing inputs $X_m$. The mean and covariance matrix of the Guassian Process is defined by\n",
    "\n",
    "\\begin{align}\n",
    "q(f_*) =& \\mathcal{N}(f_* | \\mu_*^q, \\Sigma_*^q) \\\\\n",
    "\\mu_*^q =& K_{*m} K_{mm}^{-1} \\mu_m  \\\\\n",
    "\\Sigma_*^q =& K_{**} - K_{*m} K_{mm}^{-1} K_{m*} + K_{*m} K_{mm}^{-1} \\mathbf{A}_m K_{mm}^{-1} K_{m*} \n",
    "\\end{align}\n",
    "\n",
    "where $K_{mm}=K(X_m,X_m)$, $K_{*m}=K(X_*,X_m)$ and $K_{m*}=K(X_m,X_*)=K_{*m}^T$. Now we can see, that we have to do the inversion of a $m \\times m$ matrix if we found the optimal values for $\\mu_m, A_m$ and $X_m$. For optimization of this we will use a variational approach by minimizing the Kullback-Leibler (KL) divergence between the approximate $q(f)$ and the exact posterior $p(f|y)$ over latent variables $f$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a7db0",
   "metadata": {},
   "source": [
    "The minimization of KL divergence is equivalent to maximization of a lower bound $\\mathcal{L}(\\mu_m, A_m,X_m)$ on the true log marginal likelihood $\\log p(y).$ This lower bound can be optimized by analytically solving for $μ_m$ and $A_m$. The resulting lower bound after optimization is a function of $X_m$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(X_m)= \\log \\mathcal{N}(y|0,\\sigma^2_y I+ Q_{nn})-\\frac{1}{2 \\sigma^2_y}\\textrm{Tr}(K_{nn}-Q_{nn})\n",
    "$$\n",
    "with $Q_{nn}=K_{nm}K_{mm}^{-1}K_{mn}$. The first term on the RHS is the approximate log likelihood term and the second one is a regularization term which result of using variational approach.The second term can be interpreted as minimizing the error predicting $f$ from inducing variables $f_m$. The better the variables $f_m$ represent the function to be modeled the smaller this error will be. So the optimization will try to find the best postions for the inducing inputs $X_m$. With optimal inducing inputs $X_m$ we ca analytically find values for $\\mu_m$ and $A_m$.\n",
    "\n",
    "$$\n",
    "\\mu_m = \\frac{1}{\\sigma_y^2} K_{mm} \\Sigma K_{mn} y\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_m = K_{mm} \\Sigma K_{mm} \n",
    "$$\n",
    "\n",
    "where $\\Sigma = (K_{mm}+\\sigma^2_yK_{mm}K_{nm}^{-1})$ is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa93b4f",
   "metadata": {},
   "source": [
    "For a more detailed description of sparse and variational GP, please refer to {cite}```Leibfried2021```.\n",
    "\n",
    "**Visualization of SVGP training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77a435",
   "metadata": {},
   "source": [
    "```{figure} svgpsamples.gif\n",
    "---\n",
    "width: 700px\n",
    "name: directive-fig\n",
    "---\n",
    "Evolution of the inducing points. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6acb5cf",
   "metadata": {},
   "source": [
    "```{figure} svgppreds.gif \n",
    "---\n",
    "width: 700px\n",
    "name: directive-fig\n",
    "---\n",
    "Evolution of the model points displayed as $\\mu_*\\pm 2\\sigma_*^2$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1fcd63",
   "metadata": {},
   "source": [
    "As the training goes on, some inducing points are moved outside of the training boundaries and their values are set to mean ($0$) which relating to a small variance where we do not have any datapoints. To avoid such a behavior, we can set \"whiten = True\" the SVGP, which a variable transformation and should not effect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37eed2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ConstantKernel, WhiteKernel\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gpflow.ci_utils import ci_niter\n",
    "import time\n",
    "\n",
    "\n",
    "# load data\n",
    "housing_dataset = fetch_california_housing()\n",
    "\n",
    "# use first feature (median income) as additional label\n",
    "X = housing_dataset[\"data\"]#[:, 1:]\n",
    "#y = np.stack((housing_dataset[\"target\"], housing_dataset[\"data\"][:, 0]), axis=1)\n",
    "y = housing_dataset[\"target\"].reshape(-1, 1) #als Übung\n",
    "# split data (50% train and 50% test)\n",
    "# training: 10320 samples, test: 10320 samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# scale data\n",
    "scalerX, scalery = StandardScaler(), StandardScaler()\n",
    "X_train_ = scalerX.fit_transform(X_train)\n",
    "y_train_ = scalery.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb72b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X_train_)\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "X = X_train_\n",
    "Y = y_train_ \n",
    "data = (X, Y)\n",
    "\n",
    "\n",
    "# Choose equidistance locations for initial values for inducing locations.\n",
    "M = 5  # Number of inducing locations\n",
    "idx = [int(i) for i in list(np.linspace(0, N, M, endpoint=False))]\n",
    "Z = X[idx, :].copy()  # Initialize inducing locations to the first M inputs in the dataset\n",
    "\n",
    "\n",
    "# Create SVGP model.\n",
    "start_time = time.time()\n",
    "kernel = gpflow.kernels.SquaredExponential()\n",
    "\n",
    "# whiten=True toggles the fₛ=Lu parameterization.\n",
    "# whiten=False uses the original parameterization. see e.g.: https://towardsdatascience.com/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7\n",
    "m = gpflow.models.SVGP(kernel, gpflow.likelihoods.Gaussian(), Z, num_data=N, whiten=True)\n",
    "# Enable the model to train the inducing locations.\n",
    "gpflow.set_trainable(m.inducing_variable, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555b31f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -20222.25999866008\n",
      "1000 -14482.374430027356\n",
      "2000 -13003.58781728674\n",
      "3000 -10188.432377988136\n",
      "4000 -11492.638190511052\n",
      "5000 -12639.439790135742\n",
      "6000 -10170.374511297925\n",
      "7000 -9522.218133477993\n",
      "8000 -10660.730242307336\n",
      "9000 -10301.516341396069\n",
      "10000 -10159.830518487901\n",
      "11000 -7729.894608357307\n",
      "12000 -10375.753024366632\n",
      "13000 -10936.726274038821\n",
      "14000 -10213.366064974267\n",
      "15000 -9772.123785135893\n",
      "16000 -11289.154383840001\n",
      "17000 -8683.425254500584\n",
      "18000 -9472.522113928679\n",
      "19000 -10802.898018417705\n",
      "20000 -10695.549488236797\n",
      "21000 -8464.78685919514\n",
      "22000 -10073.512543821193\n",
      "23000 -7624.815088496258\n",
      "24000 -10690.345525680475\n",
      "25000 -12232.215858685458\n",
      "26000 -8769.56863686641\n",
      "27000 -8991.181014518046\n",
      "28000 -11150.114254287839\n",
      "29000 -12494.928158969995\n"
     ]
    }
   ],
   "source": [
    "# Set batch size.\n",
    "minibatch_size = 100\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).repeat().shuffle(N)\n",
    "\n",
    "def run_adam(model, iterations):\n",
    "    \"\"\"\n",
    "    Utility function running the Adam optimizer\n",
    "    :param model: GPflow model\n",
    "    :param interations: number of iterations\n",
    "    \"\"\"\n",
    "    # Create an Adam Optimizer action\n",
    "    logf = []\n",
    "    train_iter = iter(train_dataset.batch(minibatch_size))\n",
    "    training_loss = model.training_loss_closure(train_iter, compile=True)\n",
    "    optimizer = tf.optimizers.Adam()\n",
    "\n",
    "    @tf.function\n",
    "    def optimization_step():\n",
    "        optimizer.minimize(training_loss, model.trainable_variables)\n",
    "\n",
    "    for step in range(iterations):\n",
    "        optimization_step()\n",
    "        if step % 1000 == 0:\n",
    "            elbo = -training_loss().numpy()\n",
    "            print(step, elbo)\n",
    "            logf.append(elbo)\n",
    "    return logf\n",
    "\n",
    "# Specify the number of optimization steps.\n",
    "maxiter = ci_niter(30000)\n",
    "\n",
    "# Perform optimization.\n",
    "logf = run_adam(m, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f330ed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 36.33657693862915 seconds ---\n",
      "\n",
      "r2 = 0.6395, mse = 0.48, mae = 0.51\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "y_pred ,y_pred_v = m.predict_y(scalerX.transform(X_test))\n",
    "#y_pred = pca.inverse_transform(y_pred)\n",
    "y_pred = scalery.inverse_transform(y_pred)\n",
    "#y_pred = np.rint(y_pred) # labels should be integer valued\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"---SVGP execution in %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\nr2 = {:.4f}, mse = {:.2f}, mae = {:.2f}\".format(r2, mse, mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e77ae",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    ":style: plain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79565f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
