
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Fundamentals of Probability Theory &#8212; Advanced Machine Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="canonical" href="https://probabilistic-ml.github.io/lecture-notes/preface.html/01_fund/fundprob.html" />
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Bayesian vs. Frequentists View" href="stat.html" />
    <link rel="prev" title="Notation" href="../00_preface/notation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Advanced Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../welcome.html">
   Welcome to the AML lecture 2021/22
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Preface
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00_preface/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00_preface/about.html">
   About the Authors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00_preface/python.html">
   Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00_preface/notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Fundamentals
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Fundamentals of Probability Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stat.html">
   2. Bayesian vs. Frequentists View
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bayes.html">
   3. MLE, MAP &amp; Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linregr.html">
   4. An illustrative example for MLE and MAP: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="opt.html">
   5. Optimization Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLworkflow.html">
   6. Machine Learning Workflow
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Probabilistic Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../02_probML/motivation.html">
   7. Motivation of Probabilistic Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02_probML/kernelmethods.html">
   8. Kernel-based Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02_probML/overview.html">
   14. Overview of Further Probabilistic Models
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../03_appl/BO.html">
   15. Bayesian Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03_appl/uncertainty.html">
   16. Quantification of Design Uncertainty
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03_appl/RL.html">
   17. Data-efficient Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../04_appendix/placeholder.html">
   Appendix A
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org/"> <img alt="Jupyter Book" src="https://jupyterbook.org/badge.svg" width="100"> </a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/01_fund/fundprob.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Probabilistic-ML/lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-spaces">
   1.1. Probability Spaces
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-probability-spaces">
     1.1.1. Discrete Probability Spaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-probability-spaces">
     1.1.2. Continuous Probability Spaces
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-variables">
   1.2. Random Variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#independence">
   1.3. Independence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#important-probability-distributions">
   1.4. Important Probability Distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-distributions">
     1.4.1. Discrete Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bernoulli-distribution">
       1.4.1.1. Bernoulli Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#binomial-distribution">
       1.4.1.2. Binomial Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#geometric-distribution">
       1.4.1.3. Geometric Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#poisson-distribution">
       1.4.1.4. Poisson Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discrete-uniform-distribution">
       1.4.1.5. Discrete Uniform Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-distributions">
     1.4.2. Continuous Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#normal-distribution">
       1.4.2.1. Normal Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#beta-distribution">
       1.4.2.2. Beta Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#uniform-distribution">
       1.4.2.3. Uniform Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gamma-distribution">
       1.4.2.4. Gamma Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exponential-distribution">
       1.4.2.5. Exponential Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#laplace-distribution">
       1.4.2.6. Laplace Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cauchy-distribution">
       1.4.2.7. Cauchy Distribution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#essential-theorems">
   1.5. Essential Theorems
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- #region -->
<div class="section" id="fundamentals-of-probability-theory">
<h1><span class="section-number">1. </span>Fundamentals of Probability Theory<a class="headerlink" href="#fundamentals-of-probability-theory" title="Permalink to this headline">¶</a></h1>
<p>In the present section, we define the basic terms of probability theory and statistics. Moreover, we state the most common examples of discrete and continuous probability distributions.</p>
<p>The content follows the textbooks</p>
<p><a class="reference external" href="https://www.springer.com/de/book/9783642548567">“Statistik für Ingenieure -
Wahrscheinlichkeitsrechnung und Datenauswertung endlich verständlich”</a></p>
<p>by Aeneas Rooch <span id="id1">[<a class="reference internal" href="#id8">Roo14</a>]</span> and</p>
<p><a class="reference external" href="https://www.springer.com/de/book/9783662541616">“Grundlagen der
Wahrscheinlichkeitsrechnung
und Statistik - Eine Einführung für Studierende
der Informatik, der Ingenieur- und
Wirtschaftswissenschaften”</a></p>
<p>by Erhard Cramer and Udo Kamps <span id="id2">[<a class="reference internal" href="#id9">CK17</a>]</span>.</p>
<p>The goal is to avoid unnecessarily complex mathematical backround, but to provide the required framework to understand the subsequent machine learning methods. Nevertheless, for the the sake of completeness, additional references are given from time to time. A more profound mathematical theory can for example be found in <a class="reference external" href="https://www.springer.com/de/book/9783642360183">“Wahrscheinlichkeitstheorie”</a> by Achim Klenke <span id="id3">[<a class="reference internal" href="#id6">Kle13</a>]</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All three books are available free of charge via <a class="reference external" href="https://hs-niederrhein.digibib.net/">DigiBib</a>.</p>
</div>
<div class="section" id="probability-spaces">
<h2><span class="section-number">1.1. </span>Probability Spaces<a class="headerlink" href="#probability-spaces" title="Permalink to this headline">¶</a></h2>
<div class="tip admonition" id="def-samplespace">
<p class="admonition-title">Definition</p>
<p>In order to model the outcome of a random experiment, we denote by <span class="math notranslate nohighlight">\(\Omega\)</span> the <strong>sample space</strong> of all possible outcomes, i.e.,</p>
<div class="math notranslate nohighlight">
\[\Omega = \{ \omega ~|~ \omega \text{ is a possible outcome of the random experiment}\}.\]</div>
<p>Accordingly, each element <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span> is called an <strong>outcome</strong>. A subset <span class="math notranslate nohighlight">\(A\)</span> of <span class="math notranslate nohighlight">\(\Omega\)</span> of possible outcomes is called an <strong>event</strong>. If <span class="math notranslate nohighlight">\(A\)</span> contains only a single outcome <span class="math notranslate nohighlight">\(\omega\)</span>, i.e., <span class="math notranslate nohighlight">\(A=\{\omega\}\)</span> for some <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span>, <span class="math notranslate nohighlight">\(A\)</span> is also called an elementary event.</p>
</div>
<p>If we model the rolling of an ordinary cubic dice, the sample space</p>
<div class="math notranslate nohighlight" id="equation-ex-dice">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-ex-dice" title="Permalink to this equation">¶</a></span>\[\Omega= \{1, 2, 3, 4, 5, 6\}\]</div>
<p>is given by the 6 possible outcomes. The event <span class="math notranslate nohighlight">\(A\)</span> of rolling an even number is given by <span class="math notranslate nohighlight">\(A = \{2, 4, 6\} \subset \Omega\)</span> and the elementary event of rolling a six is given by <span class="math notranslate nohighlight">\(A=\{6\}\)</span>.</p>
<div class="section" id="discrete-probability-spaces">
<h3><span class="section-number">1.1.1. </span>Discrete Probability Spaces<a class="headerlink" href="#discrete-probability-spaces" title="Permalink to this headline">¶</a></h3>
<div class="tip admonition" id="def-discrdistr">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(\Omega\)</span> be a <em>finite or countable</em> sample space and denote by <span class="math notranslate nohighlight">\(\mathcal{P}(\Omega) = \{A~|~A \subset \Omega\}\)</span> the set of all subsets of <span class="math notranslate nohighlight">\(\Omega\)</span> (the so-called power set). Moreover, let <span class="math notranslate nohighlight">\(p: \Omega \rightarrow [0, 1]\)</span> be a map such that <span class="math notranslate nohighlight">\(\sum_{\omega \in \Omega} p(\omega) = 1\)</span>. Then, the map <span class="math notranslate nohighlight">\(P: \mathcal{P}(\Omega) \rightarrow [0,1]\)</span> given by</p>
<div class="math notranslate nohighlight">
\[ P(A) := \sum_{\omega \in A} p(\omega) \quad \text{for } A \in \mathcal{P}(\Omega)\]</div>
<p>is called a <strong>discrete probability measure</strong> or a <strong>discrete probability distribution</strong>. The triple <span class="math notranslate nohighlight">\((\Omega, \mathcal{P}(\Omega), P)\)</span> or briefly <span class="math notranslate nohighlight">\((\Omega, P)\)</span> is called a <strong>discrete probability space</strong>.</p>
</div>
<ul class="simple">
<li><p>A probability measure <span class="math notranslate nohighlight">\(P\)</span> assigns to each possible event a probability between 0 (“impossible”) to 1 (“sure”).</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> is completely characterized by the elementary probabilities (i.e., the probabilities of elementary events specified by <span class="math notranslate nohighlight">\(p\)</span>) in the case of discrete probability distributions (by definition).</p></li>
<li><p>The condition <span class="math notranslate nohighlight">\(\sum_{\omega \in \Omega} p(\omega) = 1\)</span> guarantees that <span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span>. In other words, it has to be sure that the outcome of a random experiment is indeed in <span class="math notranslate nohighlight">\(\Omega\)</span> and moreover, <span class="math notranslate nohighlight">\(P(\Omega) &gt; 1\)</span> would make no sense in terms of probabilities.</p></li>
</ul>
<p>Assumed that we are dealing with a fair dice as before, it is reasonable to define <span class="math notranslate nohighlight">\(p(\omega):=\frac{1}{6}\)</span> for each <span class="math notranslate nohighlight">\(\omega=1, \dots,6\)</span>. Hence, each outcome of a dice roll is each likely. Consequently, the probability of rolling an even number is</p>
<div class="math notranslate nohighlight">
\[P(\{2, 4, 6\}) = \sum_{\omega \in \{2, 4, 6\}} p(\omega) = 3 \cdot \frac{1}{6} = 0.5\]</div>
<p>as expected.</p>
<div class="tip admonition" id="cor-discrdistr">
<p class="admonition-title">Corollary</p>
<p>A <a class="reference internal" href="#def-discrdistr"><span class="std std-ref">discrete probability measure</span></a> has the following properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(0 \le P(A) \le 1\)</span> for each event <span class="math notranslate nohighlight">\(A \in \mathcal{P}(\Omega)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> is <span class="math notranslate nohighlight">\(\sigma\)</span>-additive, i.e., for pairwise disjoint events <span class="math notranslate nohighlight">\(A_i\)</span>, <span class="math notranslate nohighlight">\(i \in \mathbb{N}\)</span>, it holds</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(\cup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i).\]</div>
</div>
<ul class="simple">
<li><p>The statements in the preceding corollary are also called <strong>Kolmogorov axioms</strong>.</p></li>
<li><p>The term “pairwise disjoint” means that two arbitrary events do not have any common elements. For example, the events <span class="math notranslate nohighlight">\(\{2, 4, 6\}\)</span> and <span class="math notranslate nohighlight">\(\{1, 3\}\)</span> are disjoint, but the events <span class="math notranslate nohighlight">\(\{2, 4, 6\}\)</span> and <span class="math notranslate nohighlight">\(\{2, 3\}\)</span> are not, since the share the outcome <span class="math notranslate nohighlight">\(2\)</span>.</p></li>
<li><p>The last statement also holds true for a <em>finite number</em> of sets <span class="math notranslate nohighlight">\(A_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span>, by simply choosingchoosing <span class="math notranslate nohighlight">\(A_i = \emptyset\)</span> (empty set) for <span class="math notranslate nohighlight">\(i &gt; n\)</span>. If we consider only two disjoint sets <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span>, it follows that <span class="math notranslate nohighlight">\(P(A_1 \cup A_2) = P(A_1) + P(A_2)\)</span>. This means that the probability that the event <span class="math notranslate nohighlight">\(A_1\)</span> or the event <span class="math notranslate nohighlight">\(A_2\)</span> occurs equals the sum of the probabilities, which is intuitive.</p></li>
</ul>
</div>
<div class="section" id="continuous-probability-spaces">
<h3><span class="section-number">1.1.2. </span>Continuous Probability Spaces<a class="headerlink" href="#continuous-probability-spaces" title="Permalink to this headline">¶</a></h3>
<p>It turns out that the definition of probability spaces requires a different approach in the case of sample spaces that contain <em>uncountably</em> many outcomes. For example, the sample space could be given by the real numbers (<span class="math notranslate nohighlight">\(\Omega = \mathbb{R}\)</span>) or a higher dimensional space (e.g. <span class="math notranslate nohighlight">\(\Omega = \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d &gt;= 2\)</span>). Indeed, the definition of (probability) measures on arbitrary sample spaces turns out to be a complex mathematical problem which is the foundation of <strong>measure theory</strong>. This theory introduces so-called <span class="math notranslate nohighlight">\(\sigma\)</span>-algebras which specify the measurable events, i.e., the events for which it is possible to assign a probability without generating any inconsistencies. An introduction can be found in the first chapter of <a class="reference external" href="https://www.springer.com/de/book/9783642360183">“Wahrscheinlichkeitstheorie”</a> by Achim Klenke. Measure theory is the foundation of very powerful results, since it enables mathematicians to define probability measures even on infinite dimensional sample spaces such as spaces of functions which lead to so-called stochastic processes. A special case are <strong>Gaussian processes</strong> which turn out to be very useful in the context of machine learning and are an essential part of this lecture.</p>
<p>Luckily, we do not necessarily need to consider measure theory in detail for our purposes. For the mentioned cases (<span class="math notranslate nohighlight">\(\mathbb{\Omega} = [0, 1]\)</span> or <span class="math notranslate nohighlight">\(\Omega = \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d &gt;= 1\)</span>), we can use ordinary integrals in order to define probabilities at least on “nice” events:</p>
<ul class="simple">
<li><p>Set <span class="math notranslate nohighlight">\(C := \big\{ [a_1, b_1] \times [a_2, b_2] \times \cdots \times [a_d, b_d]~\big|~-\infty \le a_i \le b_i \le \infty, ~i = 1, \dots, d\big\}\)</span>. An event <span class="math notranslate nohighlight">\(A \in C\)</span> is simply a box.</p></li>
<li><p>For d=1 we obtain an interval <span class="math notranslate nohighlight">\(A = [a_1, b_1]\)</span>.</p></li>
<li><p>For d=2 we get a rectangle <span class="math notranslate nohighlight">\(A = [a_1, b_1] \times [a_2, b_2] \subset \mathbb{R}^2\)</span>.</p></li>
<li><p>In measury theory, <span class="math notranslate nohighlight">\(C\)</span> is a so-called generating system of the <strong>Borel</strong> <span class="math notranslate nohighlight">\(\sigma\)</span>-<strong>algebra</strong> <span class="math notranslate nohighlight">\(\mathcal{B}(\mathbb{R}^d)\)</span>.</p></li>
<li><p>The Borel <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra is the smallest collection of events with sufficiently nice properties which contains alle these boxes.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}(\mathbb{R}^d)\)</span> is fairly abstract. Just rembember that</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}(\mathbb{R}^d)\)</span> contains all events we would like / are able to assign a probability to,</p></li>
<li><p>there are subsets of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> which are not in <span class="math notranslate nohighlight">\(\mathcal{B}(\mathbb{R}^d)\)</span>, but we do not care, since they are not important.</p></li>
</ul>
</li>
</ul>
<div class="tip admonition" id="def-contdistr">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(\Omega = \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d \ge 1\)</span>, be the sample space and <span class="math notranslate nohighlight">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span> be an integrable non-negative function such that</p>
<div class="math notranslate nohighlight">
\[ \int_{\mathbb{R}^d} f(x)~dx = 1. \]</div>
<p>Then, the map <span class="math notranslate nohighlight">\(P: C \rightarrow [0, 1]\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[ P(A) := \int_{a_d}^{b_d} \cdots \int_{a_1}^{b_1} f(x) ~ dx \quad \text{for } A = [a_1, b_1] \times [a_2, b_2] \times \cdots \times [a_d, b_d] \in C\]</div>
<p>extends uniquely to <span class="math notranslate nohighlight">\(\mathcal{B}(\Omega)\)</span> (not part of the lecture) and this extension is called a <strong>continuous probability measure</strong> or a <strong>continuous probability distribution</strong>. <span class="math notranslate nohighlight">\(f\)</span> is called the <strong>probability density function</strong> (PDF) of <span class="math notranslate nohighlight">\(P\)</span> or briefly probabilty density or simply density. The triple <span class="math notranslate nohighlight">\((\Omega, \mathcal{B}(\Omega), P)\)</span> or briefly <span class="math notranslate nohighlight">\((\Omega, P)\)</span> is called a <strong>continuous probability space</strong>.</p>
</div>
<p>It holds</p>
<div class="math notranslate nohighlight">
\[ \int_{\mathbb{R}} \exp\big(-\frac{1}{2} x^2\big)~dx = \sqrt{2 \pi}.\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span> defined by <span class="math notranslate nohighlight">\(f(x):= \frac{1}{\sqrt{2 \pi}}~\exp\big(-\frac{1}{2} x^2\big)\)</span> for <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> defines a continuous probability distribution with density <span class="math notranslate nohighlight">\(f\)</span>. This distribution is the <strong>standard normal distribution</strong> (also denoted by <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>).</p>
<ul class="simple">
<li><p>As in the case of discrete probability spaces, a <a class="reference internal" href="#def-contdistr"><span class="std std-ref">continuous probability measure</span></a> fulfills the <strong><a class="reference internal" href="#cor-discrdistr"><span class="std std-ref">Kolmogorov axioms</span></a></strong>.</p></li>
<li><p>In order to unify the notation of discrete and continuous probability spaces, we denote a general probability space by <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> denotes a <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra (in our case either <span class="math notranslate nohighlight">\(\mathcal{P}(\Omega)\)</span> or <span class="math notranslate nohighlight">\(\mathcal{B}(\Omega)\)</span>).</p></li>
<li><p>Keep in mind that <span class="math notranslate nohighlight">\(f\)</span> is simply a non-negative function whose volume under the graph is exactly one and the probability of some event <span class="math notranslate nohighlight">\(A\)</span> is the volume under the graph of <span class="math notranslate nohighlight">\(f\)</span> restricted to <span class="math notranslate nohighlight">\(A\)</span>. In the plot below, <span class="math notranslate nohighlight">\(P([-2, 0]) \approx 0.48\)</span> is illustrated for a standard normal distribution. In other words, the probability to observe an outcome between -2 and 0 in a standard normally distributed experiment is approximately 48%.</p></li>
</ul>
<p><img alt="" src="../_images/gaussian_pdf.png" /></p>
</div>
</div>
<div class="section" id="random-variables">
<h2><span class="section-number">1.2. </span>Random Variables<a class="headerlink" href="#random-variables" title="Permalink to this headline">¶</a></h2>
<p>Imagine that we perform multiple independent random experiments by rolling repeatedly (<span class="math notranslate nohighlight">\(n\)</span>-times) a fair dice. The corresponding sample space is given by</p>
<div class="math notranslate nohighlight">
\[\Omega = \{ \omega = (\omega_1, \omega_2, \dots, \omega_n)~|~\omega_i \in \{1, 2, 3, 4, 5, 6\} \ \text{for } i=1, \dots,n \}.\]</div>
<p>Since the experiments are independent and we consider a fair dice, it is reasonable to define</p>
<div class="math notranslate nohighlight">
\[ p(\omega) = \frac{1}{6^n} \quad \text{for each } \omega \in \Omega\]</div>
<p>which results in a discrete probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{P}(\Omega), P)\)</span>. Eventually, we are not interested in events with respect to <span class="math notranslate nohighlight">\(\Omega\)</span>, but for example in the average outcome of the experiments or the number of times of rolling a six. Instead of modelling these experiments directly by redefining <span class="math notranslate nohighlight">\(\Omega\)</span> and <span class="math notranslate nohighlight">\(p\)</span>, it is very useful to apply the concept of random variables:</p>
<div class="tip admonition" id="def-rv">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> be a probabiliy space. A map <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d \ge 1\)</span>, is called a real-valued <strong>random variable</strong>, if</p>
<div class="math notranslate nohighlight">
\[ X^{-1}(A) := \{ \omega \in \Omega~|~X(\omega) \in A \} \in \mathcal{F}\]</div>
<p>for each <span class="math notranslate nohighlight">\(A \in \mathcal{B}(\mathbb{R}^d)\)</span> and the probability measure <span class="math notranslate nohighlight">\(P_X: \mathcal{B}(\Omega) \rightarrow [0, 1]\)</span> given by</p>
<div class="math notranslate nohighlight">
\[P_X(A) := P(X^{-1}(A)) \quad \text{for } A \in \mathcal{B}(\Omega)\]</div>
<p>is called the <strong>distribution of</strong> <span class="math notranslate nohighlight">\(X\)</span> <strong>under</strong> <span class="math notranslate nohighlight">\(P\)</span>. If <span class="math notranslate nohighlight">\(P_X\)</span> admits a <a class="reference internal" href="#def-contdistr"><span class="std std-ref">probability density</span></a>, then we denote the density by <span class="math notranslate nohighlight">\(f_X\)</span>. Furthermore, the cumulative distribution function of <span class="math notranslate nohighlight">\(P_X\)</span> is denoted by <span class="math notranslate nohighlight">\(F_X\)</span>.</p>
</div>
<p>In order to model a fair dice as a random variable simply set <span class="math notranslate nohighlight">\(\Omega= \{1, 2, 3, 4, 5, 6\}\)</span> as well as <span class="math notranslate nohighlight">\(p(\omega) = \frac{1}{6}\)</span> for <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span> to obtain the discrete probability space <span class="math notranslate nohighlight">\((\Omega, P)\)</span> and define <span class="math notranslate nohighlight">\(X\)</span> by</p>
<div class="math notranslate nohighlight">
\[X(\omega) = \omega \quad \text{for } \omega \in \Omega.\]</div>
<p>Observe that <span class="math notranslate nohighlight">\(X\)</span> does only take values in <span class="math notranslate nohighlight">\(\{1, 2, 3, 4, 5, 6\}\)</span>. Hence, <span class="math notranslate nohighlight">\(X\)</span> maps to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, but <span class="math notranslate nohighlight">\(P_X(\mathbb{R} \backslash \{1, 2, 3, 4, 5, 6\}) = 0\)</span>. <span class="math notranslate nohighlight">\(\{1, 2, 3, 4, 5, 6\}\)</span> is called the <strong>support</strong> of <span class="math notranslate nohighlight">\(P_X\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(d &gt; 1\)</span>, <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}^d\)</span> is also called a multivariate random variable or random vector. In this case, <span class="math notranslate nohighlight">\(X\)</span> is a random variable if and only if each component <span class="math notranslate nohighlight">\(X_i: \Omega \rightarrow \mathbb{R}\)</span> is a scalar random variable.</p></li>
<li><p>If <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> is a <a class="reference internal" href="#def-discrdistr"><span class="std std-ref">discrete probability space</span></a>, <strong>each</strong> map <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}^d\)</span> is a random variable, since <span class="math notranslate nohighlight">\(\mathcal{F} = \mathcal{P}(\Omega)\)</span> contains all subsets of <span class="math notranslate nohighlight">\(\Omega\)</span>. Moreover, note that we can identify <span class="math notranslate nohighlight">\(P_X\)</span> with a discrete probability distribution (as seen in the example of a dice), since <span class="math notranslate nohighlight">\(X\)</span> can take at most countably many distinct values in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>. In this case, we denote the corresponding elementary probabilities by <span class="math notranslate nohighlight">\(p_X\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> is a <a class="reference internal" href="#def-contdistr"><span class="std std-ref">continuous probability space</span></a>, it can be shown that at least each <strong>continuous</strong> map <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}^d\)</span> is a random variable.</p></li>
</ul>
<p>A very important concept is the expectation of random variables:</p>
<div class="tip admonition" id="def-exp">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d \ge 1\)</span>, be a random variable on some probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span>. The <strong>expectation</strong>, <strong>expected value</strong> or <strong>mean</strong> of X is defined by</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}(X) := \sum_{x \in X(\Omega)} x ~ p_X(x) \]</div>
<p>for discrete probability spaces, if the sum if well-defined, as well as by</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}(X) := \int_{\mathbb{R}^d} x ~ f_X(x)~dx \]</div>
<p>if <span class="math notranslate nohighlight">\(P_X\)</span> admits a probability density and the integral is well-defined. Sometimes <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span> is denoted by <span class="math notranslate nohighlight">\(\mu\)</span> or <span class="math notranslate nohighlight">\(\mu_X\)</span>.</p>
</div>
<p>The expectation of rolling a fair dice is given by</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^6 i ~ p(i) = \sum_{i=1}^6 i ~ \frac{1}{6} = \frac{21}{6} = 3.5\]</div>
<p>In many cases, we need to compute the mean of a transformed random variable. For this purpose, the following results will be useful:</p>
<div class="important admonition" id="thm-trans">
<p class="admonition-title">Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d \ge 1\)</span>, be a random variable and <span class="math notranslate nohighlight">\(g: \mathbb{R}^d \rightarrow \mathbb{R}^k\)</span> a function. Then</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}(g(X)) = \sum_{x \in X(\Omega)} g(x) ~ p_X(x) \]</div>
<p>for discrete probability spaces and</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}(g(X)) := \int_{\mathbb{R}^d} g(x) ~ f_X(x)~dx \]</div>
<p>if <span class="math notranslate nohighlight">\(P_X\)</span> admits a probability density.</p>
</div>
<p>At this point, we are somewhat imprecise. Indeed, the transformation <span class="math notranslate nohighlight">\(g\)</span> needs to be sufficiently nice, but at this point we neglect additional assumptions.</p>
<p>The remaining part of this section is a little bit <strong>more involved and not necessarily required</strong>. Nevertheless, we state these results in view of a better understanding of <a class="reference internal" href="#def-multnormal"><span class="std std-ref">multivariate normal distributions</span></a>.</p>
<p>In use of the above theorem, we are able to define the covariance matrix of multivariate random variables:</p>
<div class="tip admonition" id="def-cov">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(Y: \Omega \rightarrow \mathbb{R}^k\)</span> be random variables. The <strong>covariance matrix</strong> between X and Y is defined by</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) := \mathbb{E}\big((X - \mathbb{E}(X))(Y - \mathbb{E}(Y))^T \big)\]</div>
<p>If <span class="math notranslate nohighlight">\(Y = X\)</span> the definition yields the <strong>covariance matrix of</strong> X, i.e.,</p>
<div class="math notranslate nohighlight">
\[ \text{Cov}(X) := \text{Cov}(X, X).\]</div>
</div>
<ul class="simple">
<li><p>From two random variables <span class="math notranslate nohighlight">\(X\)</span> with values in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> with values in <span class="math notranslate nohighlight">\(\mathbb{R}^k\)</span>, we can define a new single random variable <span class="math notranslate nohighlight">\(Z = (X, Y)\)</span> with values in <span class="math notranslate nohighlight">\(\mathbb{R}^{d + k}\)</span> by stacking the two vectors. Note that we need to consider <span class="math notranslate nohighlight">\(P_Z\)</span> in order to apply the <a class="reference internal" href="#thm-trans"><span class="std std-ref">transformation theorem</span></a> and to compute <span class="math notranslate nohighlight">\(\text{Cov}(X, Y)\)</span>. The distribution of <span class="math notranslate nohighlight">\(Z\)</span> is called the <strong>joint distribution</strong> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>Note that in general <span class="math notranslate nohighlight">\(\text{Cov}(X, Y)\)</span> is indeed a matrix of size <span class="math notranslate nohighlight">\(d \times k\)</span>. This matrix contains the pairwise covariances of all components of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, i.e.,</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\text{Cov}(X, Y) = \begin{pmatrix} \text{Cov}(X_1, Y_1) &amp; \text{Cov}(X_1, Y_2) &amp; \cdots &amp; \text{Cov}(X_1, Y_k) \\
\text{Cov}(X_2, Y_1) &amp; \text{Cov}(X_2, Y_2) &amp; \cdots &amp; \text{Cov}(X_2, Y_k) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\text{Cov}(X_d, Y_1) &amp; \text{Cov}(X_d, Y_2) &amp; \cdots &amp; \text{Cov}(X_d, Y_k)
\end{pmatrix}
\end{align*}\]</div>
<ul class="simple">
<li><p>In the case <span class="math notranslate nohighlight">\(d=1\)</span>, <span class="math notranslate nohighlight">\(\text{Cov}(X) \in \mathbb{R}\)</span> is simply called the <strong>variance of</strong> <span class="math notranslate nohighlight">\(X\)</span> which is also denoted by <span class="math notranslate nohighlight">\(\sigma^2\)</span> or <span class="math notranslate nohighlight">\(\sigma_X^2\)</span>. Furthermore, <span class="math notranslate nohighlight">\(\sigma := \sigma_X := \sqrt{\sigma_X^2}\)</span> is called the <strong>standard deviation</strong> of <span class="math notranslate nohighlight">\(X\)</span>. It holds</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\sigma^2 = \int (x - \mathbb{E}(X))^2 ~ f_X(x)~dx.\]</div>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(d=k=1\)</span>, the <strong>correlation</strong> <span class="math notranslate nohighlight">\(\text{Corr}(X, Y)\)</span> (also denoted <span class="math notranslate nohighlight">\(\rho\)</span> or <span class="math notranslate nohighlight">\(\rho_{X, Y}\)</span>) is given by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ -1 \le \text{Corr}(X, Y) := \frac{\text{Cov}(X, Y)}{\sigma_X ~ \sigma_Y} \le 1\]</div>
<p>Note that the correlation is only defined if the variances of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are non-zero.</p>
<p>Expectation and covariance have some nice properties:</p>
<div class="important admonition" id="lem-expprop">
<p class="admonition-title">Lemma</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random variables and <span class="math notranslate nohighlight">\(a, b \in \mathbb{R}\)</span>. Then</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(a) = a\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(aX) = a~\mathbb{E}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(X + Y) = \mathbb{E}(X) + \mathbb{E}(Y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(|X + Y|) \le \mathbb{E}(|X|) + \mathbb{E}(|Y|)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(X \le Y\)</span> (i.e., <span class="math notranslate nohighlight">\(X(\omega) \le Y(\omega)\)</span> for each <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{E}(X) \le \mathbb{E}(Y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(|X|) = 0 ~ \Leftrightarrow ~ P(X \ne 0) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(X, Y) = \mathbb{E}(XY^T) - \mathbb{E}(X) \mathbb{E}(Y)^T.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(X) = 0 ~ \Leftrightarrow ~ P(X \ne \mathbb{E}(X)) = 0\)</span>. In particular, <span class="math notranslate nohighlight">\(\text{Cov}(a) = 0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(X, Y) = \text{Cov}(Y, X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(a, b) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(aX, bY) = ab~\text{Cov}(X, Y)\)</span></p></li>
</ol>
</div>
</div>
<div class="section" id="independence">
<h2><span class="section-number">1.3. </span>Independence<a class="headerlink" href="#independence" title="Permalink to this headline">¶</a></h2>
<p>Independence of random variables can be defined in multiple, but equivalent, ways. For this purpose, the cumulative distribution function is useful:</p>
<div class="tip admonition" id="def-cdf">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d \ge 1\)</span>, be a random variable on some probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span>. The function <span class="math notranslate nohighlight">\(F_X: \mathbb{R}^d \rightarrow [0, 1]\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[ F_X(x) := P_X((-\infty, x_1] \times \cdots \times (-\infty, x_d]) \quad \text{for } x=(x_1, \dots, x_d) \in \mathbb{R}^d\]</div>
<p>is called the <strong>cumulative distribution function</strong> (CDF) of <span class="math notranslate nohighlight">\(X\)</span> or <span class="math notranslate nohighlight">\(P_X\)</span>.</p>
</div>
<p>Now, we are able to define independence of events and random variables:</p>
<div class="tip admonition" id="def-indep">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> be a probability space. Then two events <span class="math notranslate nohighlight">\(A, B \in \mathcal{F}\)</span> are called <strong>independent</strong>, if</p>
<div class="math notranslate nohighlight">
\[ P(A \cap B) = P(A) P(B).\]</div>
<p>Let <span class="math notranslate nohighlight">\(X: \Omega \rightarrow \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(Y: \Omega \rightarrow \mathbb{R}\)</span> be two random variables. Then <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are called <strong>independent random variables</strong>, if the events <span class="math notranslate nohighlight">\(X^{-1}(A)\)</span> and <span class="math notranslate nohighlight">\(Y^{-1}(B)\)</span> are independent for all <span class="math notranslate nohighlight">\(A, B \in \mathcal{B}(\mathbb{R})\)</span>. This is equivalent to the property</p>
<div class="math notranslate nohighlight">
\[F_{(X, Y)}(x, y) = F_X(x) F_Y(y) \quad \text{for all } x,y \in \mathbb{R}\]</div>
<p>and if the corresponding densities exist to</p>
<div class="math notranslate nohighlight">
\[f_{(X, Y)}(x, y) = f_X(x) f_Y(y) \quad \text{for all } x,y \in \mathbb{R}.\]</div>
</div>
<p>Independence basically means that the occurrence of event A has no impact on the occurrence of event B or in terms of random variables, the outcomes of random variables should not impact each other. The definition of independent random variables generalizes easily to <span class="math notranslate nohighlight">\(d &gt; 1\)</span>.</p>
<div class="important admonition" id="lem-indep">
<p class="admonition-title">Lemma</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two independent random variables. Then</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(X, Y) = 0\)</span></p></li>
</ol>
</div>
<p>If <span class="math notranslate nohighlight">\(\text{Cov}(X, Y) = 0\)</span>, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are called <strong>uncorrelated</strong>. Note that independent random variables are always uncorrelated, but uncorrelated random variables are not necessarily independent (see e.g. Bemerkung C 5.21 in <span id="id4">[<a class="reference internal" href="#id9">CK17</a>]</span>).</p>
<p>In many cases, we are able to observe outcomes of independent random variables with the same distribution (so-called <strong>independent identical distributed</strong> (i.i.d.) random variables), but do not know the underlying distribution exactly. Thus, we would like to make conclusions about the distribution based on our observations. In this way, our probabilistic definitions are linked to statistics and estimators. For example, think of rolling a dice multiple times and initially we do not know if the dice is fair or not. We have shown that a fair dice has an expectation of <span class="math notranslate nohighlight">\(3.5\)</span>. Thus, it is reasonable to verify this expectation in use of the given observations. If the sample mean (see below) of many independent experiments is not sufficiently close to <span class="math notranslate nohighlight">\(3.5\)</span>, we may reject the hypothesis that the dice is fair. This procedure is called <strong>hypothesis testing</strong> in statistics.</p>
<p>Assume that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are two random variables and that <span class="math notranslate nohighlight">\(x_1, \dots, x_n\)</span> and <span class="math notranslate nohighlight">\(y_1, \dots, y_m\)</span> are samples of i.i.d. experiments with respect to <span class="math notranslate nohighlight">\(P_X\)</span> and <span class="math notranslate nohighlight">\(P_Y\)</span>, respectively. Then, the preceding definitions of expectation, covariance, variance, standard deviation and correlation have the following statistical counterparts:</p>
<ul class="simple">
<li><p>expectation:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\overline{x} := \frac{1}{n}~\sum_{i=1}^n x_i \quad \textbf{(sample mean)}\]</div>
<ul class="simple">
<li><p>covariance:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[s_{xy} := \frac{1}{n-1}~\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y}) \quad \textbf{(sample covariance)}\]</div>
<ul class="simple">
<li><p>variance:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[s^2_x := s_{xx} = \frac{1}{n-1}~\sum_{i=1}^n (x_i - \overline{x})^2 \quad \textbf{(sample variance)}\]</div>
<ul class="simple">
<li><p>standard deviation:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[s_x := \sqrt{s^2_x} \quad \textbf{(sample standard deviation)}\]</div>
<ul class="simple">
<li><p>correlation:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[r_{xy} := \frac{s_{xy}}{s_x s_y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i=1}^n (x_i - \overline{x})^2} \sqrt{\sum_{i=1}^n (y_i - \overline{y})^2}} \quad \textbf{(Pearson correlation coefficient)}\]</div>
</div>
<div class="section" id="important-probability-distributions">
<h2><span class="section-number">1.4. </span>Important Probability Distributions<a class="headerlink" href="#important-probability-distributions" title="Permalink to this headline">¶</a></h2>
<p>An extensive collection of important probability distributions can be found on <a class="reference external" href="https://en.wikipedia.org/wiki/List_of_probability_distributions">Wikipedia</a>. In the following subsections, we will shortly review some of them.</p>
<div class="section" id="discrete-distributions">
<h3><span class="section-number">1.4.1. </span>Discrete Distributions<a class="headerlink" href="#discrete-distributions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="bernoulli-distribution">
<h4><span class="section-number">1.4.1.1. </span>Bernoulli Distribution<a class="headerlink" href="#bernoulli-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="binomial-distribution">
<h4><span class="section-number">1.4.1.2. </span>Binomial Distribution<a class="headerlink" href="#binomial-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="geometric-distribution">
<h4><span class="section-number">1.4.1.3. </span>Geometric Distribution<a class="headerlink" href="#geometric-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="poisson-distribution">
<h4><span class="section-number">1.4.1.4. </span>Poisson Distribution<a class="headerlink" href="#poisson-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="discrete-uniform-distribution">
<h4><span class="section-number">1.4.1.5. </span>Discrete Uniform Distribution<a class="headerlink" href="#discrete-uniform-distribution" title="Permalink to this headline">¶</a></h4>
</div>
</div>
<div class="section" id="continuous-distributions">
<h3><span class="section-number">1.4.2. </span>Continuous Distributions<a class="headerlink" href="#continuous-distributions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="normal-distribution">
<span id="def-multnormal"></span><h4><span class="section-number">1.4.2.1. </span>Normal Distribution<a class="headerlink" href="#normal-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="beta-distribution">
<h4><span class="section-number">1.4.2.2. </span>Beta Distribution<a class="headerlink" href="#beta-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="uniform-distribution">
<h4><span class="section-number">1.4.2.3. </span>Uniform Distribution<a class="headerlink" href="#uniform-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="gamma-distribution">
<h4><span class="section-number">1.4.2.4. </span>Gamma Distribution<a class="headerlink" href="#gamma-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="exponential-distribution">
<h4><span class="section-number">1.4.2.5. </span>Exponential Distribution<a class="headerlink" href="#exponential-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="laplace-distribution">
<h4><span class="section-number">1.4.2.6. </span>Laplace Distribution<a class="headerlink" href="#laplace-distribution" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="cauchy-distribution">
<h4><span class="section-number">1.4.2.7. </span>Cauchy Distribution<a class="headerlink" href="#cauchy-distribution" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<div class="section" id="essential-theorems">
<h2><span class="section-number">1.5. </span>Essential Theorems<a class="headerlink" href="#essential-theorems" title="Permalink to this headline">¶</a></h2>
<p id="id5"><dl class="citation">
<dt class="label" id="id9"><span class="brackets">CK17</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>E. Cramer and U. Kamps. <em>Grundlagen der Wahrscheinlichkeitsrechnung und Statistik - Eine Einführung für Studierende der Informatik, der Ingenieur- und Wirtschaftswissenschaften</em>. Springer-Lehrbuch. Springer Spektrum, 4th edition, 2017. <a class="reference external" href="https://doi.org/10.1007/978-3-662-54161-6">doi:10.1007/978-3-662-54161-6</a>.</p>
</dd>
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id3">Kle13</a></span></dt>
<dd><p>A. Klenke. <em>Wahrscheinlichkeitstheorie</em>. Masterclass. Springer Spektrum, 3rd edition, 2013. <a class="reference external" href="https://doi.org/10.1007/978-3-642-36018-3">doi:10.1007/978-3-642-36018-3</a>.</p>
</dd>
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id1">Roo14</a></span></dt>
<dd><p>A. Rooch. <em>Statistik für Ingenieure - Wahrscheinlichkeitsrechnung und Datenauswertung endlich verständlich</em>. Springer-Lehrbuch. Springer Spektrum, 1st edition, 2014. <a class="reference external" href="https://doi.org/10.1007/978-3-642-54857-4">doi:10.1007/978-3-642-54857-4</a>.</p>
</dd>
</dl>
</p>
<!-- #endregion -->
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./01_fund"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../00_preface/notation.html" title="previous page">Notation</a>
    <a class='right-next' id="next-link" href="stat.html" title="next page"><span class="section-number">2. </span>Bayesian vs. Frequentists View</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By R. Vosshall, C. Bogoclu & N. Friedlich<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>