%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=0,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Fundamentals}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Advanced Machine Learning}
\date{Aug 17, 2021}
\release{}
\author{R.\@{} Vosshall, C.\@{} Bogoclu \& N.\@{} Friedlich}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{preface::doc}}


\sphinxAtStartPar
This is the preface…


\part{Fundamentals}


\chapter{Notation}
\label{\detokenize{fund/notation:notation}}\label{\detokenize{fund/notation::doc}}

\chapter{Fundamentals of Probability Theory}
\label{\detokenize{fund/fundprob:fundamentals-of-probability-theory}}\label{\detokenize{fund/fundprob::doc}}
\sphinxAtStartPar
In the present section, we define the basic terms of probability theory and statistics. Moreover, we state the most common examples of discrete and continuous probability distributions.

\sphinxAtStartPar
The content follows the textbooks

\sphinxAtStartPar
\sphinxhref{https://www.springer.com/de/book/9783642548567}{“Statistik für Ingenieure \sphinxhyphen{}
Wahrscheinlichkeitsrechnung und Datenauswertung endlich verständlich”}

\sphinxAtStartPar
by Aeneas Rooch and

\sphinxAtStartPar
\sphinxhref{https://www.springer.com/de/book/9783662541616}{“Grundlagen der
Wahrscheinlichkeitsrechnung
und Statistik \sphinxhyphen{} Eine Einführung für Studierende
der Informatik, der Ingenieur\sphinxhyphen{} und
Wirtschaftswissenschaften”}

\sphinxAtStartPar
by Erhard Cramer and Udo Kamps.

\sphinxAtStartPar
The goal is to avoid unnecessarily complex mathematical backround, but to provide the required framework to understand the subsequent machine learning methods. Nevertheless, for the the sake of completeness, additional references are given from time to time. A more profound mathematical theory can for example be found in \sphinxhref{https://www.springer.com/de/book/9783642360183}{“Wahrscheinlichkeitstheorie”} by Achim Klenke.

\sphinxAtStartPar
All three books are available free of charge via \sphinxhref{https://hs-niederrhein.digibib.net/}{DigiBib}.


\section{Probability Spaces}
\label{\detokenize{fund/fundprob:probability-spaces}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: In order to model the outcome of a random experiment, we denote by \(\Omega\) the \sphinxstylestrong{sample space} of all possible outcomes, i.e.,
\begin{equation*}
\begin{split}\Omega = \{ \omega ~|~ \omega \text{ is a possible outcome of the random experiment}\}.\end{split}
\end{equation*}
\sphinxAtStartPar
Accordingly, each element \(\omega \in \Omega\) is called an \sphinxstylestrong{outcome}. A subset \(A\) of \(\Omega\) of possible outcomes is called an \sphinxstylestrong{event}. If \(A\) contains only a single outcome \(\omega\), i.e., \(A=\{\omega\}\) for some \(\omega \in \Omega\), \(A\) is also called an elementary event.

\sphinxAtStartPar
\sphinxstylestrong{Example}: If we model the rolling of an ordinary cubic dice, the sample space
\begin{equation*}
\begin{split}\Omega= \{1, 2, 3, 4, 5, 6\}\end{split}
\end{equation*}
\sphinxAtStartPar
is given by the 6 possible outcomes. The event \(A\) of rolling an even number is given by \(A = \{2, 4, 6\} \subset \Omega\) and the elementary event of rolling a six is given by \(A=\{6\}\).


\subsection{Discrete Probability Spaces}
\label{\detokenize{fund/fundprob:discrete-probability-spaces}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: Let \(\Omega\) be a \sphinxstyleemphasis{finite or countable} sample space and denote by \(\mathcal{P}(\Omega) = \{A~|~A \subset \Omega\}\) the set of all subsets of \(\Omega\) (the so\sphinxhyphen{}called power set). Moreover, let \(p: \Omega \rightarrow [0, 1]\) be a map such that \(\sum_{\omega \in \Omega} p(\omega) = 1\). Then, the map \(P: \mathcal{P}(\Omega) \rightarrow [0,1]\) given by
\begin{equation*}
\begin{split} P(A) := \sum_{\omega \in A} p(\omega) \quad \text{for } A \in \mathcal{P}(\Omega)\end{split}
\end{equation*}
\sphinxAtStartPar
is called a \sphinxstylestrong{discrete probability measure} or a \sphinxstylestrong{discrete probability distribution}. The triple \((\Omega, \mathcal{P}(\Omega), P)\) or briefly \((\Omega, P)\) is called a \sphinxstylestrong{discrete probability space}.

\sphinxAtStartPar
\sphinxstylestrong{Remark}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
A probability measure \(P\) assigns to each possible event a probability between 0 (“impossible”) to 1 (“sure”).

\item {} 
\sphinxAtStartPar
\(P\) is completely characterized by the elementary probabilities (i.e., the probabilities of elementary events specified by \(p\)) in the case of discrete probability distributions (by definition).

\item {} 
\sphinxAtStartPar
The condition \(\sum_{\omega \in \Omega} p(\omega) = 1\) guarantees that \(P(\Omega) = 1\). In other words, it has to be sure that the outcome of a random experiment is indeed in \(\Omega\) and moreover, \(P(\Omega) > 1\) would make no sense in terms of probabilities.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example}: Assumed that we are dealing with a fair dice in Example 1.2, it is reasonable to define \(p(\omega):=\frac{1}{6}\) for each \(\omega=1, \dots,6\). Hence, each outcome of a dice roll is each likely. Consequently, the probability of rolling an even number is
\begin{equation*}
\begin{split}P(\{2, 4, 6\}) = \sum_{\omega \in \{2, 4, 6\}} p(\omega) = 3 \cdot \frac{1}{6} = 0.5\end{split}
\end{equation*}
\sphinxAtStartPar
as expected.

\sphinxAtStartPar
\sphinxstylestrong{Corollary}: As a direct consequence of Definition 1.3, a discrete probability measure has the following properties:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(0 \le P(A) \le 1\) for each event \(A \in \mathcal{P}(\Omega)\),

\item {} 
\sphinxAtStartPar
\(P(\Omega) = 1\),

\item {} 
\sphinxAtStartPar
\(P\) is \(\sigma\)\sphinxhyphen{}additive, i.e., for pairwise disjoint events \(A_i\), \(i \in \mathbb{N}\), it holds

\end{itemize}
\begin{equation*}
\begin{split}P(\cup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i).\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Remark}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The statements in Corollary 1.6 are also called \sphinxstylestrong{Kolmogorov axioms}.

\item {} 
\sphinxAtStartPar
The term “pairwise disjoint” means that two arbitrary events do not have any common elements. For example, the events \(\{2, 4, 6\}\) and \(\{1, 3\}\) are disjoint, but the events \(\{2, 4, 6\}\) and \(\{2, 3\}\) are not, since the share the outcome \(2\).

\item {} 
\sphinxAtStartPar
The last statement of Corollary 1.6 also holds true for a \sphinxstyleemphasis{finite number} of sets \(A_i\), \(i=1,\dots,n\), by simply choosingchoosing \(A_i = \emptyset\) (empty set) for \(i > n\). If we consider only two disjoint sets \(A_1\) and \(A_2\), it follows that \(P(A_1 \cup A_2) = P(A_1) + P(A_2)\). This means that the probability that the event \(A_1\) or the event \(A_2\) occurs equals the sum of the probabilities, which is intuitive.

\end{itemize}


\section{Continuous Probability Spaces}
\label{\detokenize{fund/fundprob:continuous-probability-spaces}}
\sphinxAtStartPar
It turns out that the definition of probability spaces requires a different approach in the case of sample spaces that contain \sphinxstyleemphasis{uncountably} many outcomes. For example, the sample space could be given by the real numbers (\(\Omega = \mathbb{R}\)) or a higher dimensional space (e.g. \(\Omega = \mathbb{R}^d\), \(d >= 2\)). Indeed, the definition of (probability) measures on arbitrary sample spaces turns out to be a complex mathematical problem which is the foundation of \sphinxstylestrong{measure theory}. This theory introduces so\sphinxhyphen{}called \(\sigma\)\sphinxhyphen{}algebras which specify the measurable events, i.e., the events for which it is possible to assign a probability without generating any inconsistencies. An introduction can be found in the first chapter of \sphinxhref{https://www.springer.com/de/book/9783642360183}{“Wahrscheinlichkeitstheorie”} by Achim Klenke. Measure theory is the foundation of very powerful results, since it enables mathematicians to define probability measures even on infinite dimensional sample spaces such as spaces of functions which lead to so\sphinxhyphen{}called stochastic processes. A special case are \sphinxstylestrong{Gaussian processes} which turn out to be very useful in the context of machine learning and are an essential part of this lecture.

\sphinxAtStartPar
Luckily, we do not necessarily need to consider measure theory in detail for our purposes. For the mentioned cases (\(\mathbb{\Omega} = [0, 1]\) or \(\Omega = \mathbb{R}^d\), \(d >= 1\)), we can use ordinary integrals in order to define probabilities at least on “nice” events.

\sphinxAtStartPar
\sphinxstylestrong{Remark}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Set \(C := \big\{ [a_1, b_1] \times [a_2, b_2] \times \cdots \times [a_d, b_d]~\big|~-\infty \le a_i \le b_i \le \infty, ~i = 1, \dots, d\big\}\). An event \(A \in C\) is simply a box. For d=1 we obtain an interval \(A = [a_1, b_1]\) and for d=2 we get a rectangle \(A = [a_1, b_1] \times [a_2, b_2] \subset \mathbb{R}^2\).

\item {} 
\sphinxAtStartPar
In measury theory, \(C\) is a so\sphinxhyphen{}called generating system of the \sphinxstylestrong{Borel} \(\sigma\)\sphinxhyphen{}\sphinxstylestrong{algebra} \(\mathcal{B}(\mathbb{R}^d)\). The Borel \(\sigma\)\sphinxhyphen{}algebra is the smallest collection of events with sufficiently nice properties which contains alle these boxes.

\item {} 
\sphinxAtStartPar
\(\mathcal{B}(\mathbb{R}^d)\) is fairly abstract. Just rembember that
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\mathcal{B}(\mathbb{R}^d)\) contains all events we would like / are able to assign a probability to,

\item {} 
\sphinxAtStartPar
there are subsets of \(\mathbb{R}^d\) which are not in \(\mathcal{B}(\mathbb{R}^d)\), but we do not care, since they are not important.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Definition}: Let \(\Omega = \mathbb{R}^d\), \(d \ge 1\), be the sample space and \(f: \mathbb{R}^d \rightarrow \mathbb{R}\) be an integrable non\sphinxhyphen{}negative function such that
\begin{equation*}
\begin{split} \int_{\mathbb{R}^d} f(x)~dx = 1. \end{split}
\end{equation*}
\sphinxAtStartPar
Then, the map \(P: C \rightarrow [0, 1]\) defined by
\begin{equation*}
\begin{split} P(A) := \int_{a_d}^{b_d} \cdots \int_{a_1}^{b_1} f(x) ~ dx \quad \text{for } A = [a_1, b_1] \times [a_2, b_2] \times \cdots \times [a_d, b_d] \in C\end{split}
\end{equation*}
\sphinxAtStartPar
extends uniquely to \(\mathcal{B}(\Omega)\) (not part of the lecture) and this extension is called a \sphinxstylestrong{continuous probability measure} or a \sphinxstylestrong{continuous probability distribution}. \(f\) is called the \sphinxstylestrong{probability density function} (PDF) of \(P\) or briefly probabilty density or simply density. The triple \((\Omega, \mathcal{B}(\Omega), P)\) or briefly \((\Omega, P)\) is called a \sphinxstylestrong{continuous probability space}. Furthermore, the function \(F_X: \mathbb{R}^d \rightarrow [0, 1]\) defined by
\begin{equation*}
\begin{split} F(x) := P((-\infty, x_1] \times \cdots \times (-\infty, x_d]) \quad \text{for } x=(x_1, \dots, x_d) \in \mathbb{R}^d\end{split}
\end{equation*}
\sphinxAtStartPar
is called the \sphinxstylestrong{cumulative distribution function} (CDF) of \(P\).

\sphinxAtStartPar
\sphinxstylestrong{Example} (Standard Normal Distribution): It holds
\begin{equation*}
\begin{split} \int_{\mathbb{R}} \exp\big(-\frac{1}{2} x^2\big)~dx = \sqrt{2 \pi}.\end{split}
\end{equation*}
\sphinxAtStartPar
Therefore, \(f: \mathbb{R} \rightarrow \mathbb{R}\) defined by \(f(x):= \frac{1}{\sqrt{2 \pi}}~\exp\big(-\frac{1}{2} x^2\big)\) for \(x \in \mathbb{R}\) defines a continuous probability distribution with density \(f\).

\sphinxAtStartPar
\sphinxstylestrong{Remark}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
As in the case of discrete probability spaces, a probability measure as defined in Definition 1.9 fulfills the \sphinxstylestrong{Kolmogorov axioms} stated in Corollary 1.6.

\item {} 
\sphinxAtStartPar
In order to unify the notation of discrete and continuous probability spaces, we denote a general probability space by \((\Omega, \mathcal{F}, P)\), where \(\mathcal{F}\) denotes a \(\sigma\)\sphinxhyphen{}algebra (in our case either \(\mathcal{P}(\Omega)\) or \(\mathcal{B}(\Omega)\)).

\item {} 
\sphinxAtStartPar
Keep in mind that \(f\) is simply a non\sphinxhyphen{}negative function whose volume under the graph is exactly one and the probability of some event \(A\) is the volume under the graph of \(f\) restricted to \(A\). In the plot below, \(P([-2, 0]) \approx 0.48\) is illustrated for a standard normal distribution. In other words, the probability to observe an outcome between \sphinxhyphen{}2 and 0 in a standard normally distributed experiment is approximately 48\%.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{gaussian_pdf}.png}


\section{Random Variables}
\label{\detokenize{fund/fundprob:random-variables}}
\sphinxAtStartPar
Imagine that we perform multiple independent random experiments by rolling repeatedly (\(n\)\sphinxhyphen{}times) a fair dice as in Example 1.5. The corresponding sample space is given by
\begin{equation*}
\begin{split}\Omega = \{ \omega = (\omega_1, \omega_2, \dots, \omega_n)~|~\omega_i \in \{1, 2, 3, 4, 5, 6\} \ \text{for } i=1, \dots,n \}.\end{split}
\end{equation*}
\sphinxAtStartPar
Since the experiments are independent and we consider a fair dice, it is reasonable to define
\begin{equation*}
\begin{split} p(\omega) = \frac{1}{6^n} \quad \text{for each } \omega \in \Omega\end{split}
\end{equation*}
\sphinxAtStartPar
which results in a discrete probability space \((\Omega, \mathcal{P}(\Omega), P)\). Eventually, we are not interested in events with respect to \(\Omega\), but for example in the average outcome of the experiments or the number of times of rolling a six. Instead of modelling these experiments directly by redefining \(\Omega\) and \(p\), it is very useful to apply the concept of random variables:

\sphinxAtStartPar
\sphinxstylestrong{Definition}: Let \((\Omega, \mathcal{F}, P)\) be a probabiliy space. A map \(X: \Omega \rightarrow \mathbb{R}^d\), \(d \ge 1\), is called a real\sphinxhyphen{}valued \sphinxstylestrong{random variable}, if
\begin{equation*}
\begin{split} X^{-1}(A) := \{ \omega \in \Omega~|~X(\omega) \in A \} \in \mathcal{F}\end{split}
\end{equation*}
\sphinxAtStartPar
for each \(A \in \mathcal{B}(\mathbb{R}^d)\) and the probability measure \(P_X: \mathcal{B}(\Omega) \rightarrow [0, 1]\) given by
\begin{equation*}
\begin{split}P_X(A) := P(X^{-1}(A)) \quad \text{for } A \in \mathcal{B}(\Omega)\end{split}
\end{equation*}
\sphinxAtStartPar
is called the \sphinxstylestrong{distribution of} \(X\) \sphinxstylestrong{under} \(P\). If \(P_X\) admits a probability density as defined in Definition 1.9, then we denote the density by \(f_X\). Furthermore, the cumulative distribution function of \(P_X\) is denoted by \(F_X\).

\sphinxAtStartPar
\sphinxstylestrong{Remark}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
If \((\Omega, \mathcal{F}, P)\) is a discrete probability space as defined in Definition 1.3, \sphinxstylestrong{each} map \(X: \Omega \rightarrow \mathbb{R}^d\) is a random variable, since \(\mathcal{F} = \mathcal{P}(\Omega)\) contains all subsets of \(\Omega\).

\item {} 
\sphinxAtStartPar
If \((\Omega, \mathcal{F}, P)\) is a continuous probability space as defined in Definition 1.9, it can be shown that at least each \sphinxstylestrong{continuous} map \(X: \Omega \rightarrow \mathbb{R}^d\) is a random variable.

\item {} 
\sphinxAtStartPar
If \(d > 1\), \(X: \Omega \rightarrow \mathbb{R}^d\) is a random variable if and only if each component \(X_i: \Omega \rightarrow \mathbb{R}\) is a random variable.

\end{itemize}


\section{Independence}
\label{\detokenize{fund/fundprob:independence}}
\sphinxAtStartPar
\sphinxstylestrong{Definition}: Let \((\Omega, \mathcal{F}, P)\) be a probability space. Then two events \(A, B \in \mathcal{F}\) are called \sphinxstylestrong{independent}, if
\begin{equation*}
\begin{split} P(A \cap B) = P(A) P(B).\end{split}
\end{equation*}
\sphinxAtStartPar
Let \(X: \Omega \rightarrow \mathbb{R}\) and \(Y: \Omega \rightarrow \mathbb{R}\) be two random variables. Then \(X\) and \(Y\) are called \sphinxstylestrong{independent random variables}, if the events \(X^{-1}(A)\) and \(Y^{-1}(B)\) are independent for all \(A, B \in \mathcal{B}(\mathbb{R})\). This is equivalent to the property
\begin{equation*}
\begin{split}F_{(X, Y)}(x, y) = F_X(x) F_Y(y) \quad \text{for all } x,y \in \mathbb{R}\end{split}
\end{equation*}
\sphinxAtStartPar
and if the corresponding densities exist to
\begin{equation*}
\begin{split}f_{(X, Y)}(x, y) = f_X(x) f_Y(y) \quad \text{for all } x,y \in \mathbb{R}.\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Remark}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Independence basically means that the occurrence of event A has no impact on the occurrence of event B or in terms of random variables, the random variables should not impact each other.

\item {} 
\sphinxAtStartPar
The definition of independent random variables generalizes easily to \(d > 1\).

\end{itemize}


\section{Important Probability Distributions}
\label{\detokenize{fund/fundprob:important-probability-distributions}}

\subsection{Discrete Distributions}
\label{\detokenize{fund/fundprob:discrete-distributions}}

\subsection{Continuous Distributions}
\label{\detokenize{fund/fundprob:continuous-distributions}}

\chapter{Bayesian vs. Frequentists View}
\label{\detokenize{fund/stat:bayesian-vs-frequentists-view}}\label{\detokenize{fund/stat::doc}}

\section{Bayes’ Theorem}
\label{\detokenize{fund/stat:bayes-theorem}}

\chapter{MLE, MAP \& Bayesian Inference}
\label{\detokenize{fund/bayes:mle-map-bayesian-inference}}\label{\detokenize{fund/bayes::doc}}

\section{Bayesian Inference}
\label{\detokenize{fund/bayes:bayesian-inference}}

\section{Maximum Likelihood Estimation (MLE)}
\label{\detokenize{fund/bayes:maximum-likelihood-estimation-mle}}

\section{Maximum A\sphinxhyphen{}posteriori Method (MAP)}
\label{\detokenize{fund/bayes:maximum-a-posteriori-method-map}}

\chapter{An illustrative example for MLE and MAP: Linear Regression}
\label{\detokenize{fund/linregr:an-illustrative-example-for-mle-and-map-linear-regression}}\label{\detokenize{fund/linregr::doc}}

\section{Ordinary Least Squares}
\label{\detokenize{fund/linregr:ordinary-least-squares}}

\section{Ridge Regression}
\label{\detokenize{fund/linregr:ridge-regression}}

\section{LASSO}
\label{\detokenize{fund/linregr:lasso}}

\chapter{Optimization Methods}
\label{\detokenize{fund/opt:optimization-methods}}\label{\detokenize{fund/opt::doc}}

\chapter{Machine Learning Workflow}
\label{\detokenize{fund/MLworkflow:machine-learning-workflow}}\label{\detokenize{fund/MLworkflow::doc}}

\part{Probabilistic Machine Learning}


\chapter{Motivation of Probabilistic Models}
\label{\detokenize{probML/motivation:motivation-of-probabilistic-models}}\label{\detokenize{probML/motivation::doc}}

\chapter{Kernel\sphinxhyphen{}based Methods}
\label{\detokenize{probML/kernelmethods:kernel-based-methods}}\label{\detokenize{probML/kernelmethods::doc}}

\section{Gaussian Processes}
\label{\detokenize{probML/kernelmethods:gaussian-processes}}

\section{Additional Kernel\sphinxhyphen{}based Methods}
\label{\detokenize{probML/kernelmethods:additional-kernel-based-methods}}

\chapter{Overview of Further Probabilistic Models}
\label{\detokenize{probML/overview:overview-of-further-probabilistic-models}}\label{\detokenize{probML/overview::doc}}

\part{Applications}


\chapter{Bayesian Optimization}
\label{\detokenize{appl/BO:bayesian-optimization}}\label{\detokenize{appl/BO::doc}}

\chapter{Quantification of design uncertainty}
\label{\detokenize{appl/uncertainty:quantification-of-design-uncertainty}}\label{\detokenize{appl/uncertainty::doc}}

\chapter{Data\sphinxhyphen{}efficient Reinforcement Learning}
\label{\detokenize{appl/RL:data-efficient-reinforcement-learning}}\label{\detokenize{appl/RL::doc}}






\renewcommand{\indexname}{Index}
\printindex
\end{document}