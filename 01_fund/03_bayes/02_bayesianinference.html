
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3.2. Bayesian Inference &#8212; Introduction to Probabilistic Machine Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/additional.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://probabilistic-ml.github.io/lecture-notes/welcome.html/01_fund/03_bayes/02_bayesianinference.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="3.3. MAP and MLE" href="03_MLEandMAP.html" />
    <link rel="prev" title="3.1. Coin Toss" href="01_cointoss.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Probabilistic Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../welcome.html">
   Welcome
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Preface
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/01_preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/02_python.html">
   Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/03_notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Fundamentals
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01_fundprob.html">
   1. Fundamentals of Probability Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01_fundprob/01_probabilityspaces.html">
     1.1. Probability Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01_fundprob/02_randomvariables.html">
     1.2. Random Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01_fundprob/03_independence.html">
     1.3. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01_fundprob/04_impprobdistr.html">
     1.4. Important Probability Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01_fundprob/05_essthms.html">
     1.5. Essential Theorems
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02_stat.html">
   2. Bayesian vs. Frequentists View
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../03_bayes.html">
   3. Bayesian Inference, MAP &amp; MLE
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_cointoss.html">
     3.1. Coin Toss
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.2. Bayesian Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_MLEandMAP.html">
     3.3. MAP and MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_linregr.html">
     3.4. Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04_opt.html">
   4. Optimization Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05_MLworkflow.html">
   5. Machine Learning Workflow
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Probabilistic Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../02_probML/01_motivation.html">
   6. Motivation of Probabilistic Models
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../02_probML/02_GPforML.html">
   7. Gaussian Processes for Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/01_kerneltrick.html">
     7.1. The Kernel Trick: Implicit embeddings from inner products
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/02_GP.html">
     7.2. Gaussian Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/03_GPregression.html">
     7.3. Gaussian Process Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/04_kernels.html">
     7.4. Kernel Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/05_hyperparamimpact.html">
     7.5. Impact of Hyperparameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/06_hyperparamselect.html">
     7.6. Selection of Hyperparameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/07_multiout.html">
     7.7. Extension to Multiple Outputs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/08_GPclassification.html">
     7.8. Gaussian Process Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/09_examples.html">
     7.9. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../02_probML/02_GPforML/10_advanced.html">
     7.10. Advanced Methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../02_probML/03_overview.html">
   8. Overview of Further Probabilistic Models
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/BO.html">
   9. Bayesian Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/uncertainty.html">
   10. Design Uncertainty Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/03_RL.html">
   11. Efficient Reinforcement Learning
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org/"><img alt="Jupyter Book" src="https://jupyterbook.org/badge.svg" width="100"></a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/01_fund/03_bayes/02_bayesianinference.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Probabilistic-ML/lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- #region -->
<div class="tex2jax_ignore mathjax_ignore section" id="bayesian-inference">
<span id="sec-bayesianinference"></span><h1><span class="section-number">3.2. </span>Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this headline">¶</a></h1>
<p>We already discussed <a class="reference internal" href="../01_fundprob/05_essthms.html#thm-bt"><span class="std std-ref">Bayes’ theorem</span></a> as well as Bayesian inference using the example of a coin toss in the previous section. In the present section, we present Bayesian inference in a more formal way in order to be able to apply the general concept later on to machine learning models.</p>
<p>The following setting is assumed:</p>
<ul class="simple">
<li><p>Some data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is given. For example, this data contains the outcomes of coin tosses. <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> can also contain multiple input-output pairs <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> which are related by some function <span class="math notranslate nohighlight">\(f\)</span> (i.e., <span class="math notranslate nohighlight">\(y_i = f(x_i)\)</span>). This situation is of particular importance in supervised machine learning.</p></li>
<li><p><span class="math notranslate nohighlight">\(x\)</span> denotes a general data point</p></li>
<li><p>A parameter <span class="math notranslate nohighlight">\(\theta\)</span> (possibly vector valued) specifies the distribution of the data. For example, <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter of the Bernoulli distribution in the coin toss example.</p></li>
</ul>
<p>In order to perform Bayesian inference, the following steps are necessary:</p>
<ul>
<li><p>Assign a prior distribution to <span class="math notranslate nohighlight">\(\theta\)</span>. This distribution can again depend on some so-called hyperparameters. In the coin toss example, we used the beta distribution <span class="math notranslate nohighlight">\(\text{Beta}(\alpha, \beta)\)</span>, i.e., <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are the hyperparameters.</p></li>
<li><p>Use Bayes’ theorem and the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> in order to caluclate the posterior distribution of <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(\theta~|~\mathcal{D}) = \frac{p(\mathcal{D}~|~\theta)~p(\theta)}{p(\mathcal{D})}\]</div>
<ul>
<li><p><span class="math notranslate nohighlight">\(p(\theta~|~\mathcal{D})\)</span> denotes the <strong>posterior distribution</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\mathcal{D}~|~\theta)\)</span> is the <strong>likelihood</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\theta)\)</span> is the <strong>prior distribution</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span> is called the <strong>marginal likelihood</strong> or <strong>evidence</strong> and is given by</p>
<div class="math notranslate nohighlight">
\[p(\mathcal{D}) = \int p(\mathcal{D}~|~\theta)~p(\theta)~d\theta\]</div>
</li>
</ul>
<p>In the coin toss example, it turned out that the posterior distribution is again a beta distribution. Thus, the prior was chosen such that its combination with the likelihood in Bayes’ theorem yields a posterior which is from the same family of probability distributions. As mentioned before, this kind of prior is called <strong>conjugate prior</strong>. A table of conjugate priors for many likelihoods can be found on <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">Wikipedia</a>.</p>
</li>
</ul>
<p>The prior as well as the positrior distribution can be used to make probabilistic predicitions about the distribution of new data points. The <strong>prior predictive distribution</strong> is given by</p>
<div class="math notranslate nohighlight">
\[p(x) = \int p(x~|~\theta)~p(\theta)~d\theta\]</div>
<p>and the <strong>posterior predictive distribution</strong> by</p>
<div class="math notranslate nohighlight">
\[p(x~|~\mathcal{D}) = \int p(x~|~\theta)~p(\theta~|~\mathcal{D})~d\theta\]</div>
<p>For example, in the coin toss example we can calulate the probability for heads in the next experiment. The use of the prior predictive distribution means that we only include our prior beliefs on <span class="math notranslate nohighlight">\(\theta\)</span> which was given by the <span class="math notranslate nohighlight">\(\text{Beta}(2, 2)\)</span> distribution. Recall that the likelihood is <span class="math notranslate nohighlight">\(\text{B}(1, \theta)\)</span>-distributed and therefore, <span class="math notranslate nohighlight">\(p(H~|~\theta) = \theta\)</span>, where <span class="math notranslate nohighlight">\(H\)</span> denotes the elementary event “heads”. Thus, it follows</p>
<div class="math notranslate nohighlight">
\[p(H) = \int \theta~p(\theta)~d\theta = \mathbb{E}(\theta)\]</div>
<p>The probability for heads equals in this case the expectation of the prior distribution which is <span class="math notranslate nohighlight">\(0.5\)</span> in this example. Here, we used that the expectation of <span class="math notranslate nohighlight">\(\text{Beta}(\alpha, \beta)\)</span> is <span class="math notranslate nohighlight">\(\alpha/(\alpha + \beta)\)</span>.</p>
<p>The posterior predictive distribution uses the posterior distribution of <span class="math notranslate nohighlight">\(\theta\)</span> and hence, also the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. Assume that we tossed the coin five times and observed 3 heads and 2 tails. In the previous section, we have seen that <span class="math notranslate nohighlight">\(\text{Beta}(2 + 3, 2 + 2) = \text{Beta}(5, 4)\)</span> is the posterior distribtion. Similarly as before,</p>
<div class="math notranslate nohighlight">
\[p(H~|~\mathcal{D}) = \int \theta~p(\theta~|~\mathcal{D})~d\theta\]</div>
<p>is exactly the expectation of the posterior distribution which equals <span class="math notranslate nohighlight">\(5/9\)</span>.</p>
<p>Hence, by observing more heads than tails our belief tend to expect heads more likely than before.</p>
<p>Depending on the choice of the prior distribution and the distributional form of the likelihood, it can be very demanding to caluclate the marginal likelihood</p>
<div class="math notranslate nohighlight">
\[p(\mathcal{D}) = \int p(\mathcal{D}~|~\theta)~p(\theta)~d\theta\]</div>
<p>In particular, in high dimensions this term can become intractable. In this case, the posterior distribution can not be caluclated expicitly, but some approximation techniques can be used. Very common are Markov chain Monte Carlo (MCMC) methods or variational inference (refer to chapters 21 to 24 in <span id="id1">[<a class="reference internal" href="#id8">1</a>]</span>). Nevertheless, these methods are out of the scope of this lecture. Instead, it is also possible to simplify the problem and <strong>not</strong> to compute the complete posterior distribution. This will be discussed in the following section.</p>
<p id="id2"><dl class="citation">
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>K. P. Murphy. <em>Machine learning : A Probabilistic Perspective</em>. MIT Press, 2012.</p>
</dd>
</dl>
</p>
<!-- #endregion -->
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./01_fund/03_bayes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="01_cointoss.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">3.1. </span>Coin Toss</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="03_MLEandMAP.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">3.3. </span>MAP and MLE</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By C. Bogoclu, N. Friedlich & R. Vosshall<br/>
        
          <div class="extra_footer">
            Content on this site is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">a CC BY-NC-NB 4.0 licence</a>.
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>