
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1.1. Probability Spaces &#8212; Probabilistic Machine Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/videocont.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://probabilistic-ml.github.io/lecture-notes/welcome.html/01_fund/01_fundprob/01_probabilityspaces.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.2. Random Variables" href="02_randomvariables.html" />
    <link rel="prev" title="1. Fundamentals of Probability Theory" href="../01_fundprob.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Probabilistic Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../welcome.html">
   Welcome
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Preface
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/about.html">
   About the Authors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/python.html">
   Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Fundamentals
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../01_fundprob.html">
   1. Fundamentals of Probability Theory
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     1.1. Probability Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_randomvariables.html">
     1.2. Random Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_independence.html">
     1.3. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_impprobdistr.html">
     1.4. Important Probability Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05_essthms.html">
     1.5. Essential Theorems
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02_stat.html">
   2. Bayesian vs. Frequentists View
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03_bayes.html">
   3. Bayesian Inference, MLE &amp; MAP
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03_bayes/01_cointoss.html">
     3.1. Coin Toss
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03_bayes/02_bayesianinference.html">
     3.2. Bayesian Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03_bayes/03_MLEandMAP.html">
     3.3. MLE and MAP
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04_linregr.html">
   4. An illustrative example for MLE and MAP: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05_opt.html">
   5. Optimization Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06_MLworkflow.html">
   6. Machine Learning Workflow
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Probabilistic Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../02_probML/motivation.html">
   7. Motivation of Probabilistic Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../02_probML/kernelmethods.html">
   8. Kernel-based Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../02_probML/overview.html">
   14. Overview of Further Probabilistic Models
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/BO.html">
   15. Bayesian Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/uncertainty.html">
   16. Quantification of Design Uncertainty
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/03_RL.html">
   17. Sample Efficient Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../04_appendix/placeholder.html">
   Appendix A
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org/"><img alt="Jupyter Book" src="https://jupyterbook.org/badge.svg" width="100"></a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/01_fund/01_fundprob/01_probabilityspaces.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Probabilistic-ML/lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discrete-probability-spaces">
   1.1.1. Discrete Probability Spaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#continuous-probability-spaces">
   1.1.2. Continuous Probability Spaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-probability">
   1.1.3. Conditional Probability
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- #region -->
<div class="section" id="probability-spaces">
<h1><span class="section-number">1.1. </span>Probability Spaces<a class="headerlink" href="#probability-spaces" title="Permalink to this headline">¶</a></h1>
<div class="tip admonition" id="def-samplespace">
<p class="admonition-title">Definition</p>
<p>In order to model the outcome of a random experiment, we denote by <span class="math notranslate nohighlight">\(\Omega\)</span> the <strong>sample space</strong> of all possible outcomes, i.e.,</p>
<div class="math notranslate nohighlight">
\[\Omega = \{ \omega ~|~ \omega \text{ is a possible outcome of the random experiment}\}.\]</div>
<p>Accordingly, each element <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span> is called an <strong>outcome</strong>. A subset <span class="math notranslate nohighlight">\(A\)</span> of <span class="math notranslate nohighlight">\(\Omega\)</span> of possible outcomes is called an <strong>event</strong>. If <span class="math notranslate nohighlight">\(A\)</span> contains only a single outcome <span class="math notranslate nohighlight">\(\omega\)</span>, i.e., <span class="math notranslate nohighlight">\(A=\{\omega\}\)</span> for some <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span>, <span class="math notranslate nohighlight">\(A\)</span> is also called an elementary event.</p>
</div>
<p>If we model the rolling of an ordinary cubic dice, the sample space</p>
<div class="math notranslate nohighlight" id="equation-ex-dice">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-ex-dice" title="Permalink to this equation">¶</a></span>\[\Omega= \{1, 2, 3, 4, 5, 6\}\]</div>
<p>is given by the 6 possible outcomes. The event <span class="math notranslate nohighlight">\(A\)</span> of rolling an even number is given by <span class="math notranslate nohighlight">\(A = \{2, 4, 6\} \subset \Omega\)</span> and the elementary event of rolling a six is given by <span class="math notranslate nohighlight">\(A=\{6\}\)</span>.</p>
<p>Probability theory is based in the concept of probability distributions. In the following, we will distinguish between discrete und continuous distributions. It is also possible to unify these two kinds of distributions by means of measure theory, but this is outside the scope of this lecture (some remarks can be found in the section on <a class="reference internal" href="#sec-cps"><span class="std std-ref">continuous probability spaces</span></a>). As the name suggests, the distribution determines/specifies how the outcomes of random experiments are distributed.</p>
<div class="section" id="discrete-probability-spaces">
<h2><span class="section-number">1.1.1. </span>Discrete Probability Spaces<a class="headerlink" href="#discrete-probability-spaces" title="Permalink to this headline">¶</a></h2>
<div class="tip admonition" id="def-discrdistr">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(\Omega\)</span> be a <em>finite or countable</em> sample space and denote by <span class="math notranslate nohighlight">\(\mathcal{P}(\Omega) = \{A~|~A \subset \Omega\}\)</span> the set of all subsets of <span class="math notranslate nohighlight">\(\Omega\)</span> (the so-called power set). Moreover, let <span class="math notranslate nohighlight">\(p: \Omega \rightarrow [0, 1]\)</span> be a map such that <span class="math notranslate nohighlight">\(\sum_{\omega \in \Omega} p(\omega) = 1\)</span>. Then, the map <span class="math notranslate nohighlight">\(P: \mathcal{P}(\Omega) \rightarrow [0,1]\)</span> given by</p>
<div class="math notranslate nohighlight">
\[ P(A) := \sum_{\omega \in A} p(\omega) \quad \text{for } A \in \mathcal{P}(\Omega)\]</div>
<p>is called a <strong>discrete probability measure</strong> or a <strong>discrete probability distribution</strong>. The triple <span class="math notranslate nohighlight">\((\Omega, \mathcal{P}(\Omega), P)\)</span> or briefly <span class="math notranslate nohighlight">\((\Omega, P)\)</span> is called a <strong>discrete probability space</strong>.</p>
</div>
<ul class="simple">
<li><p>A probability measure <span class="math notranslate nohighlight">\(P\)</span> assigns to each possible event a probability between 0 (“impossible”) to 1 (“sure”).</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> is completely characterized by the elementary probabilities (i.e., the probabilities of elementary events specified by <span class="math notranslate nohighlight">\(p\)</span>) in the case of discrete probability distributions (by definition).</p></li>
<li><p>The condition <span class="math notranslate nohighlight">\(\sum_{\omega \in \Omega} p(\omega) = 1\)</span> guarantees that <span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span>. In other words, it has to be sure that the outcome of a random experiment is indeed in <span class="math notranslate nohighlight">\(\Omega\)</span> and moreover, <span class="math notranslate nohighlight">\(P(\Omega) &gt; 1\)</span> would make no sense in terms of probabilities.</p></li>
</ul>
<p>Assumed that we are dealing with a fair dice as before, it is reasonable to define <span class="math notranslate nohighlight">\(p(\omega):=\frac{1}{6}\)</span> for each <span class="math notranslate nohighlight">\(\omega=1, \dots,6\)</span>. Hence, each outcome of a dice roll is each likely. Consequently, the probability of rolling an even number is</p>
<div class="math notranslate nohighlight">
\[P(\{2, 4, 6\}) = \sum_{\omega \in \{2, 4, 6\}} p(\omega) = 3 \cdot \frac{1}{6} = 0.5\]</div>
<p>as expected.</p>
<div class="tip admonition" id="cor-discrdistr">
<p class="admonition-title">Corollary</p>
<p>A <a class="reference internal" href="#def-discrdistr"><span class="std std-ref">discrete probability measure</span></a> has the following properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(0 \le P(A) \le 1\)</span> for each event <span class="math notranslate nohighlight">\(A \in \mathcal{P}(\Omega)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> is <span class="math notranslate nohighlight">\(\sigma\)</span>-additive, i.e., for pairwise disjoint events <span class="math notranslate nohighlight">\(A_i\)</span>, <span class="math notranslate nohighlight">\(i \in \mathbb{N}\)</span>, it holds</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(\cup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i).\]</div>
</div>
<ul class="simple">
<li><p>The statements in the preceding corollary are also called <strong>Kolmogorov axioms</strong>.</p></li>
<li><p>The term “pairwise disjoint” means that two arbitrary events do not have any common elements. For example, the events <span class="math notranslate nohighlight">\(\{2, 4, 6\}\)</span> and <span class="math notranslate nohighlight">\(\{1, 3\}\)</span> are disjoint, but the events <span class="math notranslate nohighlight">\(\{2, 4, 6\}\)</span> and <span class="math notranslate nohighlight">\(\{2, 3\}\)</span> are not, since the share the outcome <span class="math notranslate nohighlight">\(2\)</span>.</p></li>
<li><p>The last statement also holds true for a <em>finite number</em> of sets <span class="math notranslate nohighlight">\(A_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span>, by simply choosingchoosing <span class="math notranslate nohighlight">\(A_i = \emptyset\)</span> (empty set) for <span class="math notranslate nohighlight">\(i &gt; n\)</span>. If we consider only two disjoint sets <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span>, it follows that <span class="math notranslate nohighlight">\(P(A_1 \cup A_2) = P(A_1) + P(A_2)\)</span>. This means that the probability that the event <span class="math notranslate nohighlight">\(A_1\)</span> or the event <span class="math notranslate nohighlight">\(A_2\)</span> occurs equals the sum of the probabilities, which is intuitive.</p></li>
</ul>
</div>
<div class="section" id="continuous-probability-spaces">
<span id="sec-cps"></span><h2><span class="section-number">1.1.2. </span>Continuous Probability Spaces<a class="headerlink" href="#continuous-probability-spaces" title="Permalink to this headline">¶</a></h2>
<p>It turns out that the definition of probability spaces requires a different approach in the case of sample spaces that contain <em>uncountably</em> many outcomes. For example, the sample space could be given by the real numbers (<span class="math notranslate nohighlight">\(\Omega = \mathbb{R}\)</span>) or a higher dimensional space (e.g. <span class="math notranslate nohighlight">\(\Omega = \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d &gt;= 2\)</span>). Indeed, the definition of (probability) measures on arbitrary sample spaces turns out to be a complex mathematical problem which is the foundation of <strong>measure theory</strong>. This theory introduces so-called <span class="math notranslate nohighlight">\(\sigma\)</span>-algebras which specify the measurable events, i.e., the events for which it is possible to assign a probability without generating any inconsistencies. An introduction can be found in the first chapter of <a class="reference external" href="https://www.springer.com/de/book/9783642360183">“Wahrscheinlichkeitstheorie”</a> by Achim Klenke. Measure theory is the foundation of very powerful results, since it enables mathematicians to define probability measures even on infinite dimensional sample spaces such as spaces of functions which lead to so-called stochastic processes. A special case are <strong>Gaussian processes</strong> which turn out to be very useful in the context of machine learning and are an essential part of this lecture.</p>
<p>Luckily, we do not necessarily need to consider measure theory in detail for our purposes. For the mentioned cases (<span class="math notranslate nohighlight">\(\mathbb{\Omega} = [0, 1]\)</span> or <span class="math notranslate nohighlight">\(\Omega = \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d &gt;= 1\)</span>), we can use ordinary integrals in order to define probabilities at least on “nice” events:</p>
<ul class="simple">
<li><p>Set <span class="math notranslate nohighlight">\(C := \big\{ [a_1, b_1] \times [a_2, b_2] \times \cdots \times [a_d, b_d]~\big|~-\infty \le a_i \le b_i \le \infty, ~i = 1, \dots, d\big\}\)</span>. An event <span class="math notranslate nohighlight">\(A \in C\)</span> is simply a box.</p></li>
<li><p>For d=1 we obtain an interval <span class="math notranslate nohighlight">\(A = [a_1, b_1]\)</span>.</p></li>
<li><p>For d=2 we get a rectangle <span class="math notranslate nohighlight">\(A = [a_1, b_1] \times [a_2, b_2] \subset \mathbb{R}^2\)</span>.</p></li>
<li><p>In measury theory, <span class="math notranslate nohighlight">\(C\)</span> is a so-called generating system of the <strong>Borel</strong> <span class="math notranslate nohighlight">\(\sigma\)</span>-<strong>algebra</strong> <span class="math notranslate nohighlight">\(\mathcal{B}(\mathbb{R}^d)\)</span>.</p></li>
<li><p>The Borel <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra is the smallest collection of events with sufficiently nice properties which contains alle these boxes.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}(\mathbb{R}^d)\)</span> is fairly abstract. Just rembember that</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}(\mathbb{R}^d)\)</span> contains all events we would like / are able to assign a probability to,</p></li>
<li><p>there are subsets of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> which are not in <span class="math notranslate nohighlight">\(\mathcal{B}(\mathbb{R}^d)\)</span>, but we do not care, since they are not important.</p></li>
</ul>
</li>
</ul>
<div class="tip admonition" id="def-contdistr">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(\Omega = \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(d \ge 1\)</span>, be the sample space and <span class="math notranslate nohighlight">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span> be an integrable non-negative function such that</p>
<div class="math notranslate nohighlight">
\[ \int_{\mathbb{R}^d} f(x)~dx = 1. \]</div>
<p>Then, the map <span class="math notranslate nohighlight">\(P: C \rightarrow [0, 1]\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[ P(A) := \int_{a_d}^{b_d} \cdots \int_{a_1}^{b_1} f(x) ~ dx \quad \text{for } A = [a_1, b_1] \times [a_2, b_2] \times \cdots \times [a_d, b_d] \in C\]</div>
<p>extends uniquely to <span class="math notranslate nohighlight">\(\mathcal{B}(\Omega)\)</span> (not part of the lecture) and this extension is called a <strong>continuous probability measure</strong> or a <strong>continuous probability distribution</strong>. <span class="math notranslate nohighlight">\(f\)</span> is called the <strong>probability density function</strong> (pdf) of <span class="math notranslate nohighlight">\(P\)</span> or briefly probabilty density or simply density. The triple <span class="math notranslate nohighlight">\((\Omega, \mathcal{B}(\Omega), P)\)</span> or briefly <span class="math notranslate nohighlight">\((\Omega, P)\)</span> is called a <strong>continuous probability space</strong>.</p>
</div>
<p>It holds</p>
<div class="math notranslate nohighlight">
\[ \int_{\mathbb{R}} \exp\big(-\frac{1}{2} x^2\big)~dx = \sqrt{2 \pi}.\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span> defined by <span class="math notranslate nohighlight">\(f(x):= \frac{1}{\sqrt{2 \pi}}~\exp\big(-\frac{1}{2} x^2\big)\)</span> for <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> defines a continuous probability distribution with density <span class="math notranslate nohighlight">\(f\)</span>. This distribution is the <strong>standard normal distribution</strong> (also denoted by <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>).</p>
<ul class="simple">
<li><p>As in the case of discrete probability spaces, a <a class="reference internal" href="#def-contdistr"><span class="std std-ref">continuous probability measure</span></a> fulfills the <strong><a class="reference internal" href="#cor-discrdistr"><span class="std std-ref">Kolmogorov axioms</span></a></strong>.</p></li>
<li><p>In order to unify the notation of discrete and continuous probability spaces, we denote a general probability space by <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> denotes a <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra (in our case either <span class="math notranslate nohighlight">\(\mathcal{P}(\Omega)\)</span> or <span class="math notranslate nohighlight">\(\mathcal{B}(\Omega)\)</span>).</p></li>
<li><p>Keep in mind that <span class="math notranslate nohighlight">\(f\)</span> is simply a non-negative function whose volume under the graph is exactly one and the probability of some event <span class="math notranslate nohighlight">\(A\)</span> is the volume under the graph of <span class="math notranslate nohighlight">\(f\)</span> restricted to <span class="math notranslate nohighlight">\(A\)</span>. In the plot below, <span class="math notranslate nohighlight">\(P([-2, 0]) \approx 0.48\)</span> is illustrated for a standard normal distribution. In other words, the probability to observe an outcome between -2 and 0 in a standard normally distributed experiment is approximately 48%.</p></li>
</ul>
<p><img alt="" src="../../_images/gaussian_pdf.png" /></p>
<ul class="simple">
<li><p>In general, regions where the probability density functions takes large values are more likely. Thus, a standard normally distributed experiment will likely have outcomes near <span class="math notranslate nohighlight">\(0\)</span>, whereas outcomes far away from <span class="math notranslate nohighlight">\(0\)</span> are very unlikely.</p></li>
</ul>
</div>
<div class="section" id="conditional-probability">
<h2><span class="section-number">1.1.3. </span>Conditional Probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">¶</a></h2>
<p>The concept of conditional probability answers questions about probabilities of events <span class="math notranslate nohighlight">\(A\)</span> if we already know that some other event <span class="math notranslate nohighlight">\(B\)</span> happens for sure.</p>
<div class="tip admonition" id="def-condprob">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> be a probability space and <span class="math notranslate nohighlight">\(A, B \in \mathcal{F}\)</span> such that <span class="math notranslate nohighlight">\(P(B) &gt; 0\)</span>. Then the <strong>conditional probability</strong> of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[P(A~|~B) := \frac{P(A \cap B)}{P(B)}.\]</div>
</div>
<p><span class="math notranslate nohighlight">\(P(A~|~B)\)</span> is exactly the probability that <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> happen simultaneously normalized by the probability of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>By the third Kolmogorov axiom, the following theorem can be concluded:</p>
<div class="important admonition" id="thm-lawoftotprob">
<p class="admonition-title">Theorem</p>
<p>Let <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> be a probability space and <span class="math notranslate nohighlight">\(B_i \in \mathcal{F}\)</span>, <span class="math notranslate nohighlight">\(i=1,2,\dots\)</span>, a partition of <span class="math notranslate nohighlight">\(\Omega\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[P(A) = \sum_{i=1}^{\infty} P(A \cap B_i) = \sum_{i=1}^{\infty} P(A~|~B_i)~P(B_i) \quad \text{for each } A \in \mathcal{F}.\]</div>
</div>
<p>The theorem is also called <strong>law of total probability</strong> and it states that the probability of an event <span class="math notranslate nohighlight">\(A\)</span> can be considered/computed as a weighted average of conditional probabilities.</p>
<!-- #endregion -->
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./01_fund/01_fundprob"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="../01_fundprob.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">1. </span>Fundamentals of Probability Theory</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="02_randomvariables.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">1.2. </span>Random Variables</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By C. Bogoclu, N. Friedlich & R. Vosshall (Equal Contribution)<br/>
        
          <div class="extra_footer">
            Content on this site is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">a CC BY-NC-NB 4.0 licence</a>.
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>