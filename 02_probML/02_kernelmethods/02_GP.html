
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7.2. Gaussian Processes for Machine Learning &#8212; Introduction to Probabilistic Machine Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/additional.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://probabilistic-ml.github.io/lecture-notes/welcome.html/02_probML/02_kernelmethods/02_GP.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7.3. Additional Kernel-based Methods" href="03_addmethods.html" />
    <link rel="prev" title="7.1. The Kernel Trick: Implicit embeddings from inner products" href="01_kerneltrick.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Probabilistic Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../welcome.html">
   Welcome
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Preface
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/01_preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/02_python.html">
   Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../00_preface/03_notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Fundamentals
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../01_fund/01_fundprob.html">
   1. Fundamentals of Probability Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/01_fundprob/01_probabilityspaces.html">
     1.1. Probability Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/01_fundprob/02_randomvariables.html">
     1.2. Random Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/01_fundprob/03_independence.html">
     1.3. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/01_fundprob/04_impprobdistr.html">
     1.4. Important Probability Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/01_fundprob/05_essthms.html">
     1.5. Essential Theorems
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../01_fund/02_stat.html">
   2. Bayesian vs. Frequentists View
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../01_fund/03_bayes.html">
   3. Bayesian Inference, MAP &amp; MLE
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/03_bayes/01_cointoss.html">
     3.1. Coin Toss
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/03_bayes/02_bayesianinference.html">
     3.2. Bayesian Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/03_bayes/03_MLEandMAP.html">
     3.3. MAP and MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../01_fund/03_bayes/04_linregr.html">
     3.4. Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../01_fund/04_opt.html">
   4. Optimization Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../01_fund/05_MLworkflow.html">
   5. Machine Learning Workflow
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Probabilistic Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01_motivation.html">
   6. Motivation of Probabilistic Models
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../02_kernelmethods.html">
   7. Kernel-based Methods
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_kerneltrick.html">
     7.1. The Kernel Trick: Implicit embeddings from inner products
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     7.2. Gaussian Processes for Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_addmethods.html">
     7.3. Additional Kernel-based Methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03_overview.html">
   8. Overview of Further Probabilistic Models
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/BO.html">
   9. Bayesian Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/uncertainty.html">
   10. Design Uncertainty Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../03_appl/03_RL.html">
   11. Efficient Reinforcement Learning
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org/"><img alt="Jupyter Book" src="https://jupyterbook.org/badge.svg" width="100"></a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/02_probML/02_kernelmethods/02_GP.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Probabilistic-ML/lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/Probabilistic-ML/lecture-notes/blob/master/ProbabilisticML/02_probML/02_kernelmethods/02_GP.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-processes">
   7.2.1. Gaussian Processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-process-regression">
   7.2.2. Gaussian Process Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples-of-kernels">
   7.2.3. Examples of Kernels
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-kernel">
     7.2.3.1. Linear Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#polynomial-kernel">
     7.2.3.2. Polynomial Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#squared-exponential-kernel">
     7.2.3.3. Squared Exponential Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exponential-kernel">
     7.2.3.4. Exponential Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matern-kernel">
     7.2.3.5. Matérn Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rational-quadratic-kernel">
     7.2.3.6. Rational Quadratic Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#periodic-kernel">
     7.2.3.7. Periodic Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#brownian-motion-kernel">
     7.2.3.8. Brownian Motion Kernel
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combination-of-kernels">
   7.2.4. Combination of Kernels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#impact-of-hyperparameters">
   7.2.5. Impact of Hyperparameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#selection-of-hyperparameters">
   7.2.6. Selection of Hyperparameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extension-to-multiple-outputs">
   7.2.7. Extension to Multiple Outputs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-process-classification">
   7.2.8. Gaussian Process Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binary-classification">
     7.2.8.1. Binary Classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiclass-classification">
     7.2.8.2. Multiclass Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#remarks">
   7.2.9. Remarks
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="gaussian-processes-for-machine-learning">
<span id="sec-gpforml"></span><h1><span class="section-number">7.2. </span>Gaussian Processes for Machine Learning<a class="headerlink" href="#gaussian-processes-for-machine-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="gaussian-processes">
<h2><span class="section-number">7.2.1. </span>Gaussian Processes<a class="headerlink" href="#gaussian-processes" title="Permalink to this headline">¶</a></h2>
<p>A Gaussian proccess is a paricular type of stochastic process. <em>But what do we understand by a stochastic process?</em> Typically, a stochastic process denotes a collection of random variables with a time dependence. Hence, the term <em>stochastic</em> is justified by the consideration of <em>random variables</em> and <em>process</em> relates to the <em>time dependence</em>.
We state the following brief definition:</p>
<div class="tip admonition" id="def-tsp">
<p class="admonition-title">Definition</p>
<p>A <strong>(temporal) stochastic process</strong> is a collection of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>-valued random variables <span class="math notranslate nohighlight">\((X_t)_{t \in \mathcal{I}}\)</span>, where either <span class="math notranslate nohighlight">\(\mathcal{I} = \mathbb{N}\)</span> (discrete-time) or <span class="math notranslate nohighlight">\(\mathcal{I} = \mathbb{R}_{\ge 0}\)</span> (continuous-time).</p>
</div>
<p>Hence, this kind of stochastic process describes the temporal process of random events. In this context, it makes sense to say that the outcome of <span class="math notranslate nohighlight">\(X_s\)</span> happended before <span class="math notranslate nohighlight">\(X_t\)</span> for <span class="math notranslate nohighlight">\(s &lt; t\)</span>. Keep in mind that each random variable <span class="math notranslate nohighlight">\(X_t\)</span> is a map <span class="math notranslate nohighlight">\(X_t: \Omega \rightarrow \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(X_t(\omega)\)</span> for <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span> is the outcome of a random experiment. The use of a stochastic process makes it possible to consider the outcomes of all random variables simultaneously. In this way, a so-called <strong>sample path</strong> or <strong>random path</strong> <span class="math notranslate nohighlight">\((X_t(\omega))_{t \in \mathcal{I}}\)</span> is obtained. The shape of these paths depend strongly on the underlying properties of the stochastic process.</p>
<p>A very famous example is <strong>Brownian motion</strong> or <strong>Wiener process</strong> which has numerous applications in physics, finance, biology and many other areas. For example, the movement of a large particle (like pollen) due to collisions with small particles (like water molecules). A nice simulation can be found on the website of Andrew Duffy at Boston University (please click on the image to follow the link):</p>
<p><a href="http://physics.bu.edu/~duffy/HTML5/brownian_motion.html"> <center><img alt="../../_images/BManim.gif" src="../../_images/BManim.gif" /></center></a></p>
<p>Please note that the 2d simulation shows the current position of some particle as well as its previous path. In one dimension, we can easily simulate sample paths of Brownian motion starting at <span class="math notranslate nohighlight">\(X_0 =0\)</span> and plot the result as position against time:</p>
<a class="reference internal image-reference" href="../../_images/bm.gif"><img alt="Brownian motion" src="../../_images/bm.gif" style="width: 800px;" /></a>
<p>The concept can be generalized to general index sets and in particular, to <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>. In the preceding animation time-dependent random functions are genarted, i.e., for each input <span class="math notranslate nohighlight">\(t\)</span> is mapped to a random variable <span class="math notranslate nohighlight">\(f(t)\)</span> (<span class="math notranslate nohighlight">\(X_t\)</span> with the earlier notation). For our machine learning applications we are interested in general inputs <span class="math notranslate nohighlight">\(x \in \mathbb{R}^d\)</span> with scalar random variables <span class="math notranslate nohighlight">\(f(x)\)</span>. In other words, a time-index is too restrictive and therefore, we generalize the idea to general index sets. Moreover, in view of the nice properties of normal distributions an additional property is supposed:</p>
<div class="tip admonition" id="def-gp">
<p class="admonition-title">Definition</p>
<p>A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.<span id="id1">[<a class="reference internal" href="#id13">1</a>]</span></p>
</div>
<p>Henceforth, a Gaussian process is denoted by <span class="math notranslate nohighlight">\((f(x))_{x \in \mathcal{I}}\)</span>. We assume that the random variables <span class="math notranslate nohighlight">\(f(x)\)</span>, <span class="math notranslate nohighlight">\(x \in \mathcal{I}\)</span>, take values in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Moreover, <span class="math notranslate nohighlight">\(\mathcal{I} = \mathbb{R}^d\)</span> for some <span class="math notranslate nohighlight">\(d \in \mathbb{N}\)</span> unless stated otherwise. In this case, <span class="math notranslate nohighlight">\((f(x))_{x \in \mathcal{I}}\)</span> is also called a <strong>random field</strong>. The additional Gaussian condition means that for an arbitrary number of elements <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(i=1, \dots, n\)</span>, the <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>-valued random vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix} f(x_1) \\ f(x_2) \\ \vdots \\ f(x_n) \end{pmatrix}\end{split}\]</div>
<p>is mulivariate normally distributed. By construction the Gaussian process yields random functions from <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Moreover, Brownian motion is a specific Gaussian process.</p>
<p>In <a class="reference internal" href="../../01_fund/01_fundprob/04_impprobdistr.html#def-multnormal"><span class="std std-ref">Normal Distribution</span></a>, we have seen that a multivariate normal distribution is uniquely characterized by its mean <span class="math notranslate nohighlight">\(\mu\)</span> and covariance <span class="math notranslate nohighlight">\(\Sigma\)</span>. A Gaussian process is an infinite-dimensional analogue if the index set is infinite. Indeed, we can select an arbitrary number of <span class="math notranslate nohighlight">\(x_i\)</span> values and consider the corresponding multivariate normal distribution of arbitrary dimension. It turns out that <strong>a Gaussian process is also specified by its mean and covariance, but they are functions instead of a vector and a matrix</strong>. More precisely, the <strong>mean</strong> is a function</p>
<div class="math notranslate nohighlight">
\[m: \mathbb{R}^d \rightarrow \mathbb{R}, x \mapsto m(x)\]</div>
<p>and the <strong>covariance function</strong> or <strong>kernel</strong> is a function of two variables</p>
<div class="math notranslate nohighlight">
\[k: \mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R}, (x, x^{\prime}) \mapsto k(x, x^{\prime})\]</div>
<p>Then, the finite-dimensional distributions of the Gaussian process are given by <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(k\)</span>, i.e., the distribution of</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix} f(x_1) \\ f(x_2) \\ \vdots \\ f(x_n) \end{pmatrix} \sim \mathcal{N}(\mu, \Sigma)\end{split}\]</div>
<p>is specified by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mu = \begin{pmatrix} m(x_1) \\ m(x_2) \\ \vdots \\ m(x_n) \end{pmatrix}\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma = \begin{pmatrix} k(x_1, x_1) &amp; k(x_1, x_2) &amp; \dots &amp; k(x_1, x_n) \\ 
k(x_2, x_1) &amp; k(x_2, x_2) &amp; \dots &amp; k(x_2, x_n) \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
k(x_n, x_1) &amp; k(x_n, x_2) &amp; \dots &amp; k(x_n, x_n) \end{pmatrix}\end{split}\]</div>
<p>Thus, a Gaussian process is specified by choosing a mean function <span class="math notranslate nohighlight">\(m\)</span> and a kernel <span class="math notranslate nohighlight">\(k\)</span>. We also write</p>
<div class="math notranslate nohighlight">
\[ f \sim \mathcal{GP}(m, k).\]</div>
<p>If <span class="math notranslate nohighlight">\(m\)</span> is zero, the corresponding Gaussian process is called <strong>centered</strong>.</p>
<p>The kernel <span class="math notranslate nohighlight">\(k\)</span> generates the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> and the covariance of a multivariate normal distiribution possesses certain properties such as symmetry and positive definiteness. Consequently, a general function of two variables will not be a valid covariance function of a Gaussian process. Clearly, a covariance function is necessarily <strong>symmetric</strong>, i.e., <span class="math notranslate nohighlight">\(k(x, x^{\prime}) = k(x^{\prime}, x)\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathcal{I}\)</span>. <strong>Positive definiteness</strong> of covariance function can also be defined, but requires some additional mathematical background (refer to (4.2) in <span id="id2">[<a class="reference internal" href="#id13">1</a>]</span>).</p>
<p>A kernel <span class="math notranslate nohighlight">\(k\)</span> is called <strong>stationary</strong> if it depends only on the difference of two inputs, i.e., <span class="math notranslate nohighlight">\(k(x, x^{\prime})\)</span> depends only on <span class="math notranslate nohighlight">\(x - x^{\prime}\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathbb{R}^d\)</span>. Furthermore, it is even <strong>isotropic</strong> if it depends only on the distance <span class="math notranslate nohighlight">\(r = |x - x^{\prime}|\)</span>.</p>
<p>The choice of <span class="math notranslate nohighlight">\(k\)</span> determines the properties of the sample paths of the Gaussian process. For example, the paths can be very rough as for Brownian motion or they can be very smooth as for the squared exponential or RBF kernel:</p>
<a class="reference internal image-reference" href="../../_images/smooth.gif"><img alt="Gaussian process" src="../../_images/smooth.gif" style="width: 800px;" /></a>
</div>
<div class="section" id="gaussian-process-regression">
<h2><span class="section-number">7.2.2. </span>Gaussian Process Regression<a class="headerlink" href="#gaussian-process-regression" title="Permalink to this headline">¶</a></h2>
<p>In the machine learning context, Gaussian processes are used for <strong>Gaussian process regression</strong> or <strong>kriging</strong>. We have some data set <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> of observations</p>
<div class="math notranslate nohighlight">
\[\mathcal{D} = \{ (x_i, y_i)~|~x_i \in \mathbb{R}^d, y_i \in \mathbb{R} \quad \text{for } i=1,\dots,n \}\]</div>
<p>similarly to the example in <a class="reference internal" href="../../01_fund/03_bayes/04_linregr.html#sec-linregr"><span class="std std-ref">Linear Regression</span></a>, but now the <strong>functional relation</strong> between inputs <span class="math notranslate nohighlight">\(x_i\)</span> and outputs <span class="math notranslate nohighlight">\(y_i\)</span> is not necessarily linear. Indeed, by assumption the relation is given by</p>
<div class="math notranslate nohighlight">
\[y_i = f(x_i) + \varepsilon_i,\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is some function which is identified with (a sample path of) a suitable Gaussian process <span class="math notranslate nohighlight">\(f\)</span>. Either <span class="math notranslate nohighlight">\(\varepsilon_i=0\)</span> for <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span> if the data is noise-free or <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span>, are independent <span class="math notranslate nohighlight">\(\mathcal{N}(0, \sigma_n^2)\)</span>-distributed random variables if the data is noisy.</p>
<p>The basic idea is to consider only those sample paths of the Gaussian process which match the data. From the Bayesian point of view, the initial Gaussian process defines a <strong>prior distribution over functions</strong> and restriction to fitting sample paths yields the <strong>posterior distribution over functions</strong> given the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. The follwing animation visualizes samples paths from the posterior distribution in use of the squared exponential kernel and eight noise-free observations from the sine function:</p>
<a class="reference internal image-reference" href="../../_images/gaussianregr.gif"><img alt="Cond Gaussian process" src="../../_images/gaussianregr.gif" style="width: 800px;" /></a>
<p>Typically, the <strong>mean function is chosen to be zero</strong>. Otherwise <span class="math notranslate nohighlight">\(m\)</span> would already approximate the dependence of the output <span class="math notranslate nohighlight">\(y\)</span> of the input <span class="math notranslate nohighlight">\(x\)</span>. Learning this relation is purpose of the regression model. However, if we still like to use a non-zero mean <span class="math notranslate nohighlight">\(m\)</span>, this can also be done by substracting <span class="math notranslate nohighlight">\(m(x_i)\)</span> from <span class="math notranslate nohighlight">\(y_i\)</span> and considering a centered model on the residuals. Thus, we always make the assumption <span class="math notranslate nohighlight">\(m=0\)</span>, i.e., <strong>we only consider centered Gaussian processes</strong>.</p>
<p>For a test point <span class="math notranslate nohighlight">\(x^*\)</span> the <strong>distribution of</strong> <span class="math notranslate nohighlight">\(f(x^*)\)</span> <strong>given the data</strong> <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is the <a class="reference internal" href="../../01_fund/01_fundprob/05_essthms.html#lem-condnormaldistr"><span class="std std-ref">conditional distribution</span></a> which can be computed explicitly, since we are dealing with normal distributions. The prediction of the model at <span class="math notranslate nohighlight">\(x^*\)</span> is given by the mean of the conditional distribution and the uncertainty is quantified by the variance which corresponds to the variability of the string in the preceding animation. As seen before, this uncertainty can also be expressed in terms of credible intervals:</p>
<img alt="Cond Gaussian process" src="../../_images/gaussianregr.png" />
<p>The following interactive plot shows how the distribution of one component of a two dimensional normally distributed random vector behaves if the other component is fixed. The main observation is that the fixed value has no impact on the other component if the correlation is zero. In this case, the two components are independent. Moreover, the impact increases as the absolute value of the correlation increases. This means for Gaussian process regression that the impact of training points <span class="math notranslate nohighlight">\(x\)</span> on the prediction for test points <span class="math notranslate nohighlight">\(x^*\)</span> depends on the variance <span class="math notranslate nohighlight">\(k(x, x^*)\)</span>. Please open the notebook in Google Colab to use the visualization.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span>
<span class="o">!</span>pip install ipympl
<span class="n">clear_output</span><span class="p">()</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_formats = [&#39;svg&#39;]

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.gridspec</span> <span class="k">as</span> <span class="nn">gridspec</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">FloatSlider</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.axes_grid1</span> <span class="kn">import</span> <span class="n">make_axes_locatable</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>

<span class="c1"># Plot bivariate distribution</span>
<span class="k">def</span> <span class="nf">generate_surface</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Helper function to generate density surface.&quot;&quot;&quot;</span>
    <span class="n">nb_of_x</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># grid size</span>
    <span class="n">x1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">nb_of_x</span><span class="p">)</span>
    <span class="n">x2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">nb_of_x</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2s</span><span class="p">)</span> <span class="c1"># Generate grid</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">pdf</span>

<span class="nd">@interact</span><span class="p">(</span><span class="n">x_condition</span><span class="o">=</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
          <span class="n">y_condition</span><span class="o">=</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=-</span><span class="mf">1.</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
          <span class="n">correlation</span><span class="o">=</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.995</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.995</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">plot_conddistr</span><span class="p">(</span><span class="n">x_condition</span><span class="p">,</span> <span class="n">y_condition</span><span class="p">,</span> <span class="n">correlation</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># dimensions</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]])</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">correlation</span><span class="p">],</span> 
        <span class="p">[</span><span class="n">correlation</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="p">])</span>

    <span class="c1"># Get the mean values from the vector</span>
    <span class="n">mean_x</span> <span class="o">=</span> <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mean_y</span> <span class="o">=</span> <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Get the blocks (single values in this case) from </span>
    <span class="c1">#  the covariance matrix</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">cov</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># = C transpose in this case</span>
    <span class="c1"># Calculate x|y</span>
    <span class="n">mean_xgiveny</span> <span class="o">=</span> <span class="n">mean_x</span> <span class="o">+</span> <span class="n">C</span> <span class="o">/</span><span class="n">B</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_condition</span> <span class="o">-</span> <span class="n">mean_y</span><span class="p">)</span>
    <span class="n">cov_xgiveny</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">C</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span><span class="n">B</span>

    <span class="c1"># Calculate y|x</span>
    <span class="n">mean_ygivenx</span> <span class="o">=</span> <span class="n">mean_y</span> <span class="o">+</span> <span class="n">C</span> <span class="o">/</span><span class="n">A</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_condition</span> <span class="o">-</span> <span class="n">mean_x</span><span class="p">)</span>
    <span class="n">cov_ygivenx</span> <span class="o">=</span> <span class="n">B</span> <span class="o">-</span> <span class="n">C</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span><span class="n">A</span>
    <span class="c1"># Plot the conditional distributions</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
    <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span>
        <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">height_ratios</span><span class="o">=</span><span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">hspace</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Conditional distributions&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.93</span><span class="p">)</span>

    <span class="c1"># Plot surface on top left</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">generate_surface</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="c1"># Plot bivariate distribution</span>
    <span class="n">con</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;turbo&#39;</span><span class="p">)</span>
    <span class="c1"># y=1 that is conditioned upon</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">],</span> <span class="p">[</span><span class="n">y_condition</span><span class="p">,</span> <span class="n">y_condition</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">)</span>
    <span class="c1"># x=-1. that is conditioned upon</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x_condition</span><span class="p">,</span> <span class="n">x_condition</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">],</span> <span class="s1">&#39;b--&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>

    <span class="c1"># Plot y|x</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">yx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">250</span><span class="p">)</span>
    <span class="n">pyx</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">yx</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean_ygivenx</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov_ygivenx</span><span class="p">))</span>
    <span class="c1"># Plot univariate distribution</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pyx</span><span class="p">,</span> <span class="n">yx</span><span class="p">,</span> <span class="s1">&#39;b--&#39;</span><span class="p">,</span> 
             <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;$p(y|x=</span><span class="si">{</span><span class="n">x_condition</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">)$&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;symlog&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">ticker</span><span class="o">.</span><span class="n">ScalarFormatter</span><span class="p">())</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
    <span class="n">title2</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\mu_{y|x} =$&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">{:.2f}</span><span class="s1">, &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mean_ygivenx</span><span class="p">)</span>
    <span class="n">title2</span> <span class="o">+=</span> <span class="sa">r</span><span class="s1">&#39;$\sigma_{y|x}^2 =$&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cov_ygivenx</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title2</span><span class="p">)</span>

    <span class="c1"># Plot x|y</span>
    <span class="n">ax3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">250</span><span class="p">)</span>
    <span class="n">pxy</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean_xgiveny</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov_xgiveny</span><span class="p">))</span>
    <span class="c1"># Plot univariate distribution</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">pxy</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> 
             <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;$p(x|y=</span><span class="si">{</span><span class="n">y_condition</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">)$&#39;</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;symlog&#39;</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">ticker</span><span class="o">.</span><span class="n">ScalarFormatter</span><span class="p">())</span>
    <span class="n">title3</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\mu_{x|y} =$&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">{:.2f}</span><span class="s1">, &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mean_xgiveny</span><span class="p">)</span>
    <span class="n">title3</span> <span class="o">+=</span> <span class="sa">r</span><span class="s1">&#39;$\sigma_{x|y}^2 =$&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cov_xgiveny</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title3</span><span class="p">)</span>

    <span class="c1"># Clear axis 4 and plot colarbar in its place</span>
    <span class="n">ax4</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">ax4</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">ax4</span><span class="p">)</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="o">.</span><span class="n">append_axes</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s1">&#39;20%&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">cbar</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">con</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">)</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density: $p(x, y)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "bdee048fd7484995aad56f578419bd47"}
</script></div>
</div>
<p>The general equations for Gaussian process regression without noise are derived as follows:</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}X = \begin{pmatrix} x^T_1 \\ x^T_2 \\ \vdots \\ x^T_n \end{pmatrix}\end{split}\]</div>
<p>be the so called <strong>sample matrix</strong> and denote by</p>
<div class="math notranslate nohighlight">
\[\begin{split}y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}\end{split}\]</div>
<p>the associated <strong>labels</strong>. <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span> split the training data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> into inputs and outputs. Assume that we have some test points</p>
<div class="math notranslate nohighlight">
\[\begin{split}X^* = \begin{pmatrix} {x^*}^T_1 \\ {x^*}^T_2 \\ \vdots \\ {x^*}^T_m \end{pmatrix}\end{split}\]</div>
<p>for some <span class="math notranslate nohighlight">\(m \in \mathbb{N}\)</span>. Our goal is to determine the distribution of</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(X^*) := \begin{pmatrix} f(x^*_1) \\ f(x^*_2) \\ \vdots \\ f(x^*_m) \end{pmatrix}\end{split}\]</div>
<p>given</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix} f(x_1) \\ f(x_2) \\ \vdots \\ f(x_n) \end{pmatrix} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}\end{split}\]</div>
<p>Since we use a Gaussian process with zero mean, we know that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix} f(x^*_1) \\ f(x^*_2) \\ \vdots \\ f(x^*_m) \\ f(x_1) \\ f(x_2) \\ \vdots \\ f(x_n) \end{pmatrix} \sim \mathcal{N}(0, \Sigma)\end{split}\]</div>
<p>The kernel <span class="math notranslate nohighlight">\(k\)</span> determines the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>. We use the notation</p>
<div class="math notranslate nohighlight">
\[\begin{split}K(X, X^*) = \begin{pmatrix} k(x_1, x^*_1) &amp; k(x_1, x^*_2) &amp; \dots &amp; k(x_1, x^*_m) \\ 
k(x_2, x^*_1) &amp; k(x_2, x^*_2) &amp; \dots &amp; k(x_2, x^*_m) \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
k(x_n, x^*_1) &amp; k(x_n, x^*_2) &amp; \dots &amp; k(x_n, x^*_m) \end{pmatrix}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(K(X, X)\)</span>, <span class="math notranslate nohighlight">\(K(X^*, X^*)\)</span> and <span class="math notranslate nohighlight">\(K(X^*, X)\)</span> are defined accordingly. Consequently, it follows</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma = \begin{pmatrix} K(X^*, X^*) &amp; K(X^*, X) \\ K(X, X^*) &amp; K(X, X) \end{pmatrix}\end{split}\]</div>
<p>Finally, we can apply <a class="reference internal" href="../../01_fund/01_fundprob/05_essthms.html#lem-condnormaldistr"><span class="std std-ref">conditional distribution formula</span></a> to deduce the following:</p>
<div class="important admonition" id="lem-gpregr">
<p class="admonition-title">Lemma</p>
<p>Gaussian process regression predicts the distribution of <span class="math notranslate nohighlight">\(f(X^*)\)</span> given the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> by a multivariate normally distributed with mean</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(f(X^*)) = K(X^*, X) K(X, X)^{-1} y\]</div>
<p>and covariance</p>
<div class="math notranslate nohighlight">
\[\text{cov}(f(X^*)) = K(X^*, X^*) - K(X^*, X) K(X, X)^{-1} K(X, X^*)\]</div>
</div>
<p>The term <span class="math notranslate nohighlight">\(\alpha := K(X, X)^{-1} y\)</span> is a vector of size <span class="math notranslate nohighlight">\(n\)</span> which is independent of <span class="math notranslate nohighlight">\(x^*\)</span>. Moreover, for a single test point <span class="math notranslate nohighlight">\(x^*\)</span> the covariance matrix <span class="math notranslate nohighlight">\(K(x^*, X)\)</span> reduces to a row vector. Hence, it holds</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(f(x^*)) = K(x^*, X) \alpha = \sum_{i=1}^n \alpha_i~ k(x^*, x_i).\]</div>
<p>In other words, the <strong>mean prediction is a linear combination of the functions</strong> <span class="math notranslate nohighlight">\(k(\cdot, x_i)\)</span>, <span class="math notranslate nohighlight">\(i=1, \dots, n\)</span>. The weights <span class="math notranslate nohighlight">\(\alpha_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span>, are constructed such that the training data is fitted exactly (see plot above). This is <strong>only possible if the inverse matrix</strong> <span class="math notranslate nohighlight">\(K(X, X)^{-1}\)</span> <strong>exists</strong>. This might not always the case. Think of the linear regression example including noise in the data. In this case, it is not possible to find a linear function which fits the data exactly. Furthermore, we possibly do not like to obtain a perfect fit, since we suppose that our training data contains noise. In order to solve this issue, a <strong>noise term</strong> <span class="math notranslate nohighlight">\(\sigma_{\text{noise}}^2 &gt; 0\)</span> is added to the covariance related to the training data, i.e., <span class="math notranslate nohighlight">\(K(X, X)\)</span> is replaced by <span class="math notranslate nohighlight">\(K(X, X) + \sigma_{\text{noise}}^2 I_n\)</span>, where <span class="math notranslate nohighlight">\(I_n\)</span> denotes the identity matrix. In this way, the variance of <span class="math notranslate nohighlight">\(f(x_i)\)</span>, <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span>, is increased by <span class="math notranslate nohighlight">\(\sigma_{\text{noise}}^2\)</span>. Adding noise is additionally useful to <strong>avoid numerical problems</strong>, since it has a regularizing effect. Indeed, the inverse matrix <span class="math notranslate nohighlight">\(K(X, X)^{-1}\)</span> might exist mathematically. However, if the matrix is “almost singular”, it is often not possible to compute <span class="math notranslate nohighlight">\(K(X, X)^{-1}\)</span>. It holds</p>
<div class="important admonition" id="lem-gpregrnoise">
<p class="admonition-title">Lemma</p>
<p>Gaussian process regression with noise <span class="math notranslate nohighlight">\(\sigma_{\text{noise}}^2\)</span> predicts the distribution of <span class="math notranslate nohighlight">\(f(X^*)\)</span> given the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> by a multivariate normally distributed with mean</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(f(X^*)) = K(X^*, X) \big(K(X, X) + \sigma_{\text{noise}}^2 I_n\big)^{-1} y\]</div>
<p>and covariance</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(f(X^*)) = K(X^*, X^*) - K(X^*, X) \big(K(X, X) + \sigma_{\text{noise}}^2 I_n\big)^{-1} K(X, X^*)\]</div>
</div>
<p>It is also possible to incorporate the <a class="reference internal" href="#lem-gpregrnoise"><span class="std std-ref">result with noise</span></a> into the <a class="reference internal" href="#lem-gpregr"><span class="std std-ref">case without noise</span></a> by replacing the kernel <span class="math notranslate nohighlight">\(k\)</span> by <span class="math notranslate nohighlight">\(k + \sigma_{\text{noise}}^2 \delta\)</span>, where</p>
<div class="math notranslate nohighlight">
\[\begin{split} \delta(x, x') = \begin{cases} 1, &amp; \text{if } x = x^{\prime} \\
                                 0, &amp; \text{if } x \ne x^{\prime}
                   \end{cases}
\end{split}\]</div>
<p>In this way, the noise <span class="math notranslate nohighlight">\(\sigma_{\text{noise}}^2\)</span> is added to the diagonal of <span class="math notranslate nohighlight">\(K(X, X)\)</span> and the notation is maintained. However, this approach does not work if a test point <span class="math notranslate nohighlight">\(X^*\)</span> is also in <span class="math notranslate nohighlight">\(X\)</span>, since the noise should not appear in <span class="math notranslate nohighlight">\(K(X^*, X)\)</span>.</p>
</div>
<div class="section" id="examples-of-kernels">
<h2><span class="section-number">7.2.3. </span>Examples of Kernels<a class="headerlink" href="#examples-of-kernels" title="Permalink to this headline">¶</a></h2>
<p>As mentioned before, the choice of the kernel (the prior distribution) determines the properties of the Gaussian process and consequently also of the regression model. In the present section, we define the most common covariance functions and visualize the corresponding sample paths.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> notebook
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span><span class="p">,</span> <span class="n">FFMpegFileWriter</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>

<span class="c1"># animation for paths of a Gaussian process</span>
<span class="c1"># kernel specifies the covariance function</span>
<span class="c1"># x-values from -xbnd to xbnd</span>
<span class="c1"># y-axis has values from -ybnd to ybnd</span>
<span class="c1"># saves gif if string is passed to name</span>
<span class="k">def</span> <span class="nf">get_anim</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">xbnd</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">ybnd</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">delta_x</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">xbnd</span> <span class="o">/</span> <span class="n">nb_steps</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">xbnd</span><span class="p">,</span> <span class="n">xbnd</span><span class="p">,</span> <span class="n">delta_x</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># First set up the figure, the axis, and the plot element we want to animate</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">xbnd</span><span class="p">,</span> <span class="n">xbnd</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">ybnd</span><span class="p">,</span> <span class="n">ybnd</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># initialization function: plot the background of each frame</span>
    <span class="k">def</span> <span class="nf">init</span><span class="p">():</span>
        <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],</span> <span class="p">[])</span>
        <span class="k">return</span> <span class="n">line</span><span class="p">,</span>

    <span class="c1"># animation function.  This is called sequentially</span>
    <span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">xbnd</span><span class="p">,</span> <span class="n">xbnd</span><span class="p">,</span> <span class="n">delta_x</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>
        <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">line</span><span class="p">,</span>

    <span class="c1"># call the animator.  blit=True means only re-draw the parts that have changed.</span>
    <span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">init_func</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                         <span class="n">frames</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1">#, repeat=False)</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">anim</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="n">FFMpegFileWriter</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#, dpi=200)</span>
    <span class="k">return</span> <span class="n">anim</span>

<span class="c1"># common kernels:</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">DotProduct</span> <span class="c1"># linear kernel</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">polynomial_kernel</span> <span class="c1"># polynomial kernel</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">RBF</span> <span class="c1"># squared exponential kernel</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">Matern</span> <span class="c1"># Matern kernel</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">RationalQuadratic</span> <span class="c1"># rational quadratic kernel</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ExpSineSquared</span> <span class="c1"># periodic kernel</span>

<span class="c1"># example</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">Matern</span><span class="p">(</span><span class="n">nu</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span> 
<span class="n">anim</span> <span class="o">=</span> <span class="n">get_anim</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">xbnd</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">ybnd</span><span class="o">=</span><span class="mf">5.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/javascript">/* Put everything inside the global mpl namespace */
/* global mpl */
window.mpl = {};

mpl.get_websocket_type = function () {
    if (typeof WebSocket !== 'undefined') {
        return WebSocket;
    } else if (typeof MozWebSocket !== 'undefined') {
        return MozWebSocket;
    } else {
        alert(
            'Your browser does not have WebSocket support. ' +
                'Please try Chrome, Safari or Firefox ≥ 6. ' +
                'Firefox 4 and 5 are also supported but you ' +
                'have to enable WebSockets in about:config.'
        );
    }
};

mpl.figure = function (figure_id, websocket, ondownload, parent_element) {
    this.id = figure_id;

    this.ws = websocket;

    this.supports_binary = this.ws.binaryType !== undefined;

    if (!this.supports_binary) {
        var warnings = document.getElementById('mpl-warnings');
        if (warnings) {
            warnings.style.display = 'block';
            warnings.textContent =
                'This browser does not support binary websocket messages. ' +
                'Performance may be slow.';
        }
    }

    this.imageObj = new Image();

    this.context = undefined;
    this.message = undefined;
    this.canvas = undefined;
    this.rubberband_canvas = undefined;
    this.rubberband_context = undefined;
    this.format_dropdown = undefined;

    this.image_mode = 'full';

    this.root = document.createElement('div');
    this.root.setAttribute('style', 'display: inline-block');
    this._root_extra_style(this.root);

    parent_element.appendChild(this.root);

    this._init_header(this);
    this._init_canvas(this);
    this._init_toolbar(this);

    var fig = this;

    this.waiting = false;

    this.ws.onopen = function () {
        fig.send_message('supports_binary', { value: fig.supports_binary });
        fig.send_message('send_image_mode', {});
        if (fig.ratio !== 1) {
            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });
        }
        fig.send_message('refresh', {});
    };

    this.imageObj.onload = function () {
        if (fig.image_mode === 'full') {
            // Full images could contain transparency (where diff images
            // almost always do), so we need to clear the canvas so that
            // there is no ghosting.
            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);
        }
        fig.context.drawImage(fig.imageObj, 0, 0);
    };

    this.imageObj.onunload = function () {
        fig.ws.close();
    };

    this.ws.onmessage = this._make_on_message_function(this);

    this.ondownload = ondownload;
};

mpl.figure.prototype._init_header = function () {
    var titlebar = document.createElement('div');
    titlebar.classList =
        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';
    var titletext = document.createElement('div');
    titletext.classList = 'ui-dialog-title';
    titletext.setAttribute(
        'style',
        'width: 100%; text-align: center; padding: 3px;'
    );
    titlebar.appendChild(titletext);
    this.root.appendChild(titlebar);
    this.header = titletext;
};

mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};

mpl.figure.prototype._root_extra_style = function (_canvas_div) {};

mpl.figure.prototype._init_canvas = function () {
    var fig = this;

    var canvas_div = (this.canvas_div = document.createElement('div'));
    canvas_div.setAttribute(
        'style',
        'border: 1px solid #ddd;' +
            'box-sizing: content-box;' +
            'clear: both;' +
            'min-height: 1px;' +
            'min-width: 1px;' +
            'outline: 0;' +
            'overflow: hidden;' +
            'position: relative;' +
            'resize: both;'
    );

    function on_keyboard_event_closure(name) {
        return function (event) {
            return fig.key_event(event, name);
        };
    }

    canvas_div.addEventListener(
        'keydown',
        on_keyboard_event_closure('key_press')
    );
    canvas_div.addEventListener(
        'keyup',
        on_keyboard_event_closure('key_release')
    );

    this._canvas_extra_style(canvas_div);
    this.root.appendChild(canvas_div);

    var canvas = (this.canvas = document.createElement('canvas'));
    canvas.classList.add('mpl-canvas');
    canvas.setAttribute('style', 'box-sizing: content-box;');

    this.context = canvas.getContext('2d');

    var backingStore =
        this.context.backingStorePixelRatio ||
        this.context.webkitBackingStorePixelRatio ||
        this.context.mozBackingStorePixelRatio ||
        this.context.msBackingStorePixelRatio ||
        this.context.oBackingStorePixelRatio ||
        this.context.backingStorePixelRatio ||
        1;

    this.ratio = (window.devicePixelRatio || 1) / backingStore;

    var rubberband_canvas = (this.rubberband_canvas = document.createElement(
        'canvas'
    ));
    rubberband_canvas.setAttribute(
        'style',
        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'
    );

    // Apply a ponyfill if ResizeObserver is not implemented by browser.
    if (this.ResizeObserver === undefined) {
        if (window.ResizeObserver !== undefined) {
            this.ResizeObserver = window.ResizeObserver;
        } else {
            var obs = _JSXTOOLS_RESIZE_OBSERVER({});
            this.ResizeObserver = obs.ResizeObserver;
        }
    }

    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {
        var nentries = entries.length;
        for (var i = 0; i < nentries; i++) {
            var entry = entries[i];
            var width, height;
            if (entry.contentBoxSize) {
                if (entry.contentBoxSize instanceof Array) {
                    // Chrome 84 implements new version of spec.
                    width = entry.contentBoxSize[0].inlineSize;
                    height = entry.contentBoxSize[0].blockSize;
                } else {
                    // Firefox implements old version of spec.
                    width = entry.contentBoxSize.inlineSize;
                    height = entry.contentBoxSize.blockSize;
                }
            } else {
                // Chrome <84 implements even older version of spec.
                width = entry.contentRect.width;
                height = entry.contentRect.height;
            }

            // Keep the size of the canvas and rubber band canvas in sync with
            // the canvas container.
            if (entry.devicePixelContentBoxSize) {
                // Chrome 84 implements new version of spec.
                canvas.setAttribute(
                    'width',
                    entry.devicePixelContentBoxSize[0].inlineSize
                );
                canvas.setAttribute(
                    'height',
                    entry.devicePixelContentBoxSize[0].blockSize
                );
            } else {
                canvas.setAttribute('width', width * fig.ratio);
                canvas.setAttribute('height', height * fig.ratio);
            }
            canvas.setAttribute(
                'style',
                'width: ' + width + 'px; height: ' + height + 'px;'
            );

            rubberband_canvas.setAttribute('width', width);
            rubberband_canvas.setAttribute('height', height);

            // And update the size in Python. We ignore the initial 0/0 size
            // that occurs as the element is placed into the DOM, which should
            // otherwise not happen due to the minimum size styling.
            if (fig.ws.readyState == 1 && width != 0 && height != 0) {
                fig.request_resize(width, height);
            }
        }
    });
    this.resizeObserverInstance.observe(canvas_div);

    function on_mouse_event_closure(name) {
        return function (event) {
            return fig.mouse_event(event, name);
        };
    }

    rubberband_canvas.addEventListener(
        'mousedown',
        on_mouse_event_closure('button_press')
    );
    rubberband_canvas.addEventListener(
        'mouseup',
        on_mouse_event_closure('button_release')
    );
    rubberband_canvas.addEventListener(
        'dblclick',
        on_mouse_event_closure('dblclick')
    );
    // Throttle sequential mouse events to 1 every 20ms.
    rubberband_canvas.addEventListener(
        'mousemove',
        on_mouse_event_closure('motion_notify')
    );

    rubberband_canvas.addEventListener(
        'mouseenter',
        on_mouse_event_closure('figure_enter')
    );
    rubberband_canvas.addEventListener(
        'mouseleave',
        on_mouse_event_closure('figure_leave')
    );

    canvas_div.addEventListener('wheel', function (event) {
        if (event.deltaY < 0) {
            event.step = 1;
        } else {
            event.step = -1;
        }
        on_mouse_event_closure('scroll')(event);
    });

    canvas_div.appendChild(canvas);
    canvas_div.appendChild(rubberband_canvas);

    this.rubberband_context = rubberband_canvas.getContext('2d');
    this.rubberband_context.strokeStyle = '#000000';

    this._resize_canvas = function (width, height, forward) {
        if (forward) {
            canvas_div.style.width = width + 'px';
            canvas_div.style.height = height + 'px';
        }
    };

    // Disable right mouse context menu.
    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {
        event.preventDefault();
        return false;
    });

    function set_focus() {
        canvas.focus();
        canvas_div.focus();
    }

    window.setTimeout(set_focus, 100);
};

mpl.figure.prototype._init_toolbar = function () {
    var fig = this;

    var toolbar = document.createElement('div');
    toolbar.classList = 'mpl-toolbar';
    this.root.appendChild(toolbar);

    function on_click_closure(name) {
        return function (_event) {
            return fig.toolbar_button_onclick(name);
        };
    }

    function on_mouseover_closure(tooltip) {
        return function (event) {
            if (!event.currentTarget.disabled) {
                return fig.toolbar_button_onmouseover(tooltip);
            }
        };
    }

    fig.buttons = {};
    var buttonGroup = document.createElement('div');
    buttonGroup.classList = 'mpl-button-group';
    for (var toolbar_ind in mpl.toolbar_items) {
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) {
            /* Instead of a spacer, we start a new button group. */
            if (buttonGroup.hasChildNodes()) {
                toolbar.appendChild(buttonGroup);
            }
            buttonGroup = document.createElement('div');
            buttonGroup.classList = 'mpl-button-group';
            continue;
        }

        var button = (fig.buttons[name] = document.createElement('button'));
        button.classList = 'mpl-widget';
        button.setAttribute('role', 'button');
        button.setAttribute('aria-disabled', 'false');
        button.addEventListener('click', on_click_closure(method_name));
        button.addEventListener('mouseover', on_mouseover_closure(tooltip));

        var icon_img = document.createElement('img');
        icon_img.src = '_images/' + image + '.png';
        icon_img.srcset = '_images/' + image + '_large.png 2x';
        icon_img.alt = tooltip;
        button.appendChild(icon_img);

        buttonGroup.appendChild(button);
    }

    if (buttonGroup.hasChildNodes()) {
        toolbar.appendChild(buttonGroup);
    }

    var fmt_picker = document.createElement('select');
    fmt_picker.classList = 'mpl-widget';
    toolbar.appendChild(fmt_picker);
    this.format_dropdown = fmt_picker;

    for (var ind in mpl.extensions) {
        var fmt = mpl.extensions[ind];
        var option = document.createElement('option');
        option.selected = fmt === mpl.default_extension;
        option.innerHTML = fmt;
        fmt_picker.appendChild(option);
    }

    var status_bar = document.createElement('span');
    status_bar.classList = 'mpl-message';
    toolbar.appendChild(status_bar);
    this.message = status_bar;
};

mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {
    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,
    // which will in turn request a refresh of the image.
    this.send_message('resize', { width: x_pixels, height: y_pixels });
};

mpl.figure.prototype.send_message = function (type, properties) {
    properties['type'] = type;
    properties['figure_id'] = this.id;
    this.ws.send(JSON.stringify(properties));
};

mpl.figure.prototype.send_draw_message = function () {
    if (!this.waiting) {
        this.waiting = true;
        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));
    }
};

mpl.figure.prototype.handle_save = function (fig, _msg) {
    var format_dropdown = fig.format_dropdown;
    var format = format_dropdown.options[format_dropdown.selectedIndex].value;
    fig.ondownload(fig, format);
};

mpl.figure.prototype.handle_resize = function (fig, msg) {
    var size = msg['size'];
    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {
        fig._resize_canvas(size[0], size[1], msg['forward']);
        fig.send_message('refresh', {});
    }
};

mpl.figure.prototype.handle_rubberband = function (fig, msg) {
    var x0 = msg['x0'] / fig.ratio;
    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;
    var x1 = msg['x1'] / fig.ratio;
    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;
    x0 = Math.floor(x0) + 0.5;
    y0 = Math.floor(y0) + 0.5;
    x1 = Math.floor(x1) + 0.5;
    y1 = Math.floor(y1) + 0.5;
    var min_x = Math.min(x0, x1);
    var min_y = Math.min(y0, y1);
    var width = Math.abs(x1 - x0);
    var height = Math.abs(y1 - y0);

    fig.rubberband_context.clearRect(
        0,
        0,
        fig.canvas.width / fig.ratio,
        fig.canvas.height / fig.ratio
    );

    fig.rubberband_context.strokeRect(min_x, min_y, width, height);
};

mpl.figure.prototype.handle_figure_label = function (fig, msg) {
    // Updates the figure title.
    fig.header.textContent = msg['label'];
};

mpl.figure.prototype.handle_cursor = function (fig, msg) {
    var cursor = msg['cursor'];
    switch (cursor) {
        case 0:
            cursor = 'pointer';
            break;
        case 1:
            cursor = 'default';
            break;
        case 2:
            cursor = 'crosshair';
            break;
        case 3:
            cursor = 'move';
            break;
    }
    fig.rubberband_canvas.style.cursor = cursor;
};

mpl.figure.prototype.handle_message = function (fig, msg) {
    fig.message.textContent = msg['message'];
};

mpl.figure.prototype.handle_draw = function (fig, _msg) {
    // Request the server to send over a new figure.
    fig.send_draw_message();
};

mpl.figure.prototype.handle_image_mode = function (fig, msg) {
    fig.image_mode = msg['mode'];
};

mpl.figure.prototype.handle_history_buttons = function (fig, msg) {
    for (var key in msg) {
        if (!(key in fig.buttons)) {
            continue;
        }
        fig.buttons[key].disabled = !msg[key];
        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);
    }
};

mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {
    if (msg['mode'] === 'PAN') {
        fig.buttons['Pan'].classList.add('active');
        fig.buttons['Zoom'].classList.remove('active');
    } else if (msg['mode'] === 'ZOOM') {
        fig.buttons['Pan'].classList.remove('active');
        fig.buttons['Zoom'].classList.add('active');
    } else {
        fig.buttons['Pan'].classList.remove('active');
        fig.buttons['Zoom'].classList.remove('active');
    }
};

mpl.figure.prototype.updated_canvas_event = function () {
    // Called whenever the canvas gets updated.
    this.send_message('ack', {});
};

// A function to construct a web socket function for onmessage handling.
// Called in the figure constructor.
mpl.figure.prototype._make_on_message_function = function (fig) {
    return function socket_on_message(evt) {
        if (evt.data instanceof Blob) {
            var img = evt.data;
            if (img.type !== 'image/png') {
                /* FIXME: We get "Resource interpreted as Image but
                 * transferred with MIME type text/plain:" errors on
                 * Chrome.  But how to set the MIME type?  It doesn't seem
                 * to be part of the websocket stream */
                img.type = 'image/png';
            }

            /* Free the memory for the previous frames */
            if (fig.imageObj.src) {
                (window.URL || window.webkitURL).revokeObjectURL(
                    fig.imageObj.src
                );
            }

            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(
                img
            );
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        } else if (
            typeof evt.data === 'string' &&
            evt.data.slice(0, 21) === 'data:image/png;base64'
        ) {
            fig.imageObj.src = evt.data;
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }

        var msg = JSON.parse(evt.data);
        var msg_type = msg['type'];

        // Call the  "handle_{type}" callback, which takes
        // the figure and JSON message as its only arguments.
        try {
            var callback = fig['handle_' + msg_type];
        } catch (e) {
            console.log(
                "No handler for the '" + msg_type + "' message type: ",
                msg
            );
            return;
        }

        if (callback) {
            try {
                // console.log("Handling '" + msg_type + "' message: ", msg);
                callback(fig, msg);
            } catch (e) {
                console.log(
                    "Exception inside the 'handler_" + msg_type + "' callback:",
                    e,
                    e.stack,
                    msg
                );
            }
        }
    };
};

// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas
mpl.findpos = function (e) {
    //this section is from http://www.quirksmode.org/js/events_properties.html
    var targ;
    if (!e) {
        e = window.event;
    }
    if (e.target) {
        targ = e.target;
    } else if (e.srcElement) {
        targ = e.srcElement;
    }
    if (targ.nodeType === 3) {
        // defeat Safari bug
        targ = targ.parentNode;
    }

    // pageX,Y are the mouse positions relative to the document
    var boundingRect = targ.getBoundingClientRect();
    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);
    var y = e.pageY - (boundingRect.top + document.body.scrollTop);

    return { x: x, y: y };
};

/*
 * return a copy of an object with only non-object keys
 * we need this to avoid circular references
 * http://stackoverflow.com/a/24161582/3208463
 */
function simpleKeys(original) {
    return Object.keys(original).reduce(function (obj, key) {
        if (typeof original[key] !== 'object') {
            obj[key] = original[key];
        }
        return obj;
    }, {});
}

mpl.figure.prototype.mouse_event = function (event, name) {
    var canvas_pos = mpl.findpos(event);

    if (name === 'button_press') {
        this.canvas.focus();
        this.canvas_div.focus();
    }

    var x = canvas_pos.x * this.ratio;
    var y = canvas_pos.y * this.ratio;

    this.send_message(name, {
        x: x,
        y: y,
        button: event.button,
        step: event.step,
        guiEvent: simpleKeys(event),
    });

    /* This prevents the web browser from automatically changing to
     * the text insertion cursor when the button is pressed.  We want
     * to control all of the cursor setting manually through the
     * 'cursor' event from matplotlib */
    event.preventDefault();
    return false;
};

mpl.figure.prototype._key_event_extra = function (_event, _name) {
    // Handle any extra behaviour associated with a key event
};

mpl.figure.prototype.key_event = function (event, name) {
    // Prevent repeat events
    if (name === 'key_press') {
        if (event.key === this._key) {
            return;
        } else {
            this._key = event.key;
        }
    }
    if (name === 'key_release') {
        this._key = null;
    }

    var value = '';
    if (event.ctrlKey && event.key !== 'Control') {
        value += 'ctrl+';
    }
    else if (event.altKey && event.key !== 'Alt') {
        value += 'alt+';
    }
    else if (event.shiftKey && event.key !== 'Shift') {
        value += 'shift+';
    }

    value += 'k' + event.key;

    this._key_event_extra(event, name);

    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });
    return false;
};

mpl.figure.prototype.toolbar_button_onclick = function (name) {
    if (name === 'download') {
        this.handle_save(this, null);
    } else {
        this.send_message('toolbar_button', { name: name });
    }
};

mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {
    this.message.textContent = tooltip;
};

///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////
// prettier-ignore
var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError("Constructor requires 'new' operator");i.set(this,e)}function h(){throw new TypeError("Function is not a constructor")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line
mpl.toolbar_items = [["Home", "Reset original view", "fa fa-home icon-home", "home"], ["Back", "Back to previous view", "fa fa-arrow-left icon-arrow-left", "back"], ["Forward", "Forward to next view", "fa fa-arrow-right icon-arrow-right", "forward"], ["", "", "", ""], ["Pan", "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect", "fa fa-arrows icon-move", "pan"], ["Zoom", "Zoom to rectangle\nx/y fixes axis, CTRL fixes aspect", "fa fa-square-o icon-check-empty", "zoom"], ["", "", "", ""], ["Download", "Download plot", "fa fa-floppy-o icon-save", "download"]];

mpl.extensions = ["eps", "jpeg", "pgf", "pdf", "png", "ps", "raw", "svg", "tif"];

mpl.default_extension = "png";/* global mpl */

var comm_websocket_adapter = function (comm) {
    // Create a "websocket"-like object which calls the given IPython comm
    // object with the appropriate methods. Currently this is a non binary
    // socket, so there is still some room for performance tuning.
    var ws = {};

    ws.binaryType = comm.kernel.ws.binaryType;
    ws.readyState = comm.kernel.ws.readyState;
    function updateReadyState(_event) {
        if (comm.kernel.ws) {
            ws.readyState = comm.kernel.ws.readyState;
        } else {
            ws.readyState = 3; // Closed state.
        }
    }
    comm.kernel.ws.addEventListener('open', updateReadyState);
    comm.kernel.ws.addEventListener('close', updateReadyState);
    comm.kernel.ws.addEventListener('error', updateReadyState);

    ws.close = function () {
        comm.close();
    };
    ws.send = function (m) {
        //console.log('sending', m);
        comm.send(m);
    };
    // Register the callback with on_msg.
    comm.on_msg(function (msg) {
        //console.log('receiving', msg['content']['data'], msg);
        var data = msg['content']['data'];
        if (data['blob'] !== undefined) {
            data = {
                data: new Blob(msg['buffers'], { type: data['blob'] }),
            };
        }
        // Pass the mpl event to the overridden (by mpl) onmessage function.
        ws.onmessage(data);
    });
    return ws;
};

mpl.mpl_figure_comm = function (comm, msg) {
    // This is the function which gets called when the mpl process
    // starts-up an IPython Comm through the "matplotlib" channel.

    var id = msg.content.data.id;
    // Get hold of the div created by the display call when the Comm
    // socket was opened in Python.
    var element = document.getElementById(id);
    var ws_proxy = comm_websocket_adapter(comm);

    function ondownload(figure, _format) {
        window.open(figure.canvas.toDataURL());
    }

    var fig = new mpl.figure(id, ws_proxy, ondownload, element);

    // Call onopen now - mpl needs it, as it is assuming we've passed it a real
    // web socket which is closed, not our websocket->open comm proxy.
    ws_proxy.onopen();

    fig.parent_element = element;
    fig.cell_info = mpl.find_output_cell("<div id='" + id + "'></div>");
    if (!fig.cell_info) {
        console.error('Failed to find cell for figure', id, fig);
        return;
    }
    fig.cell_info[0].output_area.element.on(
        'cleared',
        { fig: fig },
        fig._remove_fig_handler
    );
};

mpl.figure.prototype.handle_close = function (fig, msg) {
    var width = fig.canvas.width / fig.ratio;
    fig.cell_info[0].output_area.element.off(
        'cleared',
        fig._remove_fig_handler
    );
    fig.resizeObserverInstance.unobserve(fig.canvas_div);

    // Update the output cell to use the data from the current canvas.
    fig.push_to_output();
    var dataURL = fig.canvas.toDataURL();
    // Re-enable the keyboard manager in IPython - without this line, in FF,
    // the notebook keyboard shortcuts fail.
    IPython.keyboard_manager.enable();
    fig.parent_element.innerHTML =
        '<img src="' + dataURL + '" width="' + width + '">';
    fig.close_ws(fig, msg);
};

mpl.figure.prototype.close_ws = function (fig, msg) {
    fig.send_message('closing', msg);
    // fig.ws.close()
};

mpl.figure.prototype.push_to_output = function (_remove_interactive) {
    // Turn the data on the canvas into data in the output cell.
    var width = this.canvas.width / this.ratio;
    var dataURL = this.canvas.toDataURL();
    this.cell_info[1]['text/html'] =
        '<img src="' + dataURL + '" width="' + width + '">';
};

mpl.figure.prototype.updated_canvas_event = function () {
    // Tell IPython that the notebook contents must change.
    IPython.notebook.set_dirty(true);
    this.send_message('ack', {});
    var fig = this;
    // Wait a second, then push the new image to the DOM so
    // that it is saved nicely (might be nice to debounce this).
    setTimeout(function () {
        fig.push_to_output();
    }, 1000);
};

mpl.figure.prototype._init_toolbar = function () {
    var fig = this;

    var toolbar = document.createElement('div');
    toolbar.classList = 'btn-toolbar';
    this.root.appendChild(toolbar);

    function on_click_closure(name) {
        return function (_event) {
            return fig.toolbar_button_onclick(name);
        };
    }

    function on_mouseover_closure(tooltip) {
        return function (event) {
            if (!event.currentTarget.disabled) {
                return fig.toolbar_button_onmouseover(tooltip);
            }
        };
    }

    fig.buttons = {};
    var buttonGroup = document.createElement('div');
    buttonGroup.classList = 'btn-group';
    var button;
    for (var toolbar_ind in mpl.toolbar_items) {
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) {
            /* Instead of a spacer, we start a new button group. */
            if (buttonGroup.hasChildNodes()) {
                toolbar.appendChild(buttonGroup);
            }
            buttonGroup = document.createElement('div');
            buttonGroup.classList = 'btn-group';
            continue;
        }

        button = fig.buttons[name] = document.createElement('button');
        button.classList = 'btn btn-default';
        button.href = '#';
        button.title = name;
        button.innerHTML = '<i class="fa ' + image + ' fa-lg"></i>';
        button.addEventListener('click', on_click_closure(method_name));
        button.addEventListener('mouseover', on_mouseover_closure(tooltip));
        buttonGroup.appendChild(button);
    }

    if (buttonGroup.hasChildNodes()) {
        toolbar.appendChild(buttonGroup);
    }

    // Add the status bar.
    var status_bar = document.createElement('span');
    status_bar.classList = 'mpl-message pull-right';
    toolbar.appendChild(status_bar);
    this.message = status_bar;

    // Add the close button to the window.
    var buttongrp = document.createElement('div');
    buttongrp.classList = 'btn-group inline pull-right';
    button = document.createElement('button');
    button.classList = 'btn btn-mini btn-primary';
    button.href = '#';
    button.title = 'Stop Interaction';
    button.innerHTML = '<i class="fa fa-power-off icon-remove icon-large"></i>';
    button.addEventListener('click', function (_evt) {
        fig.handle_close(fig, {});
    });
    button.addEventListener(
        'mouseover',
        on_mouseover_closure('Stop Interaction')
    );
    buttongrp.appendChild(button);
    var titlebar = this.root.querySelector('.ui-dialog-titlebar');
    titlebar.insertBefore(buttongrp, titlebar.firstChild);
};

mpl.figure.prototype._remove_fig_handler = function (event) {
    var fig = event.data.fig;
    if (event.target !== this) {
        // Ignore bubbled events from children.
        return;
    }
    fig.close_ws(fig, {});
};

mpl.figure.prototype._root_extra_style = function (el) {
    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.
};

mpl.figure.prototype._canvas_extra_style = function (el) {
    // this is important to make the div 'focusable
    el.setAttribute('tabindex', 0);
    // reach out to IPython and tell the keyboard manager to turn it's self
    // off when our div gets focus

    // location in version 3
    if (IPython.notebook.keyboard_manager) {
        IPython.notebook.keyboard_manager.register_events(el);
    } else {
        // location in version 2
        IPython.keyboard_manager.register_events(el);
    }
};

mpl.figure.prototype._key_event_extra = function (event, _name) {
    var manager = IPython.notebook.keyboard_manager;
    if (!manager) {
        manager = IPython.keyboard_manager;
    }

    // Check for shift+enter
    if (event.shiftKey && event.which === 13) {
        this.canvas_div.blur();
        // select the cell after this one
        var index = IPython.notebook.find_cell_index(this.cell_info[0]);
        IPython.notebook.select(index + 1);
    }
};

mpl.figure.prototype.handle_save = function (fig, _msg) {
    fig.ondownload(fig, null);
};

mpl.find_output_cell = function (html_output) {
    // Return the cell and output element which can be found *uniquely* in the notebook.
    // Note - this is a bit hacky, but it is done because the "notebook_saving.Notebook"
    // IPython event is triggered only after the cells have been serialised, which for
    // our purposes (turning an active figure into a static one), is too late.
    var cells = IPython.notebook.get_cells();
    var ncells = cells.length;
    for (var i = 0; i < ncells; i++) {
        var cell = cells[i];
        if (cell.cell_type === 'code') {
            for (var j = 0; j < cell.output_area.outputs.length; j++) {
                var data = cell.output_area.outputs[j];
                if (data.data) {
                    // IPython >= 3 moved mimebundle to data attribute of output
                    data = data.data;
                }
                if (data['text/html'] === html_output) {
                    return [cell, data, j];
                }
            }
        }
    }
};

// Register the function which deals with the matplotlib target/channel.
// The kernel may be null if the page has been refreshed.
if (IPython.notebook.kernel !== null) {
    IPython.notebook.kernel.comm_manager.register_target(
        'matplotlib',
        mpl.mpl_figure_comm
    );
}
</script><div class="output text_html"><div id='5b9511f3-19df-44df-b121-eaf325b87da0'></div></div></div>
</div>
<div class="section" id="linear-kernel">
<h3><span class="section-number">7.2.3.1. </span>Linear Kernel<a class="headerlink" href="#linear-kernel" title="Permalink to this headline">¶</a></h3>
<p>The linear kernel reads</p>
<div class="math notranslate nohighlight">
\[k(x, x^{\prime}) = \beta_0 + \langle x, x^{\prime} \rangle \quad \text{for } x, x^{\prime} \in \mathbb{R}^d,\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_0 \ge 0\)</span> and <span class="math notranslate nohighlight">\(\langle x, x^{\prime} \rangle\)</span> denotes the scalar product of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x^{\prime}\)</span>. In the subsequent animation, we used <span class="math notranslate nohighlight">\(\beta_0 = 0\)</span>.</p>
<a class="reference internal image-reference" href="../../_images/linear.gif"><img alt="Linear kernel" src="../../_images/linear.gif" style="width: 800px;" /></a>
</div>
<div class="section" id="polynomial-kernel">
<h3><span class="section-number">7.2.3.2. </span>Polynomial Kernel<a class="headerlink" href="#polynomial-kernel" title="Permalink to this headline">¶</a></h3>
<p>The polynomial kernel is constructed by exponentiation of the linear kernel, i.e.,</p>
<div class="math notranslate nohighlight">
\[k(x, x^{\prime}) = \big(\beta_0 + \langle x, x^{\prime} \rangle\big)^p \quad \text{for } x, x^{\prime} \in \mathbb{R}^d,\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_0 \ge 0\)</span> and <span class="math notranslate nohighlight">\(p \in \mathbb{N}\)</span>. In the subsequent animation, we used <span class="math notranslate nohighlight">\(\beta_0 = 1\)</span> and <span class="math notranslate nohighlight">\(p=3\)</span>.</p>
<a class="reference internal image-reference" href="../../_images/polynomial.gif"><img alt="Polynomial kernel" src="../../_images/polynomial.gif" style="width: 800px;" /></a>
</div>
<div class="section" id="squared-exponential-kernel">
<h3><span class="section-number">7.2.3.3. </span>Squared Exponential Kernel<a class="headerlink" href="#squared-exponential-kernel" title="Permalink to this headline">¶</a></h3>
<p>The squared exponential kernel is possibly the most important kernel in kernel-based machine learning. It is also called radial basis function (RBF) kernel. It is defined by</p>
<div class="math notranslate nohighlight">
\[k(x, x^{\prime}) = \exp \big(-\frac{r^2}{2~l^2} \big)\]</div>
<p>where <span class="math notranslate nohighlight">\(r = |x - x^{\prime}|\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathbb{R}^d\)</span>. <span class="math notranslate nohighlight">\(l\)</span> is called <strong>length scale</strong> and is assumed to be positive. In particular, the squared exponential kernel is isotropic. In the subsequent animation, we used <span class="math notranslate nohighlight">\(l=1\)</span>.</p>
<a class="reference internal image-reference" href="../../_images/rbf.gif"><img alt="RBF kernel" src="../../_images/rbf.gif" style="width: 800px;" /></a>
</div>
<div class="section" id="exponential-kernel">
<h3><span class="section-number">7.2.3.4. </span>Exponential Kernel<a class="headerlink" href="#exponential-kernel" title="Permalink to this headline">¶</a></h3>
<p>The (absolute) exponential kernel is another isotropic kernel and is defined by</p>
<div class="math notranslate nohighlight">
\[k(x, x^{\prime}) = \exp \big(-\frac{r}{l} \big)\]</div>
<p>where <span class="math notranslate nohighlight">\(r = |x - x^{\prime}|\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(l\)</span> is the length scale. In the subsequent animation, we used <span class="math notranslate nohighlight">\(l=1\)</span>.</p>
<a class="reference internal image-reference" href="../../_images/exp.gif"><img alt="Exp kernel" src="../../_images/exp.gif" style="width: 800px;" /></a>
</div>
<div class="section" id="matern-kernel">
<h3><span class="section-number">7.2.3.5. </span>Matérn Kernel<a class="headerlink" href="#matern-kernel" title="Permalink to this headline">¶</a></h3>
<p>The Matérn kernel denotes a class of isotropic kernels which is parametrized by a parameter <span class="math notranslate nohighlight">\(\nu &gt; 0\)</span>. The kernel is given by</p>
<div class="math notranslate nohighlight">
\[k_{\nu}(x, x^{\prime}) = \frac{2^{1 - \nu}}{\Gamma(\nu)}~\Big(\frac{\sqrt{2\nu}~ r}{l}\Big)^{\nu} ~K_{\nu} \Big(\frac{\sqrt{2\nu}~r}{l}\Big),\]</div>
<p>where <span class="math notranslate nohighlight">\(r = |x - x^{\prime}|\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(l &gt;0\)</span> is the length scale, <span class="math notranslate nohighlight">\(\Gamma\)</span> is the gamma function and <span class="math notranslate nohighlight">\(K_{\nu}\)</span> is a modified Bessel function.</p>
<p>For <span class="math notranslate nohighlight">\(\nu = 0.5\)</span> the Matérn kernel becomes the exponential kernel and for <span class="math notranslate nohighlight">\(\nu \rightarrow \infty\)</span> the Matérn kernel approaches the squared exponential kernel. Thus, <span class="math notranslate nohighlight">\(\nu\)</span> determines the roughness of the samples paths and the samples paths get smoother as <span class="math notranslate nohighlight">\(\nu\)</span> increases.</p>
<p>The most interesting other cases for machine learning are <span class="math notranslate nohighlight">\(\nu = 1.5\)</span> and <span class="math notranslate nohighlight">\(\nu = 2.5\)</span>. It holds</p>
<div class="math notranslate nohighlight">
\[k_{\nu = 1.5}(x, x^{\prime}) = \Big( 1 + \frac{\sqrt{3}~r}{l} \Big)~\exp\Big(\frac{\sqrt{3}~r}{l}\Big)\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[k_{\nu = 2.5}(x, x^{\prime}) = \Big( 1 + \frac{\sqrt{5}~r}{l} + \frac{5~r^2}{3~l^2} \Big)~\exp\Big(\frac{\sqrt{5}~r}{l}\Big).\]</div>
<p>For both cases, sample paths are animated below with <span class="math notranslate nohighlight">\(l=1\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\nu = 1.5\)</span>:</p>
<a class="reference internal image-reference" href="../../_images/matern15.gif"><img alt="Matern 1.5 kernel" src="../../_images/matern15.gif" style="width: 800px;" /></a>
<p><span class="math notranslate nohighlight">\(\nu = 2.5\)</span>:</p>
<a class="reference internal image-reference" href="../../_images/matern25.gif"><img alt="Matern 2.5 kernel" src="../../_images/matern25.gif" style="width: 800px;" /></a>
</div>
<div class="section" id="rational-quadratic-kernel">
<h3><span class="section-number">7.2.3.6. </span>Rational Quadratic Kernel<a class="headerlink" href="#rational-quadratic-kernel" title="Permalink to this headline">¶</a></h3>
<p>The rational quadratic kernel denotes a family of isotropic kernels with parameter <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[k_{\alpha}(x, x^{\prime}) = \Big( 1 + \frac{r^2}{2\alpha~l^2} \Big)^{-\alpha}\]</div>
<p>with <span class="math notranslate nohighlight">\(r = |x - x^{\prime}|\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(l &gt; 0\)</span>. This kernel can be seen as mixture of squared exponential kernels with different length scales (see (4.20) in <span id="id3">[<a class="reference internal" href="#id13">1</a>]</span>). In the subsequent animation, we used <span class="math notranslate nohighlight">\(\alpha = l = 1\)</span>.</p>
<a class="reference internal image-reference" href="../../_images/rq.gif"><img alt="rq kernel" src="../../_images/rq.gif" style="width: 800px;" /></a>
</div>
<div class="section" id="periodic-kernel">
<h3><span class="section-number">7.2.3.7. </span>Periodic Kernel<a class="headerlink" href="#periodic-kernel" title="Permalink to this headline">¶</a></h3>
<p>The periodic kernel is also called Exp-Sine-Squared kernel. It is given by</p>
<div class="math notranslate nohighlight">
\[k(x, x^{\prime}) = \exp\Big( - \frac{2~\sin^2\big(\pi \frac{r}{p}\big)}{l^2}\Big)\]</div>
<p>with <span class="math notranslate nohighlight">\(r = |x - x^{\prime}|\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathbb{R}^d\)</span>. <span class="math notranslate nohighlight">\(l\)</span> is the length scale and <span class="math notranslate nohighlight">\(p\)</span> the <strong>periodicity</strong>. To illustrate the sample paths we used <span class="math notranslate nohighlight">\(l=p=1\)</span>.</p>
<a class="reference internal image-reference" href="../../_images/periodic.gif"><img alt="Periodic kernel" src="../../_images/periodic.gif" style="width: 800px;" /></a>
</div>
<div class="section" id="brownian-motion-kernel">
<h3><span class="section-number">7.2.3.8. </span>Brownian Motion Kernel<a class="headerlink" href="#brownian-motion-kernel" title="Permalink to this headline">¶</a></h3>
<p>Since we illustrated the sample paths of Brownian motion as an example for a stochastic process, we state its covariance function. Nevertheless, this kernel is not of interest for our machine learning applications. It holds</p>
<div class="math notranslate nohighlight">
\[k(s, t) = \text{min}(s, t)\]</div>
<p>for <span class="math notranslate nohighlight">\(s, t \in \mathbb{R}_{&gt; 0}\)</span>.</p>
</div>
</div>
<div class="section" id="combination-of-kernels">
<span id="sec-combofkernels"></span><h2><span class="section-number">7.2.4. </span>Combination of Kernels<a class="headerlink" href="#combination-of-kernels" title="Permalink to this headline">¶</a></h2>
<p>It is possible to obtain new covariance functions from known kernels by recombination.</p>
<p>Let <span class="math notranslate nohighlight">\((f_1(x))_{x \in \mathbb{R}^d}\)</span> and <span class="math notranslate nohighlight">\((f_2(x))_{x \in \mathbb{R}^d}\)</span> be two independent centered Gaussian processes with kernels <span class="math notranslate nohighlight">\(k_1\)</span> and <span class="math notranslate nohighlight">\(k_2\)</span>, respectively. Moreover, let <span class="math notranslate nohighlight">\(a : \mathbb{R}^d \rightarrow \mathbb{R}_{&gt; 0}\)</span>. Then, sums and products can be used to generate new kernels <span class="math notranslate nohighlight">\(k\)</span> and Gaussian processes <span class="math notranslate nohighlight">\(f\)</span> from old ones:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Gaussian process</p></th>
<th class="text-align:right head"><p>kernel <span class="math notranslate nohighlight">\(k(x, x^{\prime})\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><span class="math notranslate nohighlight">\(f_1 + f_2\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(k_1(x, x^{\prime})+k_2(x, x^{\prime})\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span class="math notranslate nohighlight">\(f_1 f_2\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(k_1(x, x^{\prime}) k_2(x, x^{\prime})\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><span class="math notranslate nohighlight">\(a f_1\)</span></p></td>
<td class="text-align:right"><p><span class="math notranslate nohighlight">\(a(x) k_1(x, x^{\prime}) a(x^{\prime})\)</span></p></td>
</tr>
</tbody>
</table>
<p>Of course, a constant <span class="math notranslate nohighlight">\(a\)</span> can be used for scaling and the three approaches can be combined arbitrarily.</p>
<p>For example, by multiplication of the periodic kernel with the squared exponential kernel the <strong>locally periodic kernel</strong> is constructed:</p>
<div class="math notranslate nohighlight">
\[k(x, x^{\prime}) = \exp\Big( - \frac{2~\sin^2\big(\pi \frac{r}{p}\big)}{l^2}\Big) \exp \Big(-\frac{r^2}{2~l^2} \Big),\]</div>
<p>where <span class="math notranslate nohighlight">\(r = |x - x^{\prime}|\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathbb{R}^d\)</span>.</p>
<p>The sample paths are indeed locally periodic, i.e., the periodic part changes over time:</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel1</span> <span class="o">=</span> <span class="n">ExpSineSquared</span><span class="p">()</span>
<span class="n">kernel2</span> <span class="o">=</span> <span class="n">RBF</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">kernel</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">kernel1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">kernel2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># anim = get_anim(kernel, xbnd=4., ybnd=5.)</span>
</pre></div>
</div>
</div>
</div>
<a class="reference internal image-reference" href="../../_images/locperiodic.gif"><img alt="locally periodic kernel" src="../../_images/locperiodic.gif" style="width: 800px;" /></a>
<p>Furthermore, scaling of a Gaussian process with kernel <span class="math notranslate nohighlight">\(k\)</span> by a constant <span class="math notranslate nohighlight">\(\sigma \ne 0\)</span> (i.e., choosing <span class="math notranslate nohighlight">\(a(x) = \sigma\)</span> in the notation above) yields the kernel <span class="math notranslate nohighlight">\(\sigma^2 k\)</span>. This is also a common approach to modify kernel functions.</p>
<p>For additional techniques for creating new covariance functions please refer to section 4.2.4 in <span id="id4">[<a class="reference internal" href="#id13">1</a>]</span>.</p>
</div>
<div class="section" id="impact-of-hyperparameters">
<h2><span class="section-number">7.2.5. </span>Impact of Hyperparameters<a class="headerlink" href="#impact-of-hyperparameters" title="Permalink to this headline">¶</a></h2>
<p>Most kernels depend on so-called hyperparameters. For example, the length scale <span class="math notranslate nohighlight">\(l\)</span> which appears in many of the previously stated examples as well as the scaling parameter <span class="math notranslate nohighlight">\(\sigma^2\)</span> mentioned in <a class="reference internal" href="#sec-combofkernels"><span class="std std-ref">Combination of Kernels</span></a> are hyperparameters. In addition to the genereal choice of the kernel, the exact values of these parameters determine the properties of the underlying Gaussian process. In general, we denote the collection of all hyperparameters of some kernel <span class="math notranslate nohighlight">\(k\)</span> by <span class="math notranslate nohighlight">\(\theta\)</span>. Please note that <span class="math notranslate nohighlight">\(\theta\)</span> is possibly vector valued, if <span class="math notranslate nohighlight">\(k\)</span> possesses more than one hyperparameter.</p>
<p>For example, the scaled squared exponential kernel with positive hyperparameters <span class="math notranslate nohighlight">\(l\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[k(x, x^{\prime}) = \sigma^2~\exp \big(-\frac{r^2}{2~l^2} \big)\]</div>
<p>where <span class="math notranslate nohighlight">\(r = |x - x^{\prime}|\)</span> for <span class="math notranslate nohighlight">\(x, x^{\prime} \in \mathbb{R}^d\)</span>. Thus, this kernel yields <span class="math notranslate nohighlight">\(\theta = (l, \sigma^2)\)</span>. A larger length scale implies a higher correlation between <span class="math notranslate nohighlight">\(f(x)\)</span> and <span class="math notranslate nohighlight">\(f(x^{\prime})\)</span>. Similarly, higher value of <span class="math notranslate nohighlight">\(\sigma^2\)</span> implies a smaller correlation.</p>
<p>Given a test point <span class="math notranslate nohighlight">\(x^*\)</span>, the <a class="reference internal" href="#lem-gpregr"><span class="std std-ref">mean prediction and variance</span></a> in Gaussian process regression depend on <span class="math notranslate nohighlight">\(K(X, X)\)</span> as well as <span class="math notranslate nohighlight">\(K(x^*, X)\)</span> which in turn depend on <span class="math notranslate nohighlight">\(\theta\)</span>. Consequently, the hyperparmeters impact the model predictions.</p>
<p>For the case of the scaled squared exponential kernel, the impact is shown in the following visualization:</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gaussian process posterior</span>
<span class="k">def</span> <span class="nf">cond_distr</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the posterior mean and covariance matrix for y2</span>
<span class="sd">    based on the corresponding input X2, the observations (y1, X1), </span>
<span class="sd">    and the prior kernel function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Kernel of the observations</span>
    <span class="n">Σ11</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X1</span><span class="p">)</span>
    <span class="c1"># Kernel of observations vs to-predict</span>
    <span class="n">Σ12</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span>
    <span class="c1"># Solve</span>
    <span class="n">solved</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Σ11</span><span class="p">,</span> <span class="n">Σ12</span><span class="p">,</span> <span class="n">assume_a</span><span class="o">=</span><span class="s1">&#39;pos&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="c1"># Compute posterior mean</span>
    <span class="n">μ2</span> <span class="o">=</span> <span class="n">solved</span> <span class="o">@</span> <span class="n">y1</span>
    <span class="c1"># Compute the posterior covariance</span>
    <span class="n">Σ22</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span>
    <span class="n">Σ2</span> <span class="o">=</span> <span class="n">Σ22</span> <span class="o">-</span> <span class="p">(</span><span class="n">solved</span> <span class="o">@</span> <span class="n">Σ12</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">μ2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Σ2</span>  <span class="c1"># mean, covariance</span>

<span class="k">def</span> <span class="nf">_plot</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>
    <span class="n">fig</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_time</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;true function&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mean prediction&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mean</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">)),</span> <span class="n">mean</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">)),</span> 
                     <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.25</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;credible interval&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span>
<span class="o">!</span>pip install ipympl
<span class="n">clear_output</span><span class="p">()</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interact_manual</span><span class="p">,</span> <span class="n">FloatSlider</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span><span class="p">,</span> <span class="n">FFMpegFileWriter</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">RBF</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ConstantKernel</span>

<span class="c1"># sample training data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">3.51</span><span class="p">,</span> <span class="mf">4.51</span><span class="p">,</span> <span class="mf">7.01</span><span class="p">,</span> <span class="mf">7.91</span><span class="p">,</span> <span class="mf">9.01</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">delta_t</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">/</span> <span class="n">nb_steps</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_time</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">)</span>

<span class="nd">@interact</span><span class="p">(</span><span class="n">l</span><span class="o">=</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 
          <span class="n">sigma2</span><span class="o">=</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">anim_hyper</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="n">constant_value</span><span class="o">=</span><span class="n">sigma2</span><span class="p">)</span> <span class="o">*</span> <span class="n">RBF</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="n">l</span><span class="p">)</span> 
    <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">cond_distr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">kernel</span><span class="p">)</span> 
    <span class="n">_plot</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_1912</span><span class="o">/</span><span class="mf">3071159995.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">200</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span> <span class="n">delta_t</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">/</span> <span class="n">nb_steps</span>
<span class="ne">---&gt; </span><span class="mi">21</span> <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_time</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span> 
<span class="g g-Whitespace">     </span><span class="mi">23</span> <span class="nd">@interact</span><span class="p">(</span><span class="n">l</span><span class="o">=</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> 

<span class="ne">NameError</span>: name &#39;total_time&#39; is not defined
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="selection-of-hyperparameters">
<h2><span class="section-number">7.2.6. </span>Selection of Hyperparameters<a class="headerlink" href="#selection-of-hyperparameters" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="extension-to-multiple-outputs">
<h2><span class="section-number">7.2.7. </span>Extension to Multiple Outputs<a class="headerlink" href="#extension-to-multiple-outputs" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="gaussian-process-classification">
<h2><span class="section-number">7.2.8. </span>Gaussian Process Classification<a class="headerlink" href="#gaussian-process-classification" title="Permalink to this headline">¶</a></h2>
<p>Gaussian process are not only used for regression, but also for classification problems. Gaussian process classification uses a latent regression model to predict class probabilities. In a first step, we discuss binary classification and afterwards the setting is generalized to multiclass classification.</p>
<div class="section" id="binary-classification">
<h3><span class="section-number">7.2.8.1. </span>Binary Classification<a class="headerlink" href="#binary-classification" title="Permalink to this headline">¶</a></h3>
<p>In binary classification the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is of the form</p>
<div class="math notranslate nohighlight">
\[\mathcal{D} = \{ (x_i, y_i)~|~x_i \in \mathbb{R}^d, y_i \in \{-1, 1\} \quad \text{for } i=1,\dots,n \},\]</div>
<p>i.e., the (discrete) label has either value <span class="math notranslate nohighlight">\(-1\)</span> or <span class="math notranslate nohighlight">\(1\)</span> representing the two possible classes. The sample matrix <span class="math notranslate nohighlight">\(X\)</span> and the lables <span class="math notranslate nohighlight">\(y\)</span> are constructed as before.</p>
<p>For a test point <span class="math notranslate nohighlight">\(x^*\)</span>, the aim is to predict the <strong>class probabilities</strong> of the output <span class="math notranslate nohighlight">\(y^*\)</span>, i.e., the probabilities <span class="math notranslate nohighlight">\(p(y^*=1~|~X, y, x^*)\)</span> and <span class="math notranslate nohighlight">\(p(y^*=-1~|~X, y, x^*)\)</span>. In this way, we obtain one discrete probability distribution on <span class="math notranslate nohighlight">\(\{-1, 1\}\)</span> for each <span class="math notranslate nohighlight">\(x^*\)</span>. Since</p>
<div class="math notranslate nohighlight">
\[p(y^*=-1~|~X, y, x^*) = 1 - p(y^*=1~|~X, y, x^*),\]</div>
<p>it is sufficient to focus on <span class="math notranslate nohighlight">\(p(y^*=1~|~X, y, x^*)\)</span>. In other words, we are looking for a model which returns for a given input <span class="math notranslate nohighlight">\(x^*\)</span> the probability that the corresponding label is <span class="math notranslate nohighlight">\(1\)</span>. For example, <span class="math notranslate nohighlight">\(x^*\)</span> could be an image which shows either a cat or a dog. The model has to quantify the probability for dog and the complementary probability yields the probability for cat. Certainly, we desire values close to 0 or 1, since values near 0.5 imply that the model has difficulties to classify the input.</p>
<p>It holds <span class="math notranslate nohighlight">\(\sigma(z) \in [0,1]\)</span>, where</p>
<div class="math notranslate nohighlight" id="equation-deflogfct">
<span class="eqno">(7.1)<a class="headerlink" href="#equation-deflogfct" title="Permalink to this equation">¶</a></span>\[\sigma(z) := \frac{1}{1 + \exp(-z)} \quad \text{for } z \in \mathbb{R}\]</div>
<p>is the <strong>logistic response function</strong>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">expit</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;logistic sigmoid&#39;</span><span class="p">)</span>
<span class="n">leg</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/02_GP_35_0.png" src="../../_images/02_GP_35_0.png" />
</div>
</div>
<p>Hence, the value of <span class="math notranslate nohighlight">\(\sigma(z)\)</span> can be interpreted as a probability. It is also possible to use other response functions such as the Gaussian cdf, but we will restrict to <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>Our aim is to construct the <strong>distribution of a latent variable</strong> <span class="math notranslate nohighlight">\(f^*\)</span> given the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> and a test point <span class="math notranslate nohighlight">\(x^*\)</span> (denote the pdf by <span class="math notranslate nohighlight">\(p(f^*~|~X, y, x^*)\)</span>) such that the class probability can be computed by the expectation of <span class="math notranslate nohighlight">\(p(y^*=1~|~f^*) := \sigma(f^*)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-classprob">
<span class="eqno">(7.2)<a class="headerlink" href="#equation-classprob" title="Permalink to this equation">¶</a></span>\[\begin{split}
p(y^*=1~|~X, y, x^*) &amp;= \int_{\mathbb{R}} ~ p(y^*=1~|~f^*)~p(f^*~|~X, y, x^*)~df^* \\
&amp;= \int_{\mathbb{R}} ~ \sigma(f^*)~p(f^*~|~X, y, x^*)~df^*.
\end{split}\]</div>
<p>Please note that <span class="math notranslate nohighlight">\(p(y^*=-1~|~f^*) = 1 - p(y^*=1~|~f^*) = 1 - \sigma(f^*) = \sigma(-f^*)\)</span> due to the properties of the logistic response function. Thus, the notation <span class="math notranslate nohighlight">\(p(y^*~|~f^*) = \sigma(y^* f^*)\)</span> is also used. Moreover, we like to mention that <span class="math notranslate nohighlight">\(f^* &lt; 0\)</span> suggests that <span class="math notranslate nohighlight">\(y^* = -1\)</span> is more likely and vice versa <span class="math notranslate nohighlight">\(f^* &gt; 0\)</span> implies that <span class="math notranslate nohighlight">\(y^* = 1\)</span> is more likely.</p>
<p><em>How can we construct <span class="math notranslate nohighlight">\(p(f^*~|~X, y, x^*)\)</span> in</em> <a class="reference internal" href="#equation-classprob">(7.2)</a> <em>by means of Gaussian process regression</em>?</p>
<p>The answer to this question is rather complicated and technical. For the interested reader, we give the details in the subsequent part:</p>
<div class="dropdown admonition">
<p class="admonition-title">Explanation.</p>
<p>We follow the explanation in sections 3.3 and 3.4 of <span id="id5">[<a class="reference internal" href="#id13">1</a>]</span>.</p>
<p>In Gaussian process regression we constructed a similar distribution <span class="math notranslate nohighlight">\(p(f^*~|~X, f, x^*)\)</span> representing the <a class="reference internal" href="#lem-gpregr"><span class="std std-ref">posterior over functions</span></a> in terms of conditional distributions. Hence, the approach</p>
<div class="math notranslate nohighlight" id="equation-latentdistr">
<span class="eqno">(7.3)<a class="headerlink" href="#equation-latentdistr" title="Permalink to this equation">¶</a></span>\[p(f^*|~X, y, x^*) = \int_{\mathbb{R}^n} ~ p(f^*~|~X, f, x^*)~p(f~|~X, y)~df\]</div>
<p>is useful and natural, since it reduces the problem to determine the <strong>distribution of the latent labels</strong> <span class="math notranslate nohighlight">\(f\)</span> given the observations <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. Note that we denote the continuous labels by <span class="math notranslate nohighlight">\(f\)</span> instead of <span class="math notranslate nohighlight">\(y\)</span>, since <span class="math notranslate nohighlight">\(y\)</span> is used for the discrete class labels.</p>
<p>At this point, it is already evident that <strong>Gaussian process classification is more challenging than regression</strong>. It is required to determine the pdf <span class="math notranslate nohighlight">\(p(f~|~X, y)\)</span> of an unobserved variable <span class="math notranslate nohighlight">\(f\)</span> and afterwards two integrals have to be calculated which involve the posterior distribution function of a Gaussian process regression model.</p>
<p><em>How can we compute <span class="math notranslate nohighlight">\(p(f~|~X, y)\)</span> in</em> <a class="reference internal" href="#equation-latentdistr">(7.3)</a>?</p>
<p>Bayes’ theorem yields</p>
<div class="math notranslate nohighlight" id="equation-bayeslatentlabels">
<span class="eqno">(7.4)<a class="headerlink" href="#equation-bayeslatentlabels" title="Permalink to this equation">¶</a></span>\[p(f ~|~X, y) = \frac{p(y~|~X, f) ~ p(f~|~X)}{p(y~|~X)} = \frac{p(y~|~f) ~ p(f~|~X)}{p(y~|~X)},\]</div>
<p>since <span class="math notranslate nohighlight">\(y\)</span> depends by assumption only on the latent variable <span class="math notranslate nohighlight">\(f\)</span>. In principle, we have all ingredients to create our classification model. Indeed, it holds</p>
<div class="math notranslate nohighlight" id="equation-factclassprob">
<span class="eqno">(7.5)<a class="headerlink" href="#equation-factclassprob" title="Permalink to this equation">¶</a></span>\[p(y~|~f) = \prod_{i=1}^n p(y_i~|~f_i) = \prod_{i=1}^n \sigma(y_i f_i) = \prod_{i=1}^n  \frac{1}{1 + \exp(-y_i f_i)}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[p(y~|~X) = \int_{\mathbb{R^n}} p(y~|~f, X) ~p(f~|~X)~ df = \int_{\mathbb{R^n}} p(y~|~f) ~p(f~|~X)~df,\]</div>
<p>where <span class="math notranslate nohighlight">\(p(f~|~X)\)</span> is the marginal likelihood of the Gaussian process. Please note that we assume that the labels <span class="math notranslate nohighlight">\(y\)</span> are independent conditional on <span class="math notranslate nohighlight">\(f\)</span> in order to factorize in <a class="reference internal" href="#equation-factclassprob">(7.5)</a>. However, the non-Gaussian likelihood <span class="math notranslate nohighlight">\(p(y~|~f)\)</span> makes it analytically intracable to compute the integral in <a class="reference internal" href="#equation-latentdistr">(7.3)</a>.</p>
<p>Markov chain Monte Carlo (MCMC) methods are useful to compute the integrals. Another approach is the <strong>Laplace approximation</strong> of <span class="math notranslate nohighlight">\(p(f~|~X, y)\)</span> by a Gaussian distribution function <span class="math notranslate nohighlight">\(q(f~|~X, y)\)</span>. The idea is to replace the unknown distribution by a suitable Gaussian distribution which is easier to handle. If we assume that <span class="math notranslate nohighlight">\(p(f~|~X, y)\)</span> would be indeed the pdf of a normal distribution, then mean and covariance would be given by</p>
<div class="math notranslate nohighlight" id="equation-laplacemean">
<span class="eqno">(7.6)<a class="headerlink" href="#equation-laplacemean" title="Permalink to this equation">¶</a></span>\[\hat{f} := \underset{f}{\text{argmax}} ~ p(f~|~X, y)\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-laplacecov">
<span class="eqno">(7.7)<a class="headerlink" href="#equation-laplacecov" title="Permalink to this equation">¶</a></span>\[\Sigma_{\hat{f}} := \Big(- \nabla^2 \ln \big(p(f~|~X, y)\big)|_{f=\hat{f}}\Big)^{-1},\]</div>
<p>where <span class="math notranslate nohighlight">\(\nabla\)</span> and <span class="math notranslate nohighlight">\(\nabla^2\)</span> denote the first and second derivative, respectively. Thus, the Laplace approximation requires to find the maximum of a function depending on <span class="math notranslate nohighlight">\(f\)</span>. For this purpose, we compute the roots of the gradient. The quality of the Laplace approximation depends of course on the shape of the <em>real</em> distribution function. For simplification of the calculation, we make to following changes:</p>
<ul class="simple">
<li><p>instead of <span class="math notranslate nohighlight">\(p(f~|~X, y)\)</span> consider <span class="math notranslate nohighlight">\(p(y~|~f) ~ p(f~|~X)\)</span> in view of <a class="reference internal" href="#equation-bayeslatentlabels">(7.4)</a>,</p></li>
<li><p>apply <span class="math notranslate nohighlight">\(\ln\)</span> to <span class="math notranslate nohighlight">\(p(y~|~f) ~ p(f~|~X)\)</span></p></li>
</ul>
<p>Thus, first <span class="math notranslate nohighlight">\(p(f~|~X, y)\)</span> is scaled by a constant and afterwards the strictly increasing function <span class="math notranslate nohighlight">\(\ln\)</span> is applied. A logarithmic transformation is useful, since it turns products into sums (which are easier to differentiate) and simplifies exponential terms. These modifications have no influence on the <span class="math notranslate nohighlight">\(\text{argmax}\)</span>.</p>
<p>Set <span class="math notranslate nohighlight">\(\Psi(f) := \ln \big(p(y~|~f) ~ p(f~|~X)\big)\)</span>. It holds</p>
<div class="math notranslate nohighlight">
\[\begin{split} \Psi(f) &amp;= \ln \big(p(y~|~f)\big) + \ln \big(p(f~|~X)\big) \\
           &amp;= \ln \big(p(y~|~f)\big) - \frac{1}{2} f^T K(X, X)^{-1} f - \frac{1}{2} \ln \big(|K(X, X)|\big) - \frac{n}{2} \ln  \big(2 \pi\big)\end{split}\]</div>
<p>Thus, the gradient is given by</p>
<div class="math notranslate nohighlight">
\[ \nabla \Psi(f) = \nabla \ln \big(p(y~|~f)\big) - K(X, X)^{-1} f \]</div>
<p>and the Hessian by</p>
<div class="math notranslate nohighlight">
\[\begin{split} \nabla^2 \Psi(f) &amp;= \nabla^2 \ln \big(p(y~|~f)\big) - K(X, X)^{-1} \\
                    &amp;= -  W(f) - K(X, X)^{-1},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(W(f) := - \nabla^2 \ln \big(p(y~|~f)\big)\)</span> is a diagonal matrix, since</p>
<div class="math notranslate nohighlight">
\[\ln \big(p(y~|~f)\big) = \ln \big(\prod_{i=1}^n \sigma(y_i f_i)\big) = \sum_{i=1}^n \ln \big(\sigma(y_i f_i)\big)\]</div>
<p><span class="math notranslate nohighlight">\(\hat{f}\)</span> fulfills <span class="math notranslate nohighlight">\(\nabla \Psi(\hat{f}) = 0\)</span> and hence, it holds</p>
<div class="math notranslate nohighlight" id="equation-eqfhat">
<span class="eqno">(7.8)<a class="headerlink" href="#equation-eqfhat" title="Permalink to this equation">¶</a></span>\[ \hat{f} = K(X, X) \nabla \ln \big(p(y~|~\hat{f})\big)\]</div>
<p>Unfortunately, <span class="math notranslate nohighlight">\(\hat{f}\)</span> also appears in the right-hand side of <a class="reference internal" href="#equation-eqfhat">(7.8)</a> and the equation can not be solved analytically. Therefore, another approximation is necessary. Roots of <span class="math notranslate nohighlight">\(\nabla \Psi\)</span> can be computed by <strong>Newton’s method</strong> in use of <span class="math notranslate nohighlight">\(\nabla^2 \Psi\)</span>:</p>
<ul>
<li><p>start with some initial value <span class="math notranslate nohighlight">\(f_0\)</span></p></li>
<li><p>for <span class="math notranslate nohighlight">\(i=0, 1, \dots\)</span> compute iteratively</p>
<div class="math notranslate nohighlight">
\[ f_{i+1} = f_i - \big(\nabla^2 \Psi(f_i)\big)^{-1} \nabla \Psi(f_i)\]</div>
<p>until some stopping criterion is met</p>
</li>
<li><p>denote the result as <span class="math notranslate nohighlight">\(\hat{f}\)</span></p></li>
</ul>
<p>Thus, <span class="math notranslate nohighlight">\(p(f~|~X, y)\)</span> is replaced by the density <span class="math notranslate nohighlight">\(q(f~|~X, y)\)</span> of a <span class="math notranslate nohighlight">\(\mathcal{N}(\hat{f}, \Sigma_{\hat{f}})\)</span>-distributed random variable with</p>
<div class="math notranslate nohighlight">
\[\Sigma_{\hat{f}} = \big(W(\hat{f}) + K(X, X)^{-1}\big)^{-1}\]</div>
<p>Consequently, the approximation of <a class="reference internal" href="#equation-latentdistr">(7.3)</a> yields</p>
<div class="math notranslate nohighlight">
\[p(f^*~|~X, y, x^*) \approx q(f^*~|~X, y, x^*) := \int_{\mathbb{R}^n} ~ p(f^*~|~X, f, x^*)~q(f~|~X, y)~df\]</div>
<p>The integrand is the product of two probability distribution functions of normal distributions. Hence, <span class="math notranslate nohighlight">\(q(f^*~|~X, y, x^*)\)</span> is the density of a univariate normal distribution with mean</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(f^*~|~X, y, x^*) = k(x^*, X) K(X, X)^{-1} \hat{f}\]</div>
<p>and variance</p>
<div class="math notranslate nohighlight">
\[\text{var}(f^*~|~X, y, x^*) = k(x^*, x^*) - K(x^*, X)\big(W(\hat{f})^{-1} + K(X,X)\big)^{-1} K(X, x^*)\]</div>
<div class="dropdown admonition">
<p class="admonition-title">Don’t do it!</p>
<p>For simplification of the notation, we write <span class="math notranslate nohighlight">\(K := K(X, X)\)</span> and <span class="math notranslate nohighlight">\(W := W(\hat{f})\)</span>. It holds</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}(f^*~|~X, y, x^*) &amp;= \int_{\mathbb{R}} f^*~p(f^*~|~X, y, x^*)~df^* \\
&amp;\approx \int_{\mathbb{R}} f^*~\int_{\mathbb{R}^n} ~ p(f^*~|~X, f, x^*)~q(f~|~X, y)~df~df^* \\
&amp;= \int_{\mathbb{R}^n} \int_{\mathbb{R}} f^*~ p(f^*~|~X, f, x^*)~df^* ~q(f~|~X, y) ~ df \\
&amp;= \int_{\mathbb{R}^n} K(x^*, X) K^{-1} f ~q(f~|~X, y) ~ df \\
&amp;= K(x^*, X) K^{-1} \int_{\mathbb{R}^n} f ~q(f~|~X, y) ~ df \\
&amp;= k(x^*, X) K^{-1} \hat{f}\end{split}\]</div>
<p>and variance</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{var}(f^*~|~X, y, x^*) &amp;= \int_{\mathbb{R}} \big(f^* - \mathbb{E}(f^*~|~X, y, x^*)\big)^2~p(f^*~|~X, y, x^*)~df^* \\
&amp;\approx \int_{\mathbb{R}} \big(f^* - \mathbb{E}(f^*~|~X, y, x^*)\big)^2~\int_{\mathbb{R}^n} ~ p(f^*~|~X, f, x^*)~q(f~|~X, y)~df~df^* \\ 
&amp;= \int_{\mathbb{R}^n} \int_{\mathbb{R}} \big(f^* - \mathbb{E}(f^*~|~X, y, x^*) \big)^2 ~ p(f^*|~X, f, x^*) ~ df^* ~ q(f~|~X, y)~df \\
&amp;= \int_{\mathbb{R}^n} \int_{\mathbb{R}} \big(f^* - \mathbb{E}(f^*|~X, f, x^*) + \mathbb{E}(f^*~|~X, f, x^*) - \mathbb{E}(f^*~|~X, y, x^*)\big)^2 ~ p(f^*|~X, f, x^*) ~ df^* ~ q(f~|~X, y)~df \\
&amp;= \int_{\mathbb{R}^n} \int_{\mathbb{R}} \big(f^* - \mathbb{E}(f^*|~X, f, x^*)\big)^2 + \big(\mathbb{E}(f^*~|~X, f, x^*) - \mathbb{E}(f^*~|~X, y, x^*)\big)^2 ~ p(f^*|~X, f, x^*) ~ df^* ~ q(f~|~X, y)~df,\end{split}\]</div>
<p>since</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;\int_{\mathbb{R}^n} \int_{\mathbb{R}} \big(f^* - \mathbb{E}(f^*~|~X, f, x^*)\big)\big(\mathbb{E}(f^*~|~X, f, x^*) - \mathbb{E}(f^*~|~X, y, x^*)\big) ~ p(f^*|~X, f, x^*) ~ df^* ~ q(f~|~X, y)~df \\
&amp;= \int_{\mathbb{R}^n} \underbrace{\int_{\mathbb{R}} \big(f^* - \mathbb{E}(f^*~|~X, f, x^*)\big) ~ p(f^*~|~X, f, x^*) ~ df^*}_{=0} ~ \big(\mathbb{E}(f^*~|~X, f, x^*) - \mathbb{E}(f^*~|~X, y, x^*)\big) ~ q(f|~X, y)~df \\
&amp;= 0\end{split}\]</div>
<p>The first summand in the integral yields the variance of the posterior Gaussian process distribution at <span class="math notranslate nohighlight">\(x^*\)</span>. Recall that <span class="math notranslate nohighlight">\(\mathbb{E}(f^*~|~X, f, x^*) = K(x^*, X)K^{-1}f\)</span>. Therefore, it follows</p>
<div class="math notranslate nohighlight">
\[\begin{split} \text{var}(f^*~|~X, y, x^*) &amp;= k(x^*, x^*) - K(x^*, X)K^{-1}K(X, x^*) + \int_{\mathbb{R}^n} \int_{\mathbb{R}} \big(K(x^*, X)K^{-1}(f - \hat{f}\big)^2 ~ p(f^*|~X, f, x^*) ~ df^* ~ q(f~|~X, y)~df \\
&amp;= k(x^*, x^*) - K(x^*, X)K^{-1}K(X, x^*) + \int_{\mathbb{R}^n} \big(K(x^*, X)K^{-1}(f - \hat{f}\big)^2 ~ q(f~|~X, y)~df \\
&amp;= k(x^*, x^*) - K(x^*, X)K^{-1}K(X, x^*) + K(x^*, X) K^{-1} \big(W + K^{-1}\big)^{-1} K^{-1} K(X, x^*) \\
&amp;= k(x^*, x^*) - K(x^*, X) \big(K^{-1} - K^{-1} \big(W + K^{-1}\big)^{-1} K^{-1}\big) K(X, x^*) \\
&amp;= k(x^*, x^*) - K(x^*, X)\big(W^{-1} + K\big)^{-1} K(X, x^*)\end{split}\]</div>
<p>The last equation holds, since <span class="math notranslate nohighlight">\(K^{-1} + K^{-1} \big(W + K^{-1}\big)^{-1} K^{-1} = \big(W^{-1} + K \big)^{-1}\)</span> by</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;\big(K^{-1} - K^{-1} \big(W + K^{-1}\big)\big(W^{-1} + K \big) \\
&amp;= K^{-1} \big(I_n - (W + K^{-1})^{-1} K^{-1}\big)\big(K + W^{-1}\big) \\
&amp;= K^{-1} \big(W + K^{-1})^{-1} \big) \big(W + K^{-1} - K^{-1}\big)\big(K + W^{-1}\big) \\
&amp;= K^{-1} \big(W + K^{-1})^{-1} \big) W \big(K + W^{-1}\big) \\
&amp;= (W K + I_n)^{-1}  (W K + I_n) \\
&amp;= I_n
\end{split}\]</div>
</div>
<p>This result looks very familiar. Indeed, the mean and variance equal exactly the mean prediction and variance in Gaussian process regression with data given by <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(f\)</span> as well as additional noise specified by <span class="math notranslate nohighlight">\(W(\hat{f})^{-1}\)</span>.</p>
<p>In order to compute the marginal likelihood <span class="math notranslate nohighlight">\(p(y~|~X)\)</span> the second order approximation</p>
<div class="math notranslate nohighlight">
\[\Psi(f) \approx \Psi(\hat{f}) - \frac{1}{2} (f - \hat{f})^T {\Sigma_{\hat{f}}}^{-1} (f - \hat{f})\]</div>
<p>is used. It holds</p>
<div class="math notranslate nohighlight">
\[\begin{split} p(y~|~X) = \int_{\mathbb{R}^n} p(y~|~f) ~p(f~|~X) df &amp;= \int_{\mathbb{R}^n} \exp(\Psi(f))~df \\
&amp;\approx \exp(\Psi(\hat{f})) \int_{\mathbb{R}^n} \exp(-\frac{1}{2} (f - \hat{f})^T {\Sigma_{\hat{f}}}^{-1} (f - \hat{f}))~df \\
&amp;= \exp(\Psi(\hat{f})) \sqrt{(2\pi)^n |\Sigma_{\hat{f}}|}\end{split}\]</div>
<p>Thefore, the log marginal likelihood reads</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ln \big(p(y~|~X)\big) &amp;\approx \Psi(\hat{f}) + \frac{n}{2} \ln \big(2\pi\big) + \frac{1}{2} \ln\big(|\Sigma_{\hat{f}}|\big) \\
&amp;= \ln \big(p(y~|~\hat{f})\big) + \ln \big(p(\hat{f}~|~X)\big) + \frac{n}{2} \ln \big(2\pi\big) + \frac{1}{2} \ln\big(|\Sigma_{\hat{f}}|\big) \\
&amp;= \ln \big(p(y~|~\hat{f})\big) - \frac{1}{2} \hat{f}^T K(X,X)^{-1} \hat{f} - \frac{1}{2} \ln \big(|K(X,X)|\big) + \frac{1}{2} \ln\big(|\Sigma_{\hat{f}}|\big) \\
&amp;= \ln \big(p(y~|~\hat{f})\big) - \frac{1}{2} \hat{f}^T K(X,X)^{-1} \hat{f} - \frac{1}{2} \ln \big(|B(X, \hat{f})|\big),\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(B(X, \hat{f}) := K(X, X) {\Sigma_{\hat{f}}}^{-1}\)</span>. It holds</p>
<div class="math notranslate nohighlight">
\[B(X, \hat{f}) = K(X, X) \big(W(\hat{f}) + K(X, X)^{-1}\big) = I_n + K(X, X) W(\hat{f})\]</div>
<p>and thus, it follows</p>
<div class="math notranslate nohighlight">
\[\begin{split}|B(X, \hat{f})| = |I_n + K(X, X) W(\hat{f})| &amp;= |W(\hat{f})^{-\frac{1}{2}} + K(X, X) W(\hat{f})^{\frac{1}{2}}||W(\hat{f})^{\frac{1}{2}}| \\
&amp;= |W(\hat{f})^{\frac{1}{2}}||W(\hat{f})^{-\frac{1}{2}} + K(X, X) W(\hat{f})^{\frac{1}{2}}| \\
&amp;= |I_n + W(\hat{f})^{\frac{1}{2}} K(X,X) W(\hat{f})^{\frac{1}{2}}|\end{split}\]</div>
<p>The last representation is particularly useful for implementation purposes, since <span class="math notranslate nohighlight">\(I_n + W(\hat{f})^{\frac{1}{2}} K(X,X) W(\hat{f})^{\frac{1}{2}}\)</span> is a positive definite matrix.</p>
</div>
<p>In application, the computational process can be summarized as follows:</p>
<ol>
<li><p>Choose a Gaussian process with hyperparemeters <span class="math notranslate nohighlight">\(\theta\)</span> as prior for the latent labels <span class="math notranslate nohighlight">\(f\)</span></p></li>
<li><p>For fixed <span class="math notranslate nohighlight">\(\theta\)</span> set <span class="math notranslate nohighlight">\(\Psi(f) := \ln \big(p(y~|~f) ~ p(f~|~X)\big)\)</span> (note that <span class="math notranslate nohighlight">\(p(f~|~X)\)</span> depends on <span class="math notranslate nohighlight">\(\theta\)</span>)</p></li>
<li><p>Compute the root <span class="math notranslate nohighlight">\(\hat{f}\)</span> (depends on <span class="math notranslate nohighlight">\(\theta\)</span>) of <span class="math notranslate nohighlight">\(\nabla \Psi\)</span> in use of Newton’s method</p></li>
<li><p>Calculate the log marginal likelihood by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ln p(y~|~X) &amp;\approx \ln \big(p(y~|~\hat{f})\big) - \frac{1}{2} \hat{f}^T K(X,X)^{-1} \hat{f} - \frac{1}{2} \ln \big(|K(X,X)|\big) + \frac{1}{2} \ln\big(|\Sigma_{\hat{f}}|\big)\\
   &amp;=\ln \big(p(y~|~\hat{f})\big) - \frac{1}{2} \hat{f}^T K(X,X)^{-1} \hat{f} - \frac{1}{2} \ln \big(|I_n + W(\hat{f})^{\frac{1}{2}} K(X,X) W(\hat{f})^{\frac{1}{2}}|\big),\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(W(\hat{f}) = - \nabla^2 \ln \big(p(y~|~\hat{f})\big)\)</span>, <span class="math notranslate nohighlight">\(\Sigma_{\hat{f}} = \big(W(\hat{f}) + K(X, X)^{-1}\big)^{-1}\)</span> and</p>
<div class="math notranslate nohighlight">
\[\ln \big(p(y~|~\hat{f})\big) = - \sum_{i=1}^n \ln \big(1 + \exp(-y_i\hat{f}_i)\big)\]</div>
</li>
<li><p>Maximize <span class="math notranslate nohighlight">\(\ln \big(p(y~|~X)\big)\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span> (which means looping steps 2.-4.)</p></li>
</ol>
<p>Then, the class probability for a test point <span class="math notranslate nohighlight">\(x^*\)</span> in <a class="reference internal" href="#equation-classprob">(7.2)</a> is given by the one dimensional integral</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(y^*=1~|~X, y, x^*) &amp;= \int_{\mathbb{R}} ~ \sigma(f^*)~p(f^*~|~X, y, x^*)~df^* \\
&amp;\approx \int_{\mathbb{R}} ~ \sigma(f^*)~q(f^*~|~X, y, x^*)~df^*,\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(q(f^*~|~X, y, x^*)~df^*\)</span> is the density of a normal distribution with mean</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(f^*~|~X, y, x^*) = k(x^*, X) K(X, X)^{-1} \hat{f}\]</div>
<p>and variance</p>
<div class="math notranslate nohighlight">
\[\text{var}(f^*~|~X, y, x^*) = k(x^*, x^*) - K(x^*, X)\big(W(\hat{f})^{-1} + K(X,X)\big)^{-1} K(X, x^*)\]</div>
<p>Thus, the assigned class probability is the expectation of <span class="math notranslate nohighlight">\(\sigma\)</span> with respect to the posterior distribution of the latent variable <span class="math notranslate nohighlight">\(f^*\)</span>. If we are not interested in the precise value of the probability, but only in the favored class, it is sufficient to consider the sign of <span class="math notranslate nohighlight">\(\mathbb{E}(f^*~|~X, y, x^*)\)</span>. For negative values the predicted class is <span class="math notranslate nohighlight">\(-1\)</span> and for positive values the predicted class is <span class="math notranslate nohighlight">\(1\)</span>. This is equivalent to computing <span class="math notranslate nohighlight">\(\sigma(\mathbb{E}(f^*~|~X, y, x^*))\)</span> and to conclude that the predicted class is <span class="math notranslate nohighlight">\(-1\)</span> for a value less than <span class="math notranslate nohighlight">\(0.5\)</span> and the predicted class is <span class="math notranslate nohighlight">\(1\)</span> for a value larger than <span class="math notranslate nohighlight">\(0.5\)</span>.</p>
</div>
<div class="section" id="multiclass-classification">
<h3><span class="section-number">7.2.8.2. </span>Multiclass Classification<a class="headerlink" href="#multiclass-classification" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="remarks">
<h2><span class="section-number">7.2.9. </span>Remarks<a class="headerlink" href="#remarks" title="Permalink to this headline">¶</a></h2>
<p id="id6"><dl class="citation">
<dt class="label" id="id13"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>,<a href="#id4">4</a>,<a href="#id5">5</a>)</span></dt>
<dd><p>C.E. Rasmussen and C.K.I. Williams. <em>Gaussian Processes for Machine Learning</em>. Adaptive Computation and Machine Learning. MIT Press, 2nd edition, 2006. URL: <a class="reference external" href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./02_probML/02_kernelmethods"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="01_kerneltrick.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">7.1. </span>The Kernel Trick: Implicit embeddings from inner products</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="03_addmethods.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">7.3. </span>Additional Kernel-based Methods</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By C. Bogoclu, N. Friedlich & R. Vosshall<br/>
        
          <div class="extra_footer">
            Content on this site is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">a CC BY-NC-NB 4.0 licence</a>.
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>