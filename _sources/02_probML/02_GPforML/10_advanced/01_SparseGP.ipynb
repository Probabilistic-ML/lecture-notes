{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844e423a",
   "metadata": {},
   "source": [
    "## Scalable Gaussian Processes\n",
    "\n",
    "**Recap Gaussian Process Regression**\n",
    "\n",
    "In {ref}```GPR-Lemma<lem:gpregrnois>``` we have fully discribed a Gaussian process for regression. For repitition we will write this down again\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y_* \\lvert X_*X,y) &= \\mathcal{N}(y_* \\lvert \\mu_*, \\Sigma_*)\\\\\n",
    "\\mu_* = \\mathbb{E}(f(X^*)) &= K(X^*, X) \\big(K(X, X) + \\sigma_{\\mathrm{noise}}^2 I_n\\big)^{-1} y\\\\\n",
    "\\Sigma_* =\\text{Cov}(f(X^*)) &= K(X^*, X^*) - K(X^*, X) \\big(K(X, X) + \\sigma_{\\mathrm{noise}}^2 I_n\\big)^{-1} K(X, X^*)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We will shorten this notation of $K(X^*, X^*)$ to $K_{**}$ , $K(X^*, X)$ to $K_{*}^T$ and $K(X, X^*)$ to $K_{*}$ and this we lead to\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y_* \\lvert X_*X,y) &= \\mathcal{N}(y_* \\lvert \\mu_*, \\Sigma_*)\\\\\n",
    "\\mu_* &= K_{*}^T \\big(K + \\sigma_{\\mathrm{noise}}^2 I_n\\big)^{-1} y\\\\\n",
    "\\Sigma_* &= K_{**} - K_{*}^T \\big(K + \\sigma_{\\mathrm{noise}}^2 I_n\\big)^{-1} K_{*}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can predict function values $f^*$ at new inputs $X^*$ with this. The most costly computation step of this is to compute the invserse of a covariance matrix (with noise)\n",
    "\n",
    "$$\n",
    "\\big(K + \\sigma_{\\mathrm{noise}}^2 I_n\\big)^{-1}\n",
    "$$\n",
    "\n",
    "which is needed in the mean and the covariance of the gaussian process.\n",
    "This matrix has the size of $n \\times n$, where $n$ is the number of training samples. The problem now is that, for large $n$ a matrix inversion is intractable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8e5d8",
   "metadata": {},
   "source": [
    "The posterior can also be defined as\n",
    "\n",
    "$$\n",
    "p(f_* | X^*,X, y) = \\int p(f_*,f| X^*,X,y)df \\quad  \\textrm{marginalize }f\\\\\n",
    "p(f_* | X^*,X, y) = \\int p(f_*| X^*,X,f,y)p(f|X^*,X, y)df \\quad \\textrm{reserve chain rule}\\\\\n",
    "p(f_* | X^*,X, y) = \\int p(f_* | X^*,X,f) p(f| X^*,X, y) df \\quad \\textrm{drop irrelevant y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b61e1",
   "metadata": {},
   "source": [
    "**Sparse Gaussian Process Regression**\n",
    "\n",
    "To avoid costly matrix inversion of $n \\times n$-matrix, Authors in {cite}```Titsias2009``` propose to use the so called _inducing variables_. They assume the existence a set of $m$ inducing variables $f_m$ with corresponding inputs $X_m$ that will discribe our model well enough as if we used all datapoints $n$. Note that inducing variables $f_m$ evaluated at the pseudo-inputs $X_m$, which are independent from the training inputs. We will define the approximation posterior as\n",
    "$$\n",
    "q(f_*) = \\int p(f_* | f_m) \\phi(f_m) df_m ,\n",
    "$$\n",
    "\n",
    "where $\\phi(f_m)$ is the approximation of the intractable $p(f_m|y)$ and is defined by\n",
    "\n",
    "$$\n",
    "\\phi(f_m) = \\mathcal{N}(f_m | \\mu_m, A_m).\n",
    "$$\n",
    "Here is $\\mu_m$ the mean and $A_m$ the covariance matrix. The goal is to find optimal values for the mean $\\mu_m$ and covarince $A_m$ as well as the optimal location of the inducing inputs $X_m$. The mean and covariance matrix of the Guassian Process is defined by\n",
    "\n",
    "$$\n",
    "q(f_*) = \\mathcal{N}(f_* | \\mu_*^q, \\Sigma_*^q) \\\\\n",
    "\\mu_*^q = K_{*m} K_{mm}^{-1} \\mu_m  \\\\\n",
    "\\Sigma_*^q = K_{**} - K_{*m} K_{mm}^{-1} K_{m*} + K_{*m} K_{mm}^{-1} \\mathbf{A}_m K_{mm}^{-1} K_{m*} \n",
    "$$\n",
    "\n",
    "where $K_{mm}=K(X_m,X_m)$, $K_{*m}=K(X_*,X_m)$ and $K_{m*}=K(X_m,X_*)=K_{*m}^T$. Now we can see, that we have to do the inversion of a $m \\times m$ matrix if we found the optimal values for $\\mu_m, A_m$ and $X_m$. For optimization of this we will use a variational approach by minimizing the Kullback-Leibler (KL) divergence between the approximate $q(f)$ and the exact posterior $p(f|y)$ over latent variables $f$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a7db0",
   "metadata": {},
   "source": [
    "The minimization of KL divergence is equivalent to maximization of a lower bound $\\mathcal{L}(\\mu_m, A_m,X_m)$ on the true log marginal likelihood $\\log p(y).$ This lower bound can be optimized by analytically solving for $Î¼_m$ and $A_m$. The resulting lower bound after optimization is a function of $X_m$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(X_m)= \\log \\mathcal{N}(y|0,\\sigma^2_y I+ Q_{nn})-\\frac{1}{2 \\sigma^2_y}\\textrm{Tr}(K{nn}-Q{nn})\n",
    "$$\n",
    "with $Q{nn}=K{nm}K{mm}^{-1}K{mn}$. The first term on the RHS is the approximate log likelihood term and the second one is a regularization term which result of using variational approach.The second term can be interpreted as minimizing the error predicting $f$ from inducing variables $f_m$. The better the variables $f_m$ represent the function to be modeled the smaller this error will be. So the optimization will try to find the best postions for the inducing inputs $X_m$. With optimal inducing inputs $X_m$ we ca analytically find values for $\\mu_m$ and $A_m$.\n",
    "\n",
    "$$\n",
    "\\mu_m = \\frac{1}{\\sigma_y^2} K_{mm} \\Sigma K_{mn} y \\\\\n",
    "A_m = K_{mm} \\Sigma K_{mm} \n",
    "$$\n",
    "where $\\Sigma = (K_{mm}+\\sigma^2_yK_{mm}K_{nm}^{-1})$ is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625defda",
   "metadata": {},
   "source": [
    "For a more detailed description of sparse and variational GP, please refer to {cite}```Leibfried2021```.\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    ":style: plain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abbda4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
