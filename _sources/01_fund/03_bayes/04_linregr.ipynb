{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0233a9e9",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In this section, we illustrate the preceding concepts for the case of a linear regression model. The probabilistic concepts are applied to the linear setting and the connection to classical (deterministic) methods are shown. \n",
    "\n",
    "We assume the following setting:\n",
    "- Some data of the form\n",
    "  \n",
    "  $$\\mathcal{D} = \\{ (x_i, y_i)~|~x_i, y_i \\in \\mathbb{R} \\quad \\text{for } i = 1,\\dots, n\\}$$\n",
    "  \n",
    "  is given\n",
    "- The data $\\mathcal{D}$ is collected from observation with the relation\n",
    "  \n",
    "  $$y_i = \\beta~x_i + \\varepsilon_i,$$\n",
    "  \n",
    "  where $\\beta \\in \\mathbb{R}$ and $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, $i=1, \\dots, n$ are independent normally distributed random variables. For simplicity, we suppose that $\\sigma^2$ is known.\n",
    "  \n",
    "$\\varepsilon_i$ represents some error term (e.g. measurement errors) which is also called **noise**. The **unknown** coefficient $\\beta$ is called slope. Please note that the outputs $y_i$, $i=1,\\dots,n$, are also independent normally distributed random variables with distribution $\\mathcal{N}(\\beta~x_i, \\sigma^2)$. $x$ and $y$ denote vectors containing all $x_i$ and $y_i$, $i=1,\\dots,n$, respectively.\n",
    "\n",
    "Our goal is to use the information in $\\mathcal{D}$ to estimate $\\beta$ in an appropiate way. In use of these estimates, it is possible to make predictions for new inputs $x^*$. Similarly, the probabilistic concepts are applied later on to more advanced models.\n",
    "\n",
    "For the purpose of visualization, we generate a linear function as well as some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform, norm, laplace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate true value of beta randomly\n",
    "# in use of a standard normal distribution\n",
    "beta = norm.rvs(loc=0, scale=1)\n",
    "interval_length = 1 # lenght of the interval around 0 for x values\n",
    "boundary = np.array([-interval_length/2, interval_length/2]) # auxiliary array for plots\n",
    "\n",
    "n = 10 # number of observations\n",
    "sigma2 = 0.05 # noise level\n",
    "\n",
    "# generate data\n",
    "x = interval_length * uniform.rvs(size=n) - interval_length/2 # x values\n",
    "y = beta * x + norm.rvs(loc=0, scale=np.sqrt(sigma2), size=n) # y values\n",
    "\n",
    "print(\"beta = {:1.4f}\".format(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2350f5a",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "The most common approach is to choose an estimate $\\hat{\\beta}^{\\text{OLS}}$ for $\\beta$ such that the **sum of squared resiudals** (SSR) (or residual sum of squares (RSS)) between observations and predictions is minimized, i.e.,\n",
    "\n",
    "$$\\hat{\\beta}^{\\text{OLS}} = \\underset{\\beta}{\\text{argmin}}~\\sum_{i=1}^n \\big(y_i - \\beta~x_i\\big)^2$$\n",
    "\n",
    "The solution is given by \n",
    "\n",
    "$$\\hat{\\beta}^{\\text{OLS}} = \\frac{\\sum_{i=1}^n x_i ~ y_i }{\\sum_{i=1}^n x_i^2} = \\frac{x^T y}{x^T x}$$\n",
    "\n",
    "In our probabilistic setting, the same result is obtained by the **MLE estimate**. Since the observations $y_i$, $i=1,\\dots,n$, are independent and $\\mathcal{N}(\\beta~x_i, \\sigma^2)$-distributed, the joint probability density function of $y$ reads\n",
    "\n",
    "$$p(\\mathcal{D}~|~\\beta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} ~\\exp\\Big(-\\frac{1}{2}~\\frac{(y_i - \\beta~x_i)^2}{\\sigma^2}\\Big)$$\n",
    "\n",
    "Thus, the MLE yields\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}^{\\text{MLE}} &= \\underset{\\beta}{\\text{argmax}} ~\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} ~\\exp\\Big(-\\frac{1}{2}~\\frac{(y_i - \\beta~x_i)^2}{\\sigma^2}\\Big) \\\\\n",
    "&= \\underset{\\beta}{\\text{argmax}}~ \\prod_{i=1}^n~\\exp\\Big(-\\frac{1}{2}~\\frac{(y_i - \\beta~x_i)^2}{\\sigma^2}\\Big)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Since $\\ln$ is a monotonically increasing function, we can apply it to the righthand side without changing the $\\text{argmax}$. Consequently, it holds\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}^{\\text{MLE}} &= \\underset{\\beta}{\\text{argmax}}~ - \\frac{1}{2~\\sigma^2}~\\sum_{i=1}^n~\\big(y_i - \\beta~x_i\\big)^2 \\\\\n",
    "        &= \\underset{\\beta}{\\text{argmin}}~\\sum_{i=1}^n~\\big(y_i - \\beta~x_i\\big)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "i.e., the MLE estimate minimizes the SSR and $\\hat{\\beta}^{\\text{MLE}} = \\hat{\\beta}^{\\text{OLS}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# calculate OLS estimate\n",
    "betaOLS = np.dot(x, y)/np.dot(x, x)\n",
    "\n",
    "# plot data, true function and OLS result\n",
    "fig= plt.figure(figsize=(14, 10))\n",
    "fig.canvas.header_visible = False\n",
    "plt.scatter(x ,y, s=8.)\n",
    "plt.plot(boundary, beta*boundary, label='true function')\n",
    "plt.plot(boundary, betaOLS*boundary, c='orange', label='OLS regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1bb30",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Instead of minimizing the sum of squared resiudals, it is in some cases useful to introduce an additional **regularization** term. The use of a so-called $L^2$-regularization leads to ridge regression. In this case, the approach is to choose $\\hat{\\beta}^{\\text{Ridge}}$ such that \n",
    "\n",
    "$$\\hat{\\beta}^{\\text{Ridge}} = \\underset{\\beta}{\\text{argmin}}~\\sum_{i=1}^n \\big(y_i - \\beta~x_i\\big)^2 + \\lambda ~\\beta^2$$\n",
    "\n",
    "The regularization term penalizes large values of the slope $\\beta$. This means that a steep regression line is not desired. The parameter $\\lambda > 0$ is called **complexity parameter** and controls how much influence the regularization has.\n",
    "\n",
    "Again, the solution can be calculated explicitly and is given by \n",
    "\n",
    "$$\\hat{\\beta}^{\\text{Ridge}} = \\frac{\\sum_{i=1}^n x_i ~ y_i }{\\sum_{i=1}^n x_i^2 + \\lambda} = \\frac{x^T y}{x^T x + \\lambda}$$\n",
    "\n",
    "In our probabilistic setting, the same result is obtained by the **MAP estimate with Gaussian prior**. We assume that $\\beta \\sim \\mathcal{N}(0, \\sigma_{\\beta}^2)$. This choice expresses the expectation that $\\beta$ should probably be close to zero. The variance $\\sigma_{\\beta}^2$ is a hyperparameter and is the counterpart to the complexity parameter $\\lambda$. The MAP estimate $\\hat{\\beta}^{\\text{Gauss}}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}^{\\text{Gauss}} &= \\underset{\\beta}{\\text{argmax}} ~p(\\mathcal{D}~|~\\beta)~p(\\beta) \\\\\n",
    "&= \\underset{\\beta}{\\text{argmax}} ~\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} ~\\exp\\Big(-\\frac{1}{2}~\\frac{(y_i - \\beta~x_i)^2}{\\sigma^2}\\Big)~\\frac{1}{\\sqrt{2\\pi \\sigma_{\\beta}^2}} ~\\exp\\Big(-\\frac{1}{2}~\\frac{\\beta^2}{\\sigma_{\\beta}^2}\\Big)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Please note that $p(\\mathcal{D}~|~\\beta)~p(\\beta)$ is (up to normalization) again a normal distribution. Similarly to the MLE case, by dropping constants and application of $\\ln$ the estimate can be rewritten as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}^{\\text{Gauss}} &= \\underset{\\beta}{\\text{argmax}}~ - \\frac{1}{2~\\sigma^2}~\\sum_{i=1}^n~\\big(y_i - \\beta~x_i\\big)^2 - \\frac{1}{2~\\sigma_{\\beta}^2}~\\beta^2 \\\\\n",
    "&= \\underset{\\beta}{\\text{argmin}}~ \\sum_{i=1}^n~\\big(y_i - \\beta~x_i\\big)^2 + \\frac{\\sigma^2}{\\sigma_{\\beta}^2}~\\beta^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "Thus, $\\hat{\\beta}^{\\text{Gauss}} = \\hat{\\beta}^{\\text{Ridge}}$ with $\\lambda = \\frac{\\sigma^2}{\\sigma_{\\beta}^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a60fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# set complexity parameter for ridge regression\n",
    "lambdaRidge = 0.1\n",
    "\n",
    "# calculate ridge regression estimate\n",
    "betaRidge = np.dot(x, y)/(np.dot(x, x) + lambdaRidge)\n",
    "\n",
    "# plot data, true function and ridge regression result\n",
    "fig= plt.figure(figsize=(14, 10))\n",
    "fig.canvas.header_visible = False\n",
    "plt.scatter(x ,y, s=8.)\n",
    "plt.plot(boundary, beta*boundary, label='true function')\n",
    "plt.plot(boundary, betaRidge*boundary, c='g', label='ridge regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa16c9b",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "\n",
    "LASSO (least absolute shrinkage and selection operator) is a regression method that is similar to ridge regression, but usses $L^1$-regularization instead of $L^2$-regularization, i.e., the approach is to choose $\\hat{\\beta}^{\\text{LASSO}}$ such that \n",
    "\n",
    "$$\\hat{\\beta}^{\\text{LASSO}} = \\underset{\\beta}{\\text{argmin}}~\\sum_{i=1}^n \\big(y_i - \\beta~x_i\\big)^2 + \\lambda ~\\lvert \\beta \\rvert$$\n",
    "\n",
    "The $L^1$-regularization term also penalizes large values of the slope $\\beta$, but it is even stronger than $L^2$-regularization for $\\lvert \\beta \\rvert < 1$, since $\\lvert \\beta \\rvert > \\beta^2$.\n",
    "\n",
    "Again, the solution can be calculated explicitly and is given by \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}^{\\text{LASSO}} &= \\text{sgn}(\\hat{\\beta}^{\\text{OLS}}) ~ \\max\\Big(0, \\lvert \\hat{\\beta}^{\\text{OLS}} \\rvert - \\frac{2 \\lambda}{\\sum_{i=1}^n x_i^2}\\Big) \\\\\n",
    "&= \\text{sgn}(\\hat{\\beta}^{\\text{OLS}}) ~ \\max\\Big(0, \\lvert \\hat{\\beta}^{\\text{OLS}} \\rvert - \\frac{2 \\lambda}{x^T x}\\Big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\text{sgn}$ denotes the sign function (i.e., $\\text{sgn}(x) = 1$ if $x \\ge 0$ and $\\text{sgn}(x) = -1$ if $x < 0$). In particular, $\\hat{\\beta}^{\\text{LASSO}} = 0$ if the absolute value of $\\hat{\\beta}^{\\text{OLS}}$ is too small.\n",
    "\n",
    "In our probabilistic setting, the same result is obtained by the **MAP estimate with Laplace prior**. We assume that $\\beta \\sim \\text{Laplace}(0, b)$. Similarly to the Gaussian prior, this choice expresses the expectation that $\\beta$ should probably be close to zero, but this distribution is sharper at zero. The MAP estimate $\\hat{\\beta}^{\\text{Laplace}}$ is given by\n",
    "\n",
    "$$\\hat{\\beta}^{\\text{Laplace}} = \\underset{\\beta}{\\text{argmax}} ~\\Big(\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} ~\\exp\\Big(-\\frac{1}{2}~\\frac{(y_i - \\beta~x_i)^2}{\\sigma^2}\\Big)\\Big)~\\frac{1}{2b}~\\exp\\Big(-\\frac{\\lvert \\beta \\rvert}{b}\\Big)$$\n",
    "\n",
    "Similarly as before, by dropping constants and application of $\\ln$ the estimate can be rewritten as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}^{\\text{Laplace}} &= \\underset{\\beta}{\\text{argmax}}~ - \\frac{1}{2~\\sigma^2}~\\sum_{i=1}^n~\\big(y_i - \\beta~x_i\\big)^2 - \\frac{1}{b}~\\lvert \\beta \\rvert \\\\\n",
    "&= \\underset{\\beta}{\\text{argmin}}~ \\sum_{i=1}^n~\\big(y_i - \\beta~x_i\\big)^2 + \\frac{2 \\sigma^2}{b}~\\lvert \\beta \\rvert\n",
    "\\end{aligned}$$\n",
    "\n",
    "Thus, $\\hat{\\beta}^{\\text{Laplace}} = \\hat{\\beta}^{\\text{LASSO}}$ with $\\lambda = \\frac{2\\sigma^2}{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# set complexity parameter for LASSO\n",
    "lambdaLASSO = 0.1\n",
    "\n",
    "# calculate LASSO estimate\n",
    "betaLASSO = np.sign(betaOLS) * np.maximum(0, np.abs(betaOLS) - 2*lambdaLASSO/np.dot(x, x))\n",
    "\n",
    "# plot data, true function and LASSO result\n",
    "fig= plt.figure(figsize=(14, 10))\n",
    "fig.canvas.header_visible = False\n",
    "plt.scatter(x ,y, s=8.)\n",
    "plt.plot(boundary, beta*boundary, label='true function')\n",
    "plt.plot(boundary, betaLASSO*boundary, c='r', label='LASSO')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09a56b",
   "metadata": {},
   "source": [
    "OLS, ridge regression and LASSO can also be generalized to higher dimensions, i.e., instead of scalar values for $x$ and $\\beta$ $d$-dimensional vectors can be considered. In particular, in this more general setting LASSO is used to discard input variables, since the regularization yields $\\beta_j = 0$ if the $j$-th variables does not seem to have an impact on $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52afecf",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "\n",
    "In the ridge regression section, we have already mentioned that the use of a Gaussian prior $\\mathcal{N}(0, \\sigma_{\\beta}^2)$ for $\\beta$ results in a Gaussian posterior. In other words, the Gaussian distribution is its own conjugate prior. \n",
    "\n",
    "In order to obtain the posterior distribution, it remains to determine its mean and variance. In detail, it holds\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathcal{D}~|~\\beta)~p(\\beta) &\\propto \\prod_{i=1}^n \\exp\\Big(-\\frac{1}{2}~\\frac{\\big(y_i - \\beta~x_i\\big)^2}{\\sigma^2}\\Big)~\\exp\\Big(-\\frac{1}{2}~\\frac{\\beta^2}{\\sigma_{\\beta}^2}\\Big) \\\\\n",
    "&= \\exp\\Big(-\\frac{1}{2}~\\Big(\\sum_{i=1}^n \\frac{\\big(y_i - \\beta~x_i\\big)^2}{\\sigma^2} + \\frac{\\beta^2}{\\sigma_{\\beta}^2}\\Big)\\Big) \\\\\n",
    "&= \\exp\\Big(-\\frac{1}{2}~\\frac{y^T y - 2 x^T y \\beta + x^Tx \\beta^2}{\\sigma^2} + \\frac{\\beta^2}{\\sigma_{\\beta}^2}\\Big) \\\\\n",
    "&\\propto \\exp\\Big(-\\frac{1}{2}~\\frac{x^Tx ~ \\beta^2 - 2 x^T y ~ \\beta}{\\sigma^2} + \\frac{\\beta^2}{\\sigma_{\\beta}^2}\\Big) \\\\\n",
    "&= \\exp\\Big(-\\frac{1}{2}~\\frac{\\big(x^Tx + \\frac{\\sigma^2}{\\sigma_{\\beta}^2}\\big) ~ \\beta^2 - 2 x^T y ~ \\beta}{\\sigma^2}\\Big) \\\\\n",
    "&= \\exp\\Big(-\\frac{1}{2}~\\frac{\\beta^2 - 2 \\beta~\\Lambda^{-1} x^T y}{\\sigma^2 \\Lambda^{-1}}\\Big) \\\\\n",
    "&\\propto \\exp\\Big(-\\frac{1}{2}~\\frac{\\big(\\beta - \\Lambda^{-1} x^T y\\big)^2}{\\sigma^2 \\Lambda^{-1}}\\Big) \n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\Lambda := x^Tx + \\frac{\\sigma^2}{\\sigma_{\\beta}^2}$. Thus, $\\mathcal{N}(\\tilde{\\mu}, \\tilde{\\sigma}^2)$ is the posterior distribution with mean\n",
    "\n",
    "$$\\tilde{\\mu} = \\Lambda^{-1} x^T y = \\frac{x^T y}{x^Tx + \\frac{\\sigma^2}{\\sigma_{\\beta}^2}}$$\n",
    "\n",
    "and variance \n",
    "\n",
    "$$\\tilde{\\sigma}^2 = \\sigma^2 \\Lambda^{-1} = \\frac{\\sigma^2}{x^Tx + \\frac{\\sigma^2}{\\sigma_{\\beta}^2}}.$$ \n",
    "\n",
    "Please note that the mean $\\tilde{\\mu}$ equals the MAP estimate $\\hat{\\beta}^{\\text{Gauss}}$ which is used in ridge regression.\n",
    "\n",
    "Bayesian linear regression has the nice property that the prediction $y^* = \\beta~x^*$ for a test point $x^* \\in \\mathbb{R}$ is again normally distributed, since $\\beta \\sim \\mathcal{N}(\\tilde{\\mu}, \\tilde{\\sigma}^2)$, i.e., $y^* \\sim \\mathcal{N}(x^* ~ \\tilde{\\mu}, (x^*)^2~\\tilde{\\sigma}^2)$. In particular, confidence bounds for $y^*$ can be computed. Additionally to the expected value of the output, the **model is also able to quantify its uncertainty**! This is a huge advantage and a major reason to use probabilistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83067e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# choose prior variance such that posterior mean equals betaRidge\n",
    "sigma2_beta = sigma2/lambdaRidge # prior variance\n",
    "posterior_mean = np.dot(x, y)/(np.dot(x, x) + sigma2/sigma2_beta)\n",
    "posterior_var = sigma2 / (np.dot(x, x) + sigma2/sigma2_beta)\n",
    "\n",
    "# calculate mean predictions and variances on testpoints\n",
    "xstar = np.linspace(-interval_length/2, interval_length/2, num=50)\n",
    "ystar_mean = posterior_mean * xstar\n",
    "ystar_var = posterior_var * np.square(xstar)\n",
    "\n",
    "# plot data, true function and Bayesian linear regression result\n",
    "fig= plt.figure(figsize=(14, 10))\n",
    "fig.canvas.header_visible = False\n",
    "plt.scatter(x ,y, s=8.)\n",
    "plt.plot(boundary, beta*boundary, label='true function')\n",
    "plt.plot(xstar, ystar_mean, c='purple', label='Bayesian linear regression')\n",
    "plt.fill_between(xstar, ystar_mean - 1.96 * np.sqrt(ystar_var), ystar_mean + 1.96 * np.sqrt(ystar_var), \n",
    "                 color='purple', alpha=.25, label='confidence interval (excl. noise)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f7a5d",
   "metadata": {},
   "source": [
    "In the case that the variance $\\sigma^2$ of the noise term is not known, it is also possible to perform Bayesian inference on $\\beta$ and $\\sigma^2$ simultaneously. In this case, the inverse gamma distribution would be used as prior distribution for $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18d957",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "To visualize and to compare the preceding models for different parameter values please open the notebook in Google Colab and use the application below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "727e8b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87f93c7a5dd4199acf7ee6cc21f0705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=1, description='interval length', max=10, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c476b3d7904343c0bffa60a4773e1394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, description='n', max=40, min=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7170c2ec8174c13a41117bd2f0e29cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.05, description='sigma2', max=0.2, min=0.01, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, norm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import interact, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "il_w = widgets.IntSlider(value=1, min=1, max=10, description='interval length')\n",
    "n_w = widgets.IntSlider(value=10, min=5, max=40, description='n')\n",
    "sigma2_w = widgets.FloatSlider(value=0.05, min=0.01, max=0.2, step=0.01, description='sigma2')\n",
    "\n",
    "display(il_w, n_w, sigma2_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8dac4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d9232b5f54492aa1200adebe90c2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='Estimates', index=(0, 1, 2), options=('OLS', 'Ridge', 'LASSOâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interval_length = il_w.value\n",
    "n = n_w.value\n",
    "sigma2 = sigma2_w.value\n",
    "\n",
    "def generate(interval_length, n, sigma2):\n",
    "    # generate data\n",
    "    beta = norm.rvs(loc=0, scale=1)\n",
    "    x = interval_length * uniform.rvs(size=n) - interval_length/2 # x values\n",
    "    y = beta * x + norm.rvs(loc=0, scale=np.sqrt(sigma2), size=n) # y values\n",
    "    return beta, sigma2, interval_length, x, y\n",
    "\n",
    "beta, sigma2, interval_length, x, y = generate(interval_length, n, sigma2)\n",
    "\n",
    "estimates_w = widgets.SelectMultiple(\n",
    "                  options=['OLS', 'Ridge', 'LASSO', 'Bayesian'],\n",
    "                  value=['OLS', 'Ridge', 'LASSO'],\n",
    "                  description='Estimates')\n",
    "\n",
    "@interact(estimates=estimates_w, \n",
    "              lambdaRidge=(0., 1., 0.01),\n",
    "              lambdaLASSO=(0., 1., 0.01),\n",
    "              sigma2_beta=(0., 0.25, 0.01))\n",
    "def lin_reg(estimates,\n",
    "            lambdaRidge=0.5, lambdaLASSO=0.1, sigma2_beta=0.1):\n",
    "\n",
    "    # calculate OLS, ridge and LASSO estimates\n",
    "    betaOLS = np.dot(x, y)/np.dot(x, x)\n",
    "    betaRidge = np.dot(x, y)/(np.dot(x, x) + lambdaRidge)\n",
    "    betaLASSO = np.sign(betaOLS) * np.maximum(0, np.abs(betaOLS) - 2*lambdaLASSO/np.dot(x, x))\n",
    "\n",
    "    # Bayesian linear regression\n",
    "    # choose prior variance such that posterior mean equals betaRidge\n",
    "    posterior_mean = np.dot(x, y)/(np.dot(x, x) + sigma2/sigma2_beta)\n",
    "    posterior_var = sigma2 / (np.dot(x, x) + sigma2/sigma2_beta)\n",
    "\n",
    "    # calculate mean predictions and variances on testpoints\n",
    "    xstar = np.linspace(-interval_length/2, interval_length/2, num=50)\n",
    "    ystar_mean = posterior_mean * xstar\n",
    "    ystar_var = posterior_var * np.square(xstar)\n",
    "\n",
    "    boundary = np.array([-interval_length/2, interval_length/2])\n",
    "    fig= plt.figure(figsize=(14, 10))\n",
    "    fig.canvas.header_visible = False\n",
    "    plt.scatter(x ,y, color='b', s=8.)\n",
    "    plt.plot(boundary, beta*boundary, c='b', label='true function')\n",
    "    if 'OLS' in estimates:\n",
    "        plt.plot(boundary, betaOLS*boundary, c='orange', label='OLS')\n",
    "    if 'Ridge' in estimates:\n",
    "        plt.plot(boundary, betaRidge*boundary, c='g', label='ridge regression')\n",
    "    if 'LASSO' in estimates:\n",
    "        plt.plot(boundary, betaLASSO*boundary, c='r', label='LASSO')\n",
    "    if 'Bayesian' in estimates:\n",
    "        plt.plot(xstar, ystar_mean, c='purple', label='Bayesian linear regression')\n",
    "        plt.fill_between(xstar, \n",
    "                         ystar_mean - 1.96 * np.sqrt(ystar_var), \n",
    "                         ystar_mean + 1.96 * np.sqrt(ystar_var), \n",
    "                         color='purple', \n",
    "                         alpha=.25, label='confidence interval (excl. noise)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
